<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Flink | Hackx's Blog]]></title>
  <link href="http://untitled-life.github.io/blog/categories/flink/atom.xml" rel="self"/>
  <link href="http://untitled-life.github.io/"/>
  <updated>2020-01-10T11:12:24+08:00</updated>
  <id>http://untitled-life.github.io/</id>
  <author>
    <name><![CDATA[Mike Cao]]></name>
    <email><![CDATA[untitled2018life@gmail.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Apache Flink中保存点和检查点之间的差异(译文)]]></title>
    <link href="http://untitled-life.github.io/blog/2019/03/09/3-differences-between-savepoints-and-checkpoints-in-apache-flink/"/>
    <updated>2019-03-09T19:48:36+08:00</updated>
    <id>http://untitled-life.github.io/blog/2019/03/09/3-differences-between-savepoints-and-checkpoints-in-apache-flink</id>
    <content type="html"><![CDATA[<blockquote><p>我相信，如果你会看见敌人的伤口，你就不会拿起枪来对着他。 &ndash;龙应台 《谁欠了她们的人生》</p></blockquote>

<!-- more -->


<p>原文地址：<a href="https://www.da-platform.com/blog/differences-between-savepoints-and-checkpoints-in-flink">https://www.da-platform.com/blog/differences-between-savepoints-and-checkpoints-in-flink</a></p>

<p>在Apache Flink中，经常有人会把Savepoints和Checkpoints搞混淆，今天我们就给大家分别解释下这两个东西以及他们之间的差异。</p>

<h1>保存点和检查点分别是什么?</h1>

<p>Apache Flink保存点是一种允许对整个流应用程序进行“时间点”快照的特性。此快照包含关于您在输入中的位置的信息，以及关于源的所有位置和整个应用程序状态的信息。我们可以在不停止应用程序的情况下通过使用Chandy-Lamport算法的变体获得整个状态的一致快照。保存点包含两个主要元素:</p>

<ol>
<li>首先，保存点包括一个目录，目录中包含(通常较大的)二进制文件，这些文件表示流应用程序在检查点/保存点处的整个状态</li>
<li>一个(相对较小的)元数据文件，它包含指向保存点的所有文件的指针(路径)，这些文件存储在所选的分布式文件系统或数据存储中。</li>
</ol>


<p>请阅读我们之前的博客文章，了解<a href="https://www.da-platform.com/blog/turning-back-time-savepoints">如何在流应用程序中启用保存点的详细步骤</a>。</p>

<p>上面关于保存点的所有内容听起来与我们在前面的一篇文章中解释的Apache Flink中的检查点非常相似。检查点是Apache Flink用于从故障中恢复的内部机制，包括应用程序状态的副本和输入的读取位置。在失败的情况下，Flink通过从检查点加载应用程序状态并继续从恢复的读取位置恢复应用程序，就像什么都没有发生一样。</p>

<h1>Apache Flink中保存点和检查点之间的3个差异</h1>

<p>检查点和保存点是Apache Flink作为流处理框架非常独特的两个特性。保存点和检查点在它们的实现中可能看起来很相似，但是，这两个特性在以下3个方面有所不同:</p>

<ol>
<li>目的:从概念上讲，Flink的保存点不同于检查点，就像备份不同于传统数据库系统中的恢复日志一样。检查点的主要目标是充当Apache Flink中的恢复机制，确保一个容错处理框架能够从潜在的作业失败中恢复。相反，Savepoints的主要目标是作为在用户手动备份和恢复活动之后重新启动、继续或重新打开暂停的应用程序的方式。</li>
<li>实现:检查点和保存点的实现是不同的。检查点被设计成轻量级和快速的。例如，使用RocksDB状态后端的增量检查点使用RocksDB的内部格式，而不是Flink的本机格式。这用于加快RocksDB的检查点进程，使其成为更轻量级检查点机制的第一个实例。相反，保存点的设计目的是更加关注数据的可移植性，并支持对作业所做的任何更改，这些更改会使生成和恢复保存点的成本稍微高一些。</li>
<li>生命周期:检查点本质上是自动的、周期性的。它们由Flink自动地、定期地创建和删除，不需要任何用户交互，以确保在意外的作业失败时能够完全恢复。相反，保存点是由用户手动拥有和管理的(即它们是计划、创建和删除的)。</li>
</ol>


<p><img src="/images/post/3-diff-1.jpeg" alt="3-diff-1" /></p>

<h1>什么时候在流应用程序中使用保存点?</h1>

<p>尽管流处理应用程序处理连续生成的数据(数据“在动”)，但是在某些情况下，应用程序可能需要重新处理以前处理过的数据。Apache Flink中的保存点允许您在以下情况下这样做:</p>

<ol>
<li>部署流应用程序的更新版本，包括新特性、bug修复或更好的机器学习模型。</li>
<li>为应用程序引入A/B测试，使用相同的源数据流测试程序的不同版本，在不牺牲先前状态的情况下从相同的时间点开始测试。</li>
<li>在需要更多资源的情况下重新调整应用程序。</li>
<li>将流应用程序迁移到新的Apache Flink版本，或者将应用程序升级到不同的集群。</li>
</ol>


<h1>结论</h1>

<p>Apache Flink检查站和保存点是两个不同的特性,适用于不同的需求,以确保一致性、容错和确保应用程序状态保存在意想不到的工作失败(检查点)以及在升级的情况下,bug修复、迁移或A / B测试(使用保存点)。这两个特性结合在一起，可以在不同的实例中确保应用程序的状态在不同的场景和环境中保持不变。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[开发Flink应用程序的5个起步步骤]]></title>
    <link href="http://untitled-life.github.io/blog/2019/03/09/5-baby-steps-to-develop-a-flink-application/"/>
    <updated>2019-03-09T19:39:10+08:00</updated>
    <id>http://untitled-life.github.io/blog/2019/03/09/5-baby-steps-to-develop-a-flink-application</id>
    <content type="html"><![CDATA[<blockquote><p>一路上，两个人都很忙碌。是这样的，妈妈必须做导游，给安安介绍这个世界，安安是新来的。而妈妈漏掉的东西，安安得指出来，提醒他。 &ndash;龙应台 《孩子你慢慢来》</p></blockquote>

<!-- more -->


<p>原文地址：<a href="https://www.da-platform.com/blog/5-steps-flink-application-development">https://www.da-platform.com/blog/5-steps-flink-application-development</a></p>

<p>使Flink运转起来非常简单。在这篇文章中，我们将介绍5个起步步骤，指导您在本地环境搭建第一个可以运行的Flink应用程序。我们从讨论软件需求开始，并指出一些帮助您理解框架功能的培训资源。如果您喜欢从头开始，我们还将向您展示如何引导应用程序!这个快速概述将使您能够几乎不耗费任何时间的情况下启动Flink应用程序。</p>

<h1>1. 软件需求</h1>

<p>您可以在Linux、Mac OS X和Windows上开发和执行Flink应用程序。因为社区中的大多数开发人员都是在基于unix的设置中操作的，所以这个环境包含了最丰富的工具支持。首先，真正的需求是安装Java开发工具包(JDK) 8(或更高版本)——无论您是要使用Java还是Scala进行开发。虽然开发Flink应用程序并不是严格要求的，但我们建议您也在您的计算机上安装以下软件:</p>

<ul>
<li> Apache Maven 3. x。在我们的介绍培训中，大多数示例都使用Maven实现构建自动化。此外，Flink还提供了Maven原型来引导新的Flink Maven项目。</li>
<li> 用于Java和/或Scala开发的IDE。特别是对于Scala，我们推荐使用IntelliJ，因为它对Maven和易于安装的Scala插件提供了开箱即用的支持。对于Java, Eclipse或Netbeans也能正常工作。</li>
</ul>


<h1>2. 为你准备的训练材料</h1>

<p>如果您喜欢从头开始您自己的项目，那么您可以直接跳到下一个步骤。否则，data Artisans将提供一系列的培训练习，帮助您熟悉Flink的操作方式，并随着时间的推移使您轻松掌握时间和状态管理等更复杂的主题。这些练习(和可能的解决方案)可以在GitHub上找到，所以您可以轻松地克隆项目并使用Maven来构建它:</p>

<pre><code class="bash">git clone https://github.com/dataArtisans/flink-training-exercises.git
cd flink-training-exercises
mvn clean package
</code></pre>

<p>这需要几分钟，具体时间取决于连接的速度——Maven将下载所有必需的依赖项。如果一切按计划进行，构建成功，那么您就走上了正确的轨道!</p>

<h1>3.引导您自己的Flink项目</h1>

<p>当然，从现有项目开始是使用Flink进行应用程序开发的最简单方法。但是如果您想在某个时候从头开始创建自己的项目呢?Flink提供Maven原型来为Java和Scala应用程序生成Maven项目。例如，要创建一个quickstart Java项目作为Flink应用程序的基础，请运行以下命令:</p>

<pre><code class="bash">mvn archetype:generate \
-DarchetypeGroupId=org.apache.flink \
-DarchetypeArtifactId=flink-quickstart-java \
-DarchetypeVersion=1.7.0
</code></pre>

<p>上面的命令为Flink 1.7.0生成一个Maven项目，其中包含两个类:StreamingJob和BatchJob;它们分别为流和批处理Flink程序提供了基本框架。您可以调整参数以匹配您的版本和命名首选项!我们建议您将该项目导入您选择的IDE中，以便开发一个可运行的示例。如果您正在与灵感作斗争，您可以在<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.7/tutorials/datastream_api.html#writing-a-flink-program">Flink文档</a>中获得一些提示。</p>

<h1>4. 运行和调试您的第一个Flink应用程序</h1>

<p>尽管Flink是一个分布式数据处理系统，但在本地环境中使用您的机器更容易启动它。在典型的设置中，您会让master (JobManager)和workers (taskmanager)在不同的机器上作为独立的JVM进程运行;但是Flink还包括一种模式，允许您像多线程进程一样在相同的JVM中执行应用程序。这种模式允许您在IDE中轻松地开发、调试和执行Flink应用程序，就像任何其他Java或Scala项目一样。要启动应用程序，只需像往常一样运行main()方法!</p>

<p><img src="/images/post/flink-application.png" alt="flink-application" /></p>

<h1>5. 监视您的Flink应用程序</h1>

<p>如果您想知道正在运行的应用程序的底层发生了什么，那么您可以轻松地使用捆绑在Flink中的web接口来可视化和监视它。要为本地开发启用该接口，您需要向POM文件添加一个新的依赖项:</p>

<pre><code class="xml">&lt;dependency&gt;
    &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;
    &lt;artifactId&gt;flink-runtime-web_2.11&lt;/artifactId&gt;
    &lt;version&gt;${flink.version}&lt;/version&gt;
&lt;/dependency&gt;
</code></pre>

<p>并显式创建具有所需配置的本地执行环境:</p>

<pre><code class="java">import org.apache.flink.configuration.ConfigConstants;
...
Configuration config = new Configuration();
config.setBoolean(ConfigConstants.LOCAL_START_WEBSERVER, true);
StreamExecutionEnvironment env = StreamExecutionEnvironment.createLocalEnvironment(config);
...
</code></pre>

<p>在程序运行之后，Flink web接口现在应该可以在<a href="http://localhost:8081">本地环境</a> 下使用，看起来有点像下面的图像。记住，更改POM文件后要更新Maven项目，如果您的IDE中没有启用自动导入——这可能会在您第一次尝试访问web界面时导致错误。</p>

<p><img src="/images/post/flink-application-1.png" alt="flink-application-1" /></p>

<p><img src="/images/post/flink-application-2.png" alt="flink-application-2" /></p>

<p>恭喜你!您刚刚在IDE中运行了第一个Flink应用程序。Flink文档中的教程更进一步，展示了如何设置本地Flink集群，并像向远程集群提交作业一样提交作业。</p>

<p>我们鼓励您浏览Flink文档以获得进一步的支持或更详细的信息。如果您在任何时候感到困惑或有任何问题:我们的社区在Stack Overflow上非常活跃，您也可以使用邮件列表联系开发人员。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Apache Flink中事件时间的介绍]]></title>
    <link href="http://untitled-life.github.io/blog/2019/03/09/stream-processing-an-introduction-to-event-time-in-apache-flink/"/>
    <updated>2019-03-09T19:30:43+08:00</updated>
    <id>http://untitled-life.github.io/blog/2019/03/09/stream-processing-an-introduction-to-event-time-in-apache-flink</id>
    <content type="html"><![CDATA[<blockquote><p>一个刽子手的责任，在看准了头颈的分寸，一刀霍下，让鲜血喷起，人头落地。被杀的人究竟有罪或者冤枉，不是刽子手的事情。甚至于即使他明明知道眼前跪着的人其实无辜，也没有人会指责刽子手为凶手。我们可以说，刽子手只是奉命行事，做一天和尚当然就得撞一天钟。应该负责的，是判官；或者，是那个不健全的审判制度；再抽象一点，我们不妨这么说，错在那个封建的社会。 &ndash;龙应台 《人在欧洲》</p></blockquote>

<!-- more -->


<p>原文地址：<a href="https://www.da-platform.com/blog/stream-processing-introduction-event-time-apache-flink">https://www.da-platform.com/blog/stream-processing-introduction-event-time-apache-flink</a></p>

<p>Apache Flink在有状态流处理中支持多个时间概念。本文主要关注Apache Flink对事件时间的支持。在下面的小节中，我们将定义Apache Flink的事件时间是什么，我们将研究流处理框架中不同的时间概念，并描述Flink如何使用水印来度量事件时间的进度。</p>

<h1>流处理中的事件时间</h1>

<p>顾名思义，Apache Flink中的事件时间是在生产源上生成每个单独事件的时间。在标准场景中，从不同生产者收集的事件(即与移动应用程序、金融交易、应用程序和机器日志、传感器事件等的交互)在其元数据中存储一个时间元素。这个时间表示特定事件在生产源中生成的时间。当应用程序逻辑需要基于事件生成时间的数据进行处理或计算时，有状态流处理和实时计算将使用事件时间。</p>

<h1>事件时间、处理时间和摄入时间的不同之处</h1>

<p>为了有效地描述Flink如何在流处理中管理不同的时间概念，让我们设想一个场景(如下所示)，在这个场景中，一个手机游戏玩家坐地铁去上班。玩家在平台上启动手机游戏，生成的事件被发送到Flink操作符。但是，当玩家在地铁车厢内时，wifi连接丢失，数据存储在移动设备上，没有传输到流处理系统。一旦用户重新上线，设备就会将剩余的数据发送到流媒体基础设施，然后步行到办公室。</p>

<p>source/images/post/</p>

<p><img src="/images/post/Event-time-FFT-Icon-london-underground.png" alt="Event-time-FFT-Icon-london-underground" /></p>

<p>在上面的场景中，移动游戏提供商可能使用基于Apache Flink的实时消息应用程序来分析游戏上的用户活动，并通过推送通知或消息提供实时优惠。这个应用程序可以基于不同的时间概念来处理传入的数据:我们前面描述的事件时间、处理时间以及我们在下一节中描述的摄入时间。</p>

<p><strong>处理时间</strong>是指在流处理应用程序中执行特定操作的机器时间。当流处理应用程序使用处理时间时，它使用机器的时钟来运行任何操作。一个5小时处理时间窗口将对完整的5个小时时间间隔内到达操作符的所有事件进行合并。处理时间很简单，不需要流和机器之间的协调，而且它提供了最好的性能和最低的延迟。然而，在分布式系统和上面描述的示例这样的场景中，使用处理时间进行计算可能并不总是最合适的，因为事件将异步或无序地到达操作符。</p>

<p><img src="/images/post/Event-time-FFT-Icon-Event-time.png" alt="Event-time-FFT-Icon-Event-time" /></p>

<p><strong>摄取时间</strong>是事件到达流处理应用程序的时间。摄入时间是任何处理延迟及其潜在波动的原因，并且在处理系统“消费”消息时标记时间戳。然后将此时间戳与任何基于时间的操作的特定事件相关联。与处理时间相比，摄入时间提供了更可预测的结果，尽管仍然不是100%准确，因为它不能处理原始时间和无序数据。</p>

<p>与处理时间相比，摄入时间提供了更可预测的结果，因为它在源操作符上一次性分配了一个稳定的时间戳，这使得所有与时间相关的操作都返回到所分配的时间戳。相反，随着处理时间的增加，每个操作符可以根据本地系统时钟将记录分配到不同的窗口。</p>

<p>Apache Flink通过在源上分配自动时间戳和生成自动水印(参见下面)，以类似于事件时间的方式处理摄入时间。</p>

<p>Apache Flink中的默认时间是处理时间。但是，如果应用程序异步工作或与分布式系统一起工作，可能需要将Flink设置为事件时间。相对其他两种时间，事件时间将提供最准确的结果，因为，正如我们前面解释的，它在事件或“消息”源生成时分配时间戳。然后，这个时间戳在所有与时间相关的操作中跟踪事件。事件时间由Apache Flink中的水印处理和支持，我们将在下面介绍。</p>

<p>在Apache Flink中，处理时间可以通过以下代码更新为事件时间:</p>

<pre><code class="java">env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime)
</code></pre>

<h1>Flink中的水印和事件时间</h1>

<p>水印是Apache Flink在事件时间中度量进展的机制。水印是数据流的一部分，其携带了一个时间戳t。在数据流中，一个水印(t)宣称事件时间已达到时间t,意思就是不应该有更多的元素，其流时间戳t &lsquo; &lt; = t(即事件时间戳大于或等于水印)。水印对于无序流和不按时间戳排序的异步操作非常重要。通常，水印是一种声明，表示流中的某个特定点，即某个时间戳之前的所有事件都应该已经到达。一旦水印到达操作符，操作符可以将其内部事件时间时钟推进到水印的值。举个例子，在我们使用小时窗口操作的情况下，水印将允许Flink了解特定的小时窗口何时超过小时，以便操作符可以关闭正在运行的现有窗口。</p>

<p>我们之前的<a href="https://www.da-platform.com/blog/watermarks-in-apache-flink-made-easy">一篇文章</a>更详细地描述了这个特性，并在使用Apache Flink处理有状态流的水印时提供了一些有用的观察。</p>

<p>流处理应用程序所选择的时间支持将取决于应用程序的需求和特定的系统特性。为流应用程序选择正确的时间概念至关重要，因为这将最终影响您以后的结果和操作。Apache Flink的设计目的是提供灵活性，并根据应用程序的需求支持不同的时间概念。有关调试窗口和事件时间的更多信息，请阅读<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.6/monitoring/debugging_event_time.html">Flink文档</a>。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[4 Characteristics of Timers in Apache Flink to Keep in Mind]]></title>
    <link href="http://untitled-life.github.io/blog/2019/03/09/4-characteristics-of-timers-in-apache-flink-to-keep-in-mind/"/>
    <updated>2019-03-09T16:23:32+08:00</updated>
    <id>http://untitled-life.github.io/blog/2019/03/09/4-characteristics-of-timers-in-apache-flink-to-keep-in-mind</id>
    <content type="html"><![CDATA[<blockquote><p>虽然心中有爱，但是爱，冻结在经年累月的沉默里，好像藏着一个疼痛的伤口，没有纱布可绑。 &ndash;龙应台 《亲爱的安德烈》</p></blockquote>

<!-- more -->


<p>原文地址：<a href="https://www.da-platform.com/blog/4-characteristics-of-timers-in-apache-flink">https://www.da-platform.com/blog/4-characteristics-of-timers-in-apache-flink</a></p>

<p>本文描述了在Apache Flink中使用计时器的一些基本概念和注意事项。开发人员可以使用Flink的ProcessFunction操作符注册自己的计时器，以便可以访问流应用程序的一些基本构建块，例如：</p>

<ul>
<li>事件(流元素)</li>
<li>状态(容错、一致、仅在键控流上)</li>
<li>计时器(事件时间和处理时间，仅在键控流上)</li>
</ul>


<p>有关Apache Flink ProcessFunction的更多信息，我们建议阅读<a href="https://ci.apache.org/projects/flink/flink-docs-stable/dev/stream/operators/process_function.html">Apache Flink 1.7文档</a>以获得更多的说明和指导。</p>

<h1>Apache Flink中的计时器是什么?</h1>

<p>计时器使Flink流应用程序具有响应性，并可以对处理和事件时间的更改做出适配。我们之前的<a href="https://www.da-platform.com/blog/stream-processing-introduction-event-time-apache-flink">一篇文章</a>更详细地介绍了Apache Flink中时间的其他概念以及处理、摄入和事件时间之间的差异。当使用计时器处理事件流时，每当调用<strong>processElement(…)</strong>时，都会传递一个上下文对象，允许您访问元素的事件时间戳和TimerService。然后可以使用TimerService为将来的事件/处理时间瞬间注册回调。通过这样做，一旦到达计时器的特定时间瞬间，<strong>onTimer(…)</strong>方法将被调用。</p>

<p><strong>onTimer(…)</strong>回调被调用的时间点首先取决于是使用处理时间还是事件时间来注册计时器。特别是:</p>

<ul>
<li>当使用处理时间在Flink应用程序中注册计时器时，当机器的时钟时间达到计时器的时间戳时，将调用onTimer(…)方法。</li>
<li>当使用事件时间在Flink应用程序中注册计时器时，当操作符的水印达到或超过计时器的时间戳时，将调用onTimer(…)方法。</li>
</ul>


<p>与<strong>processElement(…)</strong>方法类似，<strong>onTimer(…)</strong>回调中的状态访问也限定在当前键(其注册计时器的键)。</p>

<p>这里值得注意的是，onTimer(…)和processElement(…)的调用都是同步的，因此在<strong>onTimer(…)</strong>和<strong>processElement(…)</strong>方法中对状态的访问和修改是安全的。</p>

<h1>牢记定时器的4个特点</h1>

<p>在这一段中，我们将讨论Apache Flink中计时器的4个基本特性，在使用它们之前应该记住这些特性。这些是:</p>

<h2>1. 定时器在KeyedStream上注册</h2>

<p>由于计时器是按每个键注册和触发的，所以KeyedStream是Apache Flink中使用计时器的任何操作和函数的先决条件。</p>

<h2>2.计时器会自动删除重复数据</h2>

<p>TimerService会自动删除计时器的重复项，每个键和时间戳最多只能有一个计时器。这意味着，当多个计时器为同一个键或时间戳注册时，onTimer()方法将只调用一次。</p>

<h2>3.定时器具有检查点特性</h2>

<p>计时器由Flink进行检查点，就像任何其他托管状态一样。当Flink从检查点或保存点恢复作业时，每一个注册的计时器应该在恢复操作触发之前被启动。</p>

<h2>4. 计时器可以被删除</h2>

<p>从Flink 1.6开始，计时器可以暂停和删除。如果您使用的Apache Flink版本比Flink 1.5老，那么您可能会遇到一个糟糕的检查点性能，因为有许多计时器无法删除或停止。</p>

<p>您可以使用以下命令停止处理时间定时器:</p>

<pre><code class="scala">long timestampOfTimerToStop = ...
ctx.timerService( ).deleteProcessingTimeTimer(timestampOfTimerToStop);
</code></pre>

<p>您还可以通过以下命令停止事件时间计时器:</p>

<pre><code class="scala">long timestampOfTimerToStop = ...
ctx.timerService( ).deleteEventTimeTimer(timestampOfTimerToStop);
</code></pre>

<p><img src="/images/post/FFT-Timers-in-flink.png" alt="Timers-in-flink" /></p>

<p>这里值得一提的是，如果没有注册具有给定时间戳的计时器，则停止计时器没有效果。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[4 Steps to Get Your Flink Application Ready for Production]]></title>
    <link href="http://untitled-life.github.io/blog/2019/03/09/4-steps-to-get-your-flink-application-ready-for-production/"/>
    <updated>2019-03-09T16:12:19+08:00</updated>
    <id>http://untitled-life.github.io/blog/2019/03/09/4-steps-to-get-your-flink-application-ready-for-production</id>
    <content type="html"><![CDATA[<blockquote><p>独裁，专制，腐败，不是哪一个主义制度所独有，但是东欧革命狂潮就应该给所有的专制政权，不管它是否什么主义，一个冰冷的警告，暴力，不能持久。 &ndash;龙应台 《百年思索》</p></blockquote>

<!-- more -->


<p>原文地址：<a href="https://www.da-platform.com/blog/4-steps-flink-application-production-ready">https://www.da-platform.com/blog/4-steps-flink-application-production-ready</a></p>

<p>本文将描述使Flink应用程序投入生产的必要配置步骤。在下面的部分中，我们概述了重要的配置参数，而这些参数是工程领导、DevOps和数据工程师将Flink作业引入生产阶段之前需要仔细考虑的。Apache Flink为大多数配置选项提供了开箱即用的默认设置，在许多情况下，这是POC阶段(概念验证)或探索不同api和Flink抽象的良好起点。</p>

<p>然而，将Flink应用程序引入生产环境需要额外的配置，这些配置能够有效地扩展和再次扩展您的应用程序，并使其可用于生产，并与不同的系统需求、Flink版本和连接器兼容，以用于未来的迭代和潜在的升级。</p>

<p>下面，我们将收集一些配置要点，以便在将Flink应用程序投入生产之前进行审查:
<img src="/images/post/FFT-4-steps-to-get-your-app-production-ready.png" alt="FFT-4-steps-to-get-your-app-production-ready.png" /></p>

<h2>1. 明确定义Flink运算符的最大并行度</h2>

<p>Flink的键控状态被组织在所谓的键组中，然后这些键组被分发到您的Flink操作符的并行实例中。这是要分发的最小原子单元，因此也会影响Flink应用程序的可伸缩性。每个操作符的键组数对于每个作业只选择一次:手动或默认情况下。默认值将给出大致的操作并行度* 1.5，下界为128，上界为32768。它可以通过setMaxParallelism(int maxParallelism)手动定义每个作业和/或每个操作符。</p>

<p>任何进入生产环境的Flink作业都应该指定最大的并行度。但是，这个值的决定应该经过仔细考虑，因为此时，一旦设置了最大并行度，就不能在稍后的阶段更新它。更改最大并行度的Flink作业只能使用全新的状态从头开始。在更改最大并行度时，从以前的检查点或保存点进行恢复不可用。</p>

<p>建议将最大并行度设置为足以满足应用程序未来对可伸缩性和可用性的需求，同时又相对较低，以避免影响应用程序的总体性能。这是由于这样一个事实:在具有最高并行度的情况下，Flink为其重新伸缩的能力维护某些元数据，这可能会增加Flink应用程序的总体状态大小。</p>

<p>Flink文档提供了关于如何<a href="https://ci.apache.org/projects/flink/flink-docs-stable/ops/state/large_state_tuning.html">使用检查点配置使用大状态的应用程序</a>的额外信息和指导。</p>

<h2>2. 为Flink运算符分配唯一的用户id (uuid)</h2>

<p>对于有状态Flink应用程序，建议为所有运算符分配惟一的用户id (uuid)。这是必要的，因为一些内置Flink运算符(比如windows)是有状态的，而其他运算符可能是无状态的，这使得很难知道哪些内置运算符实际上是有状态的，哪些不是。</p>

<p>Flink运算符的uuid可以使用uid(String uid)方法分配。运算符uuid允许Apache Flink有效地将运算符状态从保存点映射到适当的运算符，这是保存点在Flink应用程序中正常工作所必需的元素。</p>

<h2>3.全面考虑Flink应用程序的状态后端</h2>

<p>在投入生产之前，开发人员和工程领导应该仔细考虑他们的Flink应用程序的状态后端类型，因为Apache Flink目前不支持状态后端互操作性。这使得有必要从保存点恢复状态，用于最初获取保存点的相同状态后端。</p>

<p>在之前的一篇<a href="https://data-artisans.com/blog/stateful-stream-processing-apache-flink-state-backends">博客</a>文章中介绍了Apache Flink中目前支持的3种状态后端之间的差异。</p>

<p>对于生产用例，强烈建议使用RocksDB状态后端，因为这是目前唯一一种支持大状态和异步操作(如快照)的状态后端，这些操作允许在不停止Flink操作的情况下编写快照。另一方面，使用RocksDB状态后端可能会带来性能折衷，因为所有状态访问和检索都需要序列化(和反序列化)才能跨越JNI边界，这可能会影响应用程序的吞吐量(与内存状态后端相比)。</p>

<h2>4. 使作业管理器具有高可用性(HA)</h2>

<p>高可用性(HA)配置确保Flink应用程序中的JobManager组件的潜在故障能够自动恢复，从而最大限度地减少停机时间。JobManager的主要职责是协调Flink部署，比如调度和适当的资源分配。</p>

<p>默认情况下，Flink为每个Flink集群设置一个JobManager实例。这将创建一个单点故障点(SPOF):如果JobManager崩溃，则无法提交任何新程序，并且正在运行的程序会失败。因此，强烈建议为生产环境<a href="https://ci.apache.org/projects/flink/flink-docs-stable/ops/config.html#high-availability-ha">配置高可用性HA</a>。</p>

<p>以上4个步骤遵循社区设置的最佳实践，这些实践允许Flink应用程序在维护状态的同时任意扩展，处理更大容量的数据流和状态，并增加它们的可用性保证——生产用例的特定需求。Apache Flink文档中的<a href="https://ci.apache.org/projects/flink/flink-docs-stable/ops/">部署和操作</a>部分为稳定的Flink操作提供了额外的指导和支持。我们强烈建议在将应用程序转移到生产环境之前，遵循上述步骤并仔细阅读文档。</p>
]]></content>
  </entry>
  
</feed>
