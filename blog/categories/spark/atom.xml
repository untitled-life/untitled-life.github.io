<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Spark | Hackx's Blog]]></title>
  <link href="http://untitled-life.github.io/blog/categories/spark/atom.xml" rel="self"/>
  <link href="http://untitled-life.github.io/"/>
  <updated>2019-08-21T10:49:57+08:00</updated>
  <id>http://untitled-life.github.io/</id>
  <author>
    <name><![CDATA[Mike Cao]]></name>
    <email><![CDATA[untitled2018life@gmail.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[浅析Spark推测执行机制]]></title>
    <link href="http://untitled-life.github.io/blog/2019/04/30/a-brief-analysis-of-sparks-speculative-execution-mechanism/"/>
    <updated>2019-04-30T10:39:44+08:00</updated>
    <id>http://untitled-life.github.io/blog/2019/04/30/a-brief-analysis-of-sparks-speculative-execution-mechanism</id>
    <content type="html"><![CDATA[<p>幸福就是，生活中不必时时恐惧。幸福就是，寻常的人儿依旧。幸福就是，早上挥手说“再见”的人，晚上又平平常常地回来了，书包丢在同一个角落，臭球鞋塞在同一张椅下。 &ndash;龙应台 《目送》</p>

<!-- more -->


<h1>背景</h1>

<p>我们都知道，Hadoop生态系统中的一个核心思想是分而治之: 将计算任务进行分解，分发到多个节点上去执行，然后再把多个节点上的计算结果进行汇总聚合，进而得到最终结果。分而治之使任务可以被并行处理，进而加快了任务的处理速度。</p>

<p>因为存在多个处理节点，每个节点分配到的任务可能或者处理任务的速度不是完全相同的，这种现象被称为负载不均衡或者资源分布不均。这种现象的出现会导致各个任务运行时间的不一致，甚至会出现一个Task明显慢于同一Job的其它Task。</p>

<p>为了解决这个问题，Hadoop的MapReduce组件实现了一个被称为推测执行(Speculation Execution)的功能机制，当推测执行机制被开启后，MapReduce会检测每个任务的执行情况，当发现某个任务执行较慢时，MapReduce将会启动一个冗余(输入、算子、输出全是一样的)的任务来并行执行，两个任务中只要有一个任务完成，就说明此任务执行完成。下图(图片来自<a href="https://data-flair.training/">https://data-flair.training/</a>)大致展示了MapReduce的推测执行过程。</p>

<p>关于MapReduce的推测执行机制，之前已经介绍过，我们今天要介绍的是在Spark中的推测执行机制。</p>

<p><img src="/images/post/Speculative-Execution-in-Spark.gif" alt="Speculative-Execution-in-Spark" /></p>

<h1>Spark推测执行</h1>

<p>推测执行机制在Spark和MapReduce的实现思路虽然大致一样，但实现上还是有些差异的，下面我们看看Spark中的具体实现。</p>

<h2>推测执行参数</h2>

<p>下表列出了Spark(2.4.2版本)中关于推测执行的所有参数、默认状态及参数说明。</p>

<p><img src="/images/post/the_parameters_of_spark_speculation.png" alt="the_parameters_of_spark_speculation" /></p>

<h2>推测执行实现</h2>

<p>上表解释了Spark中关于推测执行参数的含义，接下来我们看看Spark是如何使用这些参数呢，搞清楚Spark是如何使用这些参数的，也就搞清楚了Spark的推测执行机制。</p>

<p>推测执行是伴随着任务调度器(<strong>TaskSchedulerImpl</strong>)启动而启动的，其中 <strong>start()</strong> 方法中用到了参数 <strong>spark.speculation</strong> 和 <strong>spark.speculation.interval</strong></p>

<pre><code class="scala">// 类名：org.apache.spark.scheduler.TaskSchedulerImpl

override def start() {
  backend.start()
  // 非本地调度后端且推测执行参数设置为true
  if (!isLocal &amp;&amp; conf.getBoolean("spark.speculation", false)) {
    logInfo("Starting speculative execution thread")
    // 启动推测执行线程
    speculationScheduler.scheduleWithFixedDelay(new Runnable {
      override def run(): Unit = Utils.tryOrStopSparkContext(sc) {
        // 检查所有的处于活跃状态的任务
        checkSpeculatableTasks()
      } // SPECULATION_INTERVAL_MS 是 spark.speculation.interval的值
    }, SPECULATION_INTERVAL_MS, SPECULATION_INTERVAL_MS, TimeUnit.MILLISECONDS)
  }
}
// How often to check for speculative tasks
val SPECULATION_INTERVAL_MS = conf.getTimeAsMs("spark.speculation.interval", "100ms")
</code></pre>

<p><strong>start()</strong>中只做了一些简单的判断和线程的启动，核心逻辑应该都在 <strong>checkSpeculatableTasks()</strong> 中</p>

<p>在 <strong>org.apache.spark.scheduler.TaskSchedulerImpl#checkSpeculatableTasks</strong> 中，并没有实际的业务逻辑，而是放到了 <strong>org.apache.spark.scheduler.TaskSetManager</strong> 中</p>

<pre><code class="scala">// 类名：org.apache.spark.scheduler.TaskSchedulerImpl


// 检查所有的处于活跃状态的任务
def checkSpeculatableTasks() {
  var shouldRevive = false
  synchronized {
    // 判断是否有需要推测执行的任务
    shouldRevive = rootPool.checkSpeculatableTasks(MIN_TIME_TO_SPECULATION)
  }
  if (shouldRevive) {
    backend.reviveOffers()
  }
}


// 类名：org.apache.spark.scheduler.TaskSetManager
override def checkSpeculatableTasks(minTimeToSpeculation: Int): Boolean = {
  // Can't speculate if we only have one task, and no need to speculate if the task set is a
  // zombie.
  if (isZombie || numTasks == 1) {
    return false
  }
  var foundTasks = false
  // 计算启动推测执行需要完成任务数的最小值
  val minFinishedForSpeculation = (SPECULATION_QUANTILE * numTasks).floor.toInt
  logDebug("Checking for speculative tasks: minFinished = " + minFinishedForSpeculation)

  // 如果成功的任务数大于上面计算的阈值并且成功的任务数大于0，进入推测执行检查
  if (tasksSuccessful &gt;= minFinishedForSpeculation &amp;&amp; tasksSuccessful &gt; 0) {
    val time = clock.getTimeMillis()
    // 成功执行Task的执行成功时间的中位数
    val medianDuration = successfulTaskDurations.median
    // 取中位数的SPECULATION_MULTIPLIER倍和minTimeToSpeculation的最大值作为阈值threshold
    val threshold = max(SPECULATION_MULTIPLIER * medianDuration, minTimeToSpeculation)
    // TODO: Threshold should also look at standard deviation of task durations and have a lower
    // bound based on that.
    logDebug("Task length threshold for speculation: " + threshold)
    // 遍历所有需要判断推测执行的task
    for (tid &lt;- runningTasksSet) {
      val info = taskInfos(tid)
      val index = info.index
      // 放入推测执行任务列表的条件：任务为成功、任务正在执行、任务执行时间超过threshold且未在推测执行任务列表
      if (!successful(index) &amp;&amp; copiesRunning(index) == 1 &amp;&amp; info.timeRunning(time) &gt; threshold &amp;&amp;
        !speculatableTasks.contains(index)) {
        logInfo(
          "Marking task %d in stage %s (on %s) as speculatable because it ran more than %.0f ms"
            .format(index, taskSet.id, info.host, threshold))
        speculatableTasks += index
        sched.dagScheduler.speculativeTaskSubmitted(tasks(index))
        foundTasks = true
      }
    }
  }
  foundTasks
}


// Quantile of tasks at which to start speculation
val SPECULATION_QUANTILE = conf.getDouble("spark.speculation.quantile", 0.75)
val SPECULATION_MULTIPLIER = conf.getDouble("spark.speculation.multiplier", 1.5)
</code></pre>

<p>在这个类( <strong>org.apache.spark.scheduler.TaskSetManager</strong> )中，我们找到了另外两个参数的使用逻辑</p>

<p>检测出来的被推测执行的任务会通过 <strong>org.apache.spark.scheduler</strong> 作为普通任务进行调度。两个任务谁先完成，另外一个任务都将会被终止。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Apache Spark Resource Management and YARN App Models]]></title>
    <link href="http://untitled-life.github.io/blog/2019/03/11/apache-spark-resource-management-and-yarn-app-models/"/>
    <updated>2019-03-11T11:16:23+08:00</updated>
    <id>http://untitled-life.github.io/blog/2019/03/11/apache-spark-resource-management-and-yarn-app-models</id>
    <content type="html"><![CDATA[<blockquote><p>爱情能持久多半是因为两人有一种“互利”的基础。 &ndash;龙应台 《目送》</p></blockquote>

<!-- more -->


<p>原文地址:<a href="https://blog.cloudera.com/blog/2014/05/apache-spark-resource-management-and-yarn-app-models/">https://blog.cloudera.com/blog/2014/05/apache-spark-resource-management-and-yarn-app-models/</a></p>

<p><strong>A concise look at the differences between how Spark and MapReduce manage cluster resources under YARN</strong></p>

<p>The most popular Apache YARN application after MapReduce itself is Apache Spark. At Cloudera, we have worked hard to stabilize Spark-on-YARN (<a href="https://issues.apache.org/jira/browse/SPARK-1101">SPARK-1101</a>), and CDH 5.0.0 added support for Spark on YARN clusters.</p>

<p>In this post, you’ll learn about the differences between the Spark and MapReduce architectures, why you should care, and how they run on the YARN cluster ResourceManager.</p>

<h1>Applications</h1>

<p>In MapReduce, the highest-level unit of computation is a job. The system loads the data, applies a map function, shuffles it, applies a reduce function, and writes it back out to persistent storage. Spark has a similar job concept (although a job can consist of more stages than just a single map and reduce), but it also has a higher-level construct called an “application,” which can run multiple jobs, in sequence or in parallel.</p>

<p><img src="/images/post/spark-yarn-f1.png" alt="spark-yarn" /></p>

<h2>Spark application architecture</h2>

<p>For those familiar with the Spark API, an application corresponds to an instance of the SparkContext class. An application can be used for a single batch job, an interactive session with multiple jobs spaced apart, or a long-lived server continually satisfying requests. Unlike MapReduce, an application will have processes, called Executors, running on the cluster on its behalf even when it’s not running any jobs. This approach enables data storage in memory for quick access, as well as lightning-fast task startup time.</p>

<h1>Executors</h1>

<p>MapReduce runs each task in its own process. When a task completes, the process goes away. In Spark, many tasks can run concurrently in a single process, and this process sticks around for the lifetime of the Spark application, even when no jobs are running.</p>

<p>The advantage of this model, as mentioned above, is speed: Tasks can start up very quickly and process in-memory data. The disadvantage is coarser-grained resource management. As the number of executors for an app is fixed and each executor has a fixed allotment of resources, an app takes up the same amount of resources for the full duration that it’s running. (When YARN supports <a href="https://issues.apache.org/jira/browse/YARN-1197">container resizing</a>, we plan to take advantage of it in Spark to acquire and give back resources dynamically.)</p>

<h1>Active Driver</h1>

<p>To manage the job flow and schedule tasks Spark relies on an active driver process. Typically, this driver process is the same as the client process used to initiate the job, although in YARN mode (covered later), the driver can run on the cluster. In contrast, in MapReduce, the client process can go away and the job can continue running. In Hadoop 1.x, the JobTracker was responsible for task scheduling, and in Hadoop 2.x, the MapReduce application master took over this responsibility.</p>

<h1>Pluggable Resource Management</h1>

<p>Spark supports pluggable cluster management. The cluster manager is responsible for starting executor processes. Spark application writers do not need to worry about what cluster manager against which Spark is running.</p>

<p>Spark supports YARN, Mesos, and its own “standalone” cluster manager. All three of these frameworks have two components. A central master service (the YARN ResourceManager, Mesos master, or Spark standalone master) decides which applications get to run executor processes, as well as where and when they get to run. A slave service running on every node (the YARN NodeManager, Mesos slave, or Spark standalone slave) actually starts the executor processes. It may also monitor their liveliness and resource consumption.</p>

<h1>Why Run on YARN?</h1>

<p>Using YARN as Spark’s cluster manager confers a few benefits over Spark standalone and Mesos:</p>

<ul>
<li>YARN allows you to dynamically share and centrally configure the same pool of cluster resources between all frameworks that run on YARN. You can throw your entire cluster at a MapReduce job, then use some of it on an Impala query and the rest on Spark application, without any changes in configuration.</li>
<li>You can take advantage of <a href="http://hadoop.apache.org/docs/r2.4.0/hadoop-yarn/hadoop-yarn-site/FairScheduler.html">all the features of YARN schedulers</a> for categorizing, isolating, and prioritizing workloads.</li>
<li>Spark standalone mode requires each application to run an executor on every node in the cluster, whereas with YARN, you choose the number of executors to use.</li>
<li>Finally, YARN is the only cluster manager for Spark that supports security. With YARN, Spark can run against Kerberized Hadoop clusters and uses secure authentication between its processes.</li>
</ul>


<h1>Running on YARN</h1>

<p>When running Spark on YARN, each Spark executor runs as a YARN container. Where MapReduce schedules a container and fires up a JVM for each task, Spark hosts multiple tasks within the same container. This approach enables several orders of magnitude faster task startup time.</p>

<p>Spark supports two modes for running on YARN, “yarn-cluster” mode and “yarn-client” mode.  Broadly, yarn-cluster mode makes sense for production jobs, while yarn-client mode makes sense for interactive and debugging uses where you want to see your application’s output immediately.</p>

<p>Understanding the difference requires an understanding of YARN’s Application Master concept. In YARN, each application instance has an Application Master process, which is the first container started for that application. The application is responsible for requesting resources from the ResourceManager, and, when allocated them, telling NodeManagers to start containers on its behalf. Application Masters obviate the need for an active client — the process starting the application can go away and coordination continues from a process managed by YARN running on the cluster.</p>

<p>In yarn-cluster mode, the driver runs in the Application Master. This means that the same process is responsible for both driving the application and requesting resources from YARN, and this process runs inside a YARN container. The client that starts the app doesn’t need to stick around for its entire lifetime.</p>

<p><img src="/images/post/spark-yarn-f31.png" alt="spark-yarn-f31" /></p>

<p>The yarn-cluster mode, however, is not well suited to using Spark interactively. Spark applications that require user input, like spark-shell and PySpark, need the Spark driver to run inside the client process that initiates the Spark application. In yarn-client mode, the Application Master is merely present to request executor containers from YARN. The client communicates with those containers to schedule work after they start:</p>

<p><img src="/images/post/spark-yarn-f22.png" alt="spark-yarn-f22" /></p>

<p>This table offers a concise list of differences between these modes:</p>

<p><img src="/images/post/spark-yarn-table.png" alt="spark-yarn-table" /></p>

<h1>Key Concepts in Summary</h1>

<ul>
<li><strong>Application</strong>: This may be a single job, a sequence of jobs, a long-running service issuing new commands as needed or an interactive exploration session.</li>
<li><strong>Spark Driver</strong>: The Spark driver is the process running the spark context (which represents the application session). This driver is responsible for converting the application to a directed graph of individual steps to execute on the cluster. There is one driver per application.</li>
<li><strong>Spark Application Master</strong>: The Spark Application Master is responsible for negotiating resource requests made by the driver with YARN and finding a suitable set of hosts/containers in which to run the Spark applications. There is one Application Master per application.</li>
<li><strong>Spark Executor</strong>: A single JVM instance on a node that serves a single Spark application. An executor runs multiple tasks over its lifetime, and multiple tasks concurrently. A node may have several Spark executors and there are many nodes running Spark Executors for each client application.</li>
<li><strong>Spark Task</strong>: A Spark Task represents a unit of work on a partition of a distributed dataset.</li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Wide vs Narrow Dependencies]]></title>
    <link href="http://untitled-life.github.io/blog/2018/12/27/wide-vs-narrow-dependencies/"/>
    <updated>2018-12-27T17:41:52+08:00</updated>
    <id>http://untitled-life.github.io/blog/2018/12/27/wide-vs-narrow-dependencies</id>
    <content type="html"><![CDATA[<p>太疼的伤口，你不敢去触碰；太深的忧伤，你不敢去安慰；太残酷的残酷，有时候，你不敢去注视。 &ndash;龙应台 《目送》</p>

<!-- more -->


<p>In this session, we are going to focus on wide versus narrow dependencies, which dictate relationships between RDDs in graphs of computation, which we&rsquo;ll see has a lot to do with shuffling.</p>

<p>So far, we have seen that some transformations significantly more expensive (latency) than others.</p>

<p>In this session we will:</p>

<ul>
<li>look at how RDD&rsquo;s are represented</li>
<li>dive into how and when Spark decides it must shuffle data</li>
<li>see how these dependencies make fault tolerance possible</li>
</ul>


<h2>Lineages</h2>

<p>Computations on RDDs are represented as a lineage graph, a DAG representing the computations done on the RDD. This representation/DAG is what Spark analyzes to do optimizations. Because of this, for a particular operation, it is possible to step back and figure out how a result of a computation is derived from a particular point.
<code>scala
val rdd = sc.textFile(...)
val filtered = rdd.map(...).filter(...).persist()
val count = filtered.count()
val reduced = filtered.reduce()
</code>
<img src="/images/post/lineage_graph.png" alt="lineage_graph" /></p>

<h2>How RDDs are represented</h2>

<p>RDDs are made up of 4 parts:</p>

<ul>
<li>Partitions: Atomic pieces of the dataset. One or many per compute node.</li>
<li>Dependencies: Models relationship between this RDD and its partitions with the RDD(s) it was derived from. (Note that the dependencies maybe modeled per partition as shown below).</li>
<li>A function for computing the dataset based on its parent RDDs.</li>
<li>Metadata about it partitioning scheme and data placement.</li>
</ul>


<p>/Users/caolei/IdeaProjects/untitled-life/source/images/post/.png</p>

<p><img src="/images/post/rdd_anatomy_1.png" alt="rdd_anatomy_1" /></p>

<p><img src="/images/post/rdd_anatomy_2.png" alt="rdd_anatomy_2" /></p>

<h2>RDD Dependencies and Shuffles</h2>

<p>Previously we saw the Rule of thumb: a shuffle can occur when the resulting RDD depends on other elements from the same RDD or another RDD.</p>

<p><strong>In fact, RDD dependencies encode when data must move across network</strong>. Thus they tell us when data is going to be shuffled.</p>

<p>Transformations cause shuffles, and can have 2 kinds of dependencies:
1. Narrow dependencies: Each partition of the parent RDD is used by at most one partition of the child RDD.
<code>text
[parent RDD partition] ---&gt; [child RDD partition]
</code>
<strong>Fast</strong>! No shuffle necessary. Optimizations like pipelining possible. Thus transformations which have narrow dependencies are fast.</p>

<ol>
<li>Wide dependencies: Each partition of the parent RDD may be used by multiple child partitions
<code>text
                    ---&gt; [child RDD partition 1]
[parent RDD partition] ---&gt; [child RDD partition 2]
                    ---&gt; [child RDD partition 3]
</code>
<strong>Slow</strong>! Shuffle necessary for all or some data over the network. Thus transformations which have narrow dependencies are slow.

<h3>Visual: Narrow dependencies Vs. Wide dependencies</h3></li>
</ol>


<p><img src="/images/post/narrow_vs_wide_dependencies.png" alt="narrow_vs_wide_dependencies" /></p>

<h3>Visual: Example</h3>

<p>Assume that we have a following DAG:</p>

<p><img src="/images/post/visual_dag.png" alt="visual_dag" /></p>

<p>What do the dependencies look like? Which are wide and which are narrow?</p>

<p><img src="/images/post/visual_dag_resolved.png" alt="visual_dag_resolved" /></p>

<p>The B to G join is narrow because groupByKey already partitions the keys and places them appropriately in B after shuffling.</p>

<p>Thus operations like join can <strong>sometimes be narrow and sometimes be wide.</strong></p>

<p><strong>Transformations with (usually) Narrow dependencies:</strong>
- map
- mapValues
- flatMap
- filter
- mapPartitions
- mapPartitionsWithIndex</p>

<p><strong>Transformations with (usually) Wide dependencies: (might cause a shuffle)</strong>
- cogroup
- groupWith
- join
- leftOuterJoin
- rightOuterJoin
- groupByKey
- reduceByKey
- combineByKey
- distinct
- intersection
- repartition
- coalesce</p>

<p><strong>This list usually holds, but as seen above, in case of join, depending on the use case, the dependency of an operation may be different from the above lists</strong></p>

<h3>How can I find out?</h3>

<p>dependencies method on RDDs: returns a sequence of Dependency objects, which are actually the dependencies used by Spark&rsquo;s scheduler to know how this RDD depends on RDDs.</p>

<p>The sorts of dependency objects that this method may return include:</p>

<ul>
<li><p>Narrow dependency objects</p>

<ul>
<li>OneToOneDependency</li>
<li>PruneDependency</li>
<li>RangeDependency</li>
</ul>
</li>
<li><p>Wide dependency objects</p>

<ul>
<li>ShuffleDependency</li>
</ul>
</li>
</ul>


<p>Another method toDebugString prints out a visualization of the RDD lineage along with other information relevant to scheduling. For example, indentations in the output separate groups of narrow transformations that may be pipelined together with wide transformations that require shuffles. These groupings are called <strong>stages.</strong></p>

<h5>Example</h5>

<pre><code class="scala">val wordsRDD = sc.parallelize(largeList)

/* dependencies */

val pairs = wordsRdd.map(c=&gt;(c,1))
                    .groupByKey
                    .dependencies          // &lt;-------------
// pairs: Seq[org.apache.spark.Dependency[_]] = List(org.apache.spark.ShuffleDependency@4294a23d)

/* toDebugString */

val pairs = wordsRdd.map(c=&gt;(c,1))
                    .groupByKey
                    .toDebugString         // &lt;-------------
// pairs: String =
// (8) ShuffledRDD[219] at groupByKey at &lt;console&gt;:38 []
//  +-(8) MapPartitionsRDD[218] at map at &lt;console&gt;:37 []
//     | ParallelCollectionRDD[217] at parallelize at &lt;console&gt;:36 []
</code></pre>

<h2>Lineages and Fault Tolerance</h2>

<p><strong>Lineages are the key to fault tolerance in Spark</strong></p>

<p>Ideas from <strong>functional programming</strong> enable fault tolerance in Spark:</p>

<ul>
<li>RDDs are immutable</li>
<li>We use higher-order-functions like map,flatMap,filter to do functional transformations on this immutable data</li>
<li>A function for computing the dataset based on its parent RDDs also is part of an RDD&rsquo;s representation.</li>
</ul>


<p>We just need to keep the information required by these 3 properties.</p>

<p>Along with keeping track of the dependency information between partitions as well, these 3 properties allow us to: <strong>Recover from failures by recomputing lost partitions from lineage graphs</strong>, as it is easy to back track in a lineage graph.</p>

<p>Thus we get Fault Tolerance without having to write the RDDs/Data to the disk! The whole data can be re-derived using the above information.</p>

<h3>Visual Example</h3>

<p>Lets assume one of our partitions from our previous example fails:</p>

<p><img src="/images/post/fault_tol_1.png" alt="fault_tol_1" /></p>

<p>We only have to recompute the data shown below to get back on track:</p>

<p><img src="/images/post/fault_tol_2.png" alt="fault_tol_2" /></p>

<p>Recomputing missing partitions is <strong>fast for narrow</strong> but <strong>slow for wide</strong> dependencies. So if above, a partition in G would have failed, it would have taken us more time to recompute that partition. So losing partitions that were derived from a transformation with wide dependencies, can be much slower.</p>
]]></content>
  </entry>
  
</feed>
