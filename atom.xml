<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Hackx's Blog]]></title>
  <link href="http://untitled-life.github.io/atom.xml" rel="self"/>
  <link href="http://untitled-life.github.io/"/>
  <updated>2019-06-21T15:53:59+08:00</updated>
  <id>http://untitled-life.github.io/</id>
  <author>
    <name><![CDATA[Mike Cao]]></name>
    <email><![CDATA[untitled2018life@gmail.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[JVM垃圾收集器]]></title>
    <link href="http://untitled-life.github.io/blog/2019/06/21/jvm-garbage-collectors/"/>
    <updated>2019-06-21T15:46:50+08:00</updated>
    <id>http://untitled-life.github.io/blog/2019/06/21/jvm-garbage-collectors</id>
    <content type="html"><![CDATA[<blockquote><p>我们都知道了，母亲要回的”家“，不是任何一个有邮递区号、邮差找得到的家，她要回的”家“，不是空间，而是一段时光。 &ndash;龙应台 《目送》</p></blockquote>

<!-- more -->


<h2>各代收集器列表</h2>

<p><img src="http://untitled-life.github.io/images/post/JVM%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6%E5%99%A8.png" alt="JVM垃圾回收器" /></p>

<h2>收集器组合</h2>

<p><img src="http://untitled-life.github.io/images/post/collectors_combinations.png" alt="JVM垃圾回收器组合" /><p class='post-footer'>
            显示信息
            <a href='http://untitled-life.github.io/blog/2019/06/21/jvm-garbage-collectors/'><a href="http://untitled-life.github.io/blog/2019/06/21/jvm-garbage-collectors/">http://untitled-life.github.io/blog/2019/06/21/jvm-garbage-collectors/</a></a><br/>
            written by <a href='http://untitled-life.github.io'>Mike Cao</a>
            &nbsp;posted at <a href='http://untitled-life.github.io'><a href="http://untitled-life.github.io">http://untitled-life.github.io</a></a>
            </p></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Java虚拟机常用参数]]></title>
    <link href="http://untitled-life.github.io/blog/2019/05/10/jvm-parameters/"/>
    <updated>2019-05-10T12:34:21+08:00</updated>
    <id>http://untitled-life.github.io/blog/2019/05/10/jvm-parameters</id>
    <content type="html"><![CDATA[<blockquote><p>孩子，我要求你读书用功，不是因为我要你跟别人比成绩，而是因为，我希望你将来会拥有选择的权利，选择有意义、有时间的工作，而不是被迫谋生。当你的工作在你心中有意义，你就有成就感。当你的工作给你时间，不剥夺你的生活，你就有尊严。成就感和尊严，给你快乐。 &ndash;龙应台 《亲爱的安德烈》</p></blockquote>

<!-- more -->


<p>虽然JVM帮我们管理着内存(内存分配和垃圾回收)，但是JVM不是那么完美，很多时候还是会出现内存泄露和溢出等问题,当这些情况发生时，首先是排查定位问题，然后进行解决。在解决问题时，经常会使用JVM参数来设置JVM的属性，下面我们列举了常用的JVM参数，方便大家理解和使用。</p>

<p><strong>-Xms20M</strong></p>

<p>设置JVM启动内存的最小值为20M。</p>

<p><strong>-Xmx20M</strong></p>

<p>设置JVM启动内存的最大值为20M，如果将-Xmx和-Xms的值设置为一样，可以避免JVM内存自动扩展。</p>

<p><strong>-verbose:gc</strong></p>

<p>输出虚拟机中GC的详细情况</p>

<p><strong>-Xss128k</strong></p>

<p>设置虚拟机栈的大小为128k</p>

<p><strong>-Xoss128k</strong></p>

<p>设置本地方法栈的大小为128k。不过HotSpot并不区分虚拟机栈和本地方法栈，因此对于HotSpot来说这个参数是无效的</p>

<p><strong>-XX:PermSize=10M</strong></p>

<p>设置JVM初始分配的永久代的容量，仅限Java7及之前版本</p>

<p><strong>-XX:MaxPermSize=10M</strong></p>

<p>设置JVM允许分配的永久代的最大容量，仅限Java7及之前版本</p>

<p><strong>-Xnoclassgc</strong></p>

<p>关闭JVM对类的垃圾回收</p>

<p><strong>-XX:+TraceClassLoading</strong></p>

<p>查看类的加载信息</p>

<p><strong>-XX:+TraceClassUnLoading</strong></p>

<p>查看类的卸载信息</p>

<p><strong>-XX:NewRatio=4</strong></p>

<p>设置年轻代：老年代的大小比值为1：4，这意味着年轻代占整个堆的1/5</p>

<p><strong>-XX:SurvivorRatio=8</strong></p>

<p>设置2个Survivor区：1个Eden区的大小比值为2:8，这意味着Survivor区占整个年轻代的1/5，这个参数默认为8</p>

<p><strong>-Xmn20M</strong></p>

<p>设置年轻代的大小为20M</p>

<p><strong>-XX:+HeapDumpOnOutOfMemoryError</strong></p>

<p>表示可以让虚拟机在出现内存溢出异常时Dump出当前的堆内存转储快照</p>

<p><strong>-XX:+UseG1GC</strong></p>

<p>设置JVM使用G1垃圾收集器</p>

<p><strong>-XX:+PrintGCDetails</strong></p>

<p>表示在控制台上打印出GC具体细节</p>

<p><strong>-XX:+PrintGC</strong></p>

<p>表示在控制台上打印出GC信息</p>

<p><strong>-XX:PretenureSizeThreshold=3145728</strong></p>

<p>表示对象大于3145728（3M）时直接进入老年代分配，这里只能以字节作为单位</p>

<p><strong>-XX:MaxTenuringThreshold=1</strong></p>

<p>表示对象年龄大于1，自动进入老年代,默认值是15</p>

<p><strong>-XX:CompileThreshold=1000</strong></p>

<p>表示一个方法被调用1000次之后，会被认为是热点代码，并触发即时编译</p>

<p><strong>-XX:+PrintHeapAtGC</strong></p>

<p>表示可以看到每次GC前后堆内存布局</p>

<p><strong>-XX:+PrintTLAB</strong></p>

<p>表示可以看到TLAB的使用情况</p>

<p><strong>-XX:+UseSpining</strong></p>

<p>开启自旋锁</p>

<p><strong>-XX:PreBlockSpin</strong></p>

<p>更改自旋锁的自旋次数，使用这个参数必须先开启自旋锁</p>

<p class='post-footer'>
            显示信息
            <a href='http://untitled-life.github.io/blog/2019/05/10/jvm-parameters/'>http://untitled-life.github.io/blog/2019/05/10/jvm-parameters/</a><br/>
            written by <a href='http://untitled-life.github.io'>Mike Cao</a>
            &nbsp;posted at <a href='http://untitled-life.github.io'>http://untitled-life.github.io</a>
            </p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[浅析Spark推测执行机制]]></title>
    <link href="http://untitled-life.github.io/blog/2019/04/30/a-brief-analysis-of-sparks-speculative-execution-mechanism/"/>
    <updated>2019-04-30T10:39:44+08:00</updated>
    <id>http://untitled-life.github.io/blog/2019/04/30/a-brief-analysis-of-sparks-speculative-execution-mechanism</id>
    <content type="html"><![CDATA[<p>幸福就是，生活中不必时时恐惧。幸福就是，寻常的人儿依旧。幸福就是，早上挥手说“再见”的人，晚上又平平常常地回来了，书包丢在同一个角落，臭球鞋塞在同一张椅下。 &ndash;龙应台 《目送》</p>

<!-- more -->


<h1>背景</h1>

<p>我们都知道，Hadoop生态系统中的一个核心思想是分而治之: 将计算任务进行分解，分发到多个节点上去执行，然后再把多个节点上的计算结果进行汇总聚合，进而得到最终结果。分而治之使任务可以被并行处理，进而加快了任务的处理速度。</p>

<p>因为存在多个处理节点，每个节点分配到的任务可能或者处理任务的速度不是完全相同的，这种现象被称为负载不均衡或者资源分布不均。这种现象的出现会导致各个任务运行时间的不一致，甚至会出现一个Task明显慢于同一Job的其它Task。</p>

<p>为了解决这个问题，Hadoop的MapReduce组件实现了一个被称为推测执行(Speculation Execution)的功能机制，当推测执行机制被开启后，MapReduce会检测每个任务的执行情况，当发现某个任务执行较慢时，MapReduce将会启动一个冗余(输入、算子、输出全是一样的)的任务来并行执行，两个任务中只要有一个任务完成，就说明此任务执行完成。下图(图片来自<a href="https://data-flair.training/">https://data-flair.training/</a>)大致展示了MapReduce的推测执行过程。</p>

<p>关于MapReduce的推测执行机制，之前已经介绍过，我们今天要介绍的是在Spark中的推测执行机制。</p>

<p><img src="http://untitled-life.github.io/images/post/Speculative-Execution-in-Spark.gif" alt="Speculative-Execution-in-Spark" /></p>

<h1>Spark推测执行</h1>

<p>推测执行机制在Spark和MapReduce的实现思路虽然大致一样，但实现上还是有些差异的，下面我们看看Spark中的具体实现。</p>

<h2>推测执行参数</h2>

<p>下表列出了Spark(2.4.2版本)中关于推测执行的所有参数、默认状态及参数说明。</p>

<p><img src="http://untitled-life.github.io/images/post/the_parameters_of_spark_speculation.png" alt="the_parameters_of_spark_speculation" /></p>

<h2>推测执行实现</h2>

<p>上表解释了Spark中关于推测执行参数的含义，接下来我们看看Spark是如何使用这些参数呢，搞清楚Spark是如何使用这些参数的，也就搞清楚了Spark的推测执行机制。</p>

<p>推测执行是伴随着任务调度器(<strong>TaskSchedulerImpl</strong>)启动而启动的，其中 <strong>start()</strong> 方法中用到了参数 <strong>spark.speculation</strong> 和 <strong>spark.speculation.interval</strong></p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
</pre></td><td class='code'><pre><code class='scala'><span class='line'><span class="c1">// 类名：org.apache.spark.scheduler.TaskSchedulerImpl</span>
</span><span class='line'>
</span><span class='line'><span class="k">override</span> <span class="k">def</span> <span class="n">start</span><span class="o">()</span> <span class="o">{</span>
</span><span class='line'>  <span class="n">backend</span><span class="o">.</span><span class="n">start</span><span class="o">()</span>
</span><span class='line'>  <span class="c1">// 非本地调度后端且推测执行参数设置为true</span>
</span><span class='line'>  <span class="k">if</span> <span class="o">(!</span><span class="n">isLocal</span> <span class="o">&amp;&amp;</span> <span class="n">conf</span><span class="o">.</span><span class="n">getBoolean</span><span class="o">(</span><span class="s">&quot;spark.speculation&quot;</span><span class="o">,</span> <span class="kc">false</span><span class="o">))</span> <span class="o">{</span>
</span><span class='line'>    <span class="n">logInfo</span><span class="o">(</span><span class="s">&quot;Starting speculative execution thread&quot;</span><span class="o">)</span>
</span><span class='line'>    <span class="c1">// 启动推测执行线程</span>
</span><span class='line'>    <span class="n">speculationScheduler</span><span class="o">.</span><span class="n">scheduleWithFixedDelay</span><span class="o">(</span><span class="k">new</span> <span class="nc">Runnable</span> <span class="o">{</span>
</span><span class='line'>      <span class="k">override</span> <span class="k">def</span> <span class="n">run</span><span class="o">()</span><span class="k">:</span> <span class="kt">Unit</span> <span class="o">=</span> <span class="nc">Utils</span><span class="o">.</span><span class="n">tryOrStopSparkContext</span><span class="o">(</span><span class="n">sc</span><span class="o">)</span> <span class="o">{</span>
</span><span class='line'>        <span class="c1">// 检查所有的处于活跃状态的任务</span>
</span><span class='line'>        <span class="n">checkSpeculatableTasks</span><span class="o">()</span>
</span><span class='line'>      <span class="o">}</span> <span class="c1">// SPECULATION_INTERVAL_MS 是 spark.speculation.interval的值</span>
</span><span class='line'>    <span class="o">},</span> <span class="nc">SPECULATION_INTERVAL_MS</span><span class="o">,</span> <span class="nc">SPECULATION_INTERVAL_MS</span><span class="o">,</span> <span class="nc">TimeUnit</span><span class="o">.</span><span class="nc">MILLISECONDS</span><span class="o">)</span>
</span><span class='line'>  <span class="o">}</span>
</span><span class='line'><span class="o">}</span>
</span><span class='line'><span class="c1">// How often to check for speculative tasks</span>
</span><span class='line'><span class="k">val</span> <span class="nc">SPECULATION_INTERVAL_MS</span> <span class="k">=</span> <span class="n">conf</span><span class="o">.</span><span class="n">getTimeAsMs</span><span class="o">(</span><span class="s">&quot;spark.speculation.interval&quot;</span><span class="o">,</span> <span class="s">&quot;100ms&quot;</span><span class="o">)</span>
</span></code></pre></td></tr></table></div></figure>


<p><strong>start()</strong>中只做了一些简单的判断和线程的启动，核心逻辑应该都在 <strong>checkSpeculatableTasks()</strong> 中</p>

<p>在 <strong>org.apache.spark.scheduler.TaskSchedulerImpl#checkSpeculatableTasks</strong> 中，并没有实际的业务逻辑，而是放到了 <strong>org.apache.spark.scheduler.TaskSetManager</strong> 中</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
<span class='line-number'>42</span>
<span class='line-number'>43</span>
<span class='line-number'>44</span>
<span class='line-number'>45</span>
<span class='line-number'>46</span>
<span class='line-number'>47</span>
<span class='line-number'>48</span>
<span class='line-number'>49</span>
<span class='line-number'>50</span>
<span class='line-number'>51</span>
<span class='line-number'>52</span>
<span class='line-number'>53</span>
<span class='line-number'>54</span>
<span class='line-number'>55</span>
<span class='line-number'>56</span>
<span class='line-number'>57</span>
<span class='line-number'>58</span>
<span class='line-number'>59</span>
<span class='line-number'>60</span>
<span class='line-number'>61</span>
</pre></td><td class='code'><pre><code class='scala'><span class='line'><span class="c1">// 类名：org.apache.spark.scheduler.TaskSchedulerImpl</span>
</span><span class='line'>
</span><span class='line'>
</span><span class='line'><span class="c1">// 检查所有的处于活跃状态的任务</span>
</span><span class='line'><span class="k">def</span> <span class="n">checkSpeculatableTasks</span><span class="o">()</span> <span class="o">{</span>
</span><span class='line'>  <span class="k">var</span> <span class="n">shouldRevive</span> <span class="k">=</span> <span class="kc">false</span>
</span><span class='line'>  <span class="n">synchronized</span> <span class="o">{</span>
</span><span class='line'>    <span class="c1">// 判断是否有需要推测执行的任务</span>
</span><span class='line'>    <span class="n">shouldRevive</span> <span class="k">=</span> <span class="n">rootPool</span><span class="o">.</span><span class="n">checkSpeculatableTasks</span><span class="o">(</span><span class="nc">MIN_TIME_TO_SPECULATION</span><span class="o">)</span>
</span><span class='line'>  <span class="o">}</span>
</span><span class='line'>  <span class="k">if</span> <span class="o">(</span><span class="n">shouldRevive</span><span class="o">)</span> <span class="o">{</span>
</span><span class='line'>    <span class="n">backend</span><span class="o">.</span><span class="n">reviveOffers</span><span class="o">()</span>
</span><span class='line'>  <span class="o">}</span>
</span><span class='line'><span class="o">}</span>
</span><span class='line'>
</span><span class='line'>
</span><span class='line'><span class="c1">// 类名：org.apache.spark.scheduler.TaskSetManager</span>
</span><span class='line'><span class="k">override</span> <span class="k">def</span> <span class="n">checkSpeculatableTasks</span><span class="o">(</span><span class="n">minTimeToSpeculation</span><span class="k">:</span> <span class="kt">Int</span><span class="o">)</span><span class="k">:</span> <span class="kt">Boolean</span> <span class="o">=</span> <span class="o">{</span>
</span><span class='line'>  <span class="c1">// Can&#39;t speculate if we only have one task, and no need to speculate if the task set is a</span>
</span><span class='line'>  <span class="c1">// zombie.</span>
</span><span class='line'>  <span class="k">if</span> <span class="o">(</span><span class="n">isZombie</span> <span class="o">||</span> <span class="n">numTasks</span> <span class="o">==</span> <span class="mi">1</span><span class="o">)</span> <span class="o">{</span>
</span><span class='line'>    <span class="k">return</span> <span class="kc">false</span>
</span><span class='line'>  <span class="o">}</span>
</span><span class='line'>  <span class="k">var</span> <span class="n">foundTasks</span> <span class="k">=</span> <span class="kc">false</span>
</span><span class='line'>  <span class="c1">// 计算启动推测执行需要完成任务数的最小值</span>
</span><span class='line'>  <span class="k">val</span> <span class="n">minFinishedForSpeculation</span> <span class="k">=</span> <span class="o">(</span><span class="nc">SPECULATION_QUANTILE</span> <span class="o">*</span> <span class="n">numTasks</span><span class="o">).</span><span class="n">floor</span><span class="o">.</span><span class="n">toInt</span>
</span><span class='line'>  <span class="n">logDebug</span><span class="o">(</span><span class="s">&quot;Checking for speculative tasks: minFinished = &quot;</span> <span class="o">+</span> <span class="n">minFinishedForSpeculation</span><span class="o">)</span>
</span><span class='line'>
</span><span class='line'>  <span class="c1">// 如果成功的任务数大于上面计算的阈值并且成功的任务数大于0，进入推测执行检查</span>
</span><span class='line'>  <span class="k">if</span> <span class="o">(</span><span class="n">tasksSuccessful</span> <span class="o">&gt;=</span> <span class="n">minFinishedForSpeculation</span> <span class="o">&amp;&amp;</span> <span class="n">tasksSuccessful</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="o">)</span> <span class="o">{</span>
</span><span class='line'>    <span class="k">val</span> <span class="n">time</span> <span class="k">=</span> <span class="n">clock</span><span class="o">.</span><span class="n">getTimeMillis</span><span class="o">()</span>
</span><span class='line'>    <span class="c1">// 成功执行Task的执行成功时间的中位数</span>
</span><span class='line'>    <span class="k">val</span> <span class="n">medianDuration</span> <span class="k">=</span> <span class="n">successfulTaskDurations</span><span class="o">.</span><span class="n">median</span>
</span><span class='line'>    <span class="c1">// 取中位数的SPECULATION_MULTIPLIER倍和minTimeToSpeculation的最大值作为阈值threshold</span>
</span><span class='line'>    <span class="k">val</span> <span class="n">threshold</span> <span class="k">=</span> <span class="n">max</span><span class="o">(</span><span class="nc">SPECULATION_MULTIPLIER</span> <span class="o">*</span> <span class="n">medianDuration</span><span class="o">,</span> <span class="n">minTimeToSpeculation</span><span class="o">)</span>
</span><span class='line'>    <span class="c1">// TODO: Threshold should also look at standard deviation of task durations and have a lower</span>
</span><span class='line'>    <span class="c1">// bound based on that.</span>
</span><span class='line'>    <span class="n">logDebug</span><span class="o">(</span><span class="s">&quot;Task length threshold for speculation: &quot;</span> <span class="o">+</span> <span class="n">threshold</span><span class="o">)</span>
</span><span class='line'>    <span class="c1">// 遍历所有需要判断推测执行的task</span>
</span><span class='line'>    <span class="k">for</span> <span class="o">(</span><span class="n">tid</span> <span class="k">&lt;-</span> <span class="n">runningTasksSet</span><span class="o">)</span> <span class="o">{</span>
</span><span class='line'>      <span class="k">val</span> <span class="n">info</span> <span class="k">=</span> <span class="n">taskInfos</span><span class="o">(</span><span class="n">tid</span><span class="o">)</span>
</span><span class='line'>      <span class="k">val</span> <span class="n">index</span> <span class="k">=</span> <span class="n">info</span><span class="o">.</span><span class="n">index</span>
</span><span class='line'>      <span class="c1">// 放入推测执行任务列表的条件：任务为成功、任务正在执行、任务执行时间超过threshold且未在推测执行任务列表</span>
</span><span class='line'>      <span class="k">if</span> <span class="o">(!</span><span class="n">successful</span><span class="o">(</span><span class="n">index</span><span class="o">)</span> <span class="o">&amp;&amp;</span> <span class="n">copiesRunning</span><span class="o">(</span><span class="n">index</span><span class="o">)</span> <span class="o">==</span> <span class="mi">1</span> <span class="o">&amp;&amp;</span> <span class="n">info</span><span class="o">.</span><span class="n">timeRunning</span><span class="o">(</span><span class="n">time</span><span class="o">)</span> <span class="o">&gt;</span> <span class="n">threshold</span> <span class="o">&amp;&amp;</span>
</span><span class='line'>        <span class="o">!</span><span class="n">speculatableTasks</span><span class="o">.</span><span class="n">contains</span><span class="o">(</span><span class="n">index</span><span class="o">))</span> <span class="o">{</span>
</span><span class='line'>        <span class="n">logInfo</span><span class="o">(</span>
</span><span class='line'>          <span class="s">&quot;Marking task %d in stage %s (on %s) as speculatable because it ran more than %.0f ms&quot;</span>
</span><span class='line'>            <span class="o">.</span><span class="n">format</span><span class="o">(</span><span class="n">index</span><span class="o">,</span> <span class="n">taskSet</span><span class="o">.</span><span class="n">id</span><span class="o">,</span> <span class="n">info</span><span class="o">.</span><span class="n">host</span><span class="o">,</span> <span class="n">threshold</span><span class="o">))</span>
</span><span class='line'>        <span class="n">speculatableTasks</span> <span class="o">+=</span> <span class="n">index</span>
</span><span class='line'>        <span class="n">sched</span><span class="o">.</span><span class="n">dagScheduler</span><span class="o">.</span><span class="n">speculativeTaskSubmitted</span><span class="o">(</span><span class="n">tasks</span><span class="o">(</span><span class="n">index</span><span class="o">))</span>
</span><span class='line'>        <span class="n">foundTasks</span> <span class="k">=</span> <span class="kc">true</span>
</span><span class='line'>      <span class="o">}</span>
</span><span class='line'>    <span class="o">}</span>
</span><span class='line'>  <span class="o">}</span>
</span><span class='line'>  <span class="n">foundTasks</span>
</span><span class='line'><span class="o">}</span>
</span><span class='line'>
</span><span class='line'>
</span><span class='line'><span class="c1">// Quantile of tasks at which to start speculation</span>
</span><span class='line'><span class="k">val</span> <span class="nc">SPECULATION_QUANTILE</span> <span class="k">=</span> <span class="n">conf</span><span class="o">.</span><span class="n">getDouble</span><span class="o">(</span><span class="s">&quot;spark.speculation.quantile&quot;</span><span class="o">,</span> <span class="mf">0.75</span><span class="o">)</span>
</span><span class='line'><span class="k">val</span> <span class="nc">SPECULATION_MULTIPLIER</span> <span class="k">=</span> <span class="n">conf</span><span class="o">.</span><span class="n">getDouble</span><span class="o">(</span><span class="s">&quot;spark.speculation.multiplier&quot;</span><span class="o">,</span> <span class="mf">1.5</span><span class="o">)</span>
</span></code></pre></td></tr></table></div></figure>


<p>在这个类( <strong>org.apache.spark.scheduler.TaskSetManager</strong> )中，我们找到了另外两个参数的使用逻辑</p>

<p>检测出来的被推测执行的任务会通过 <strong>org.apache.spark.scheduler</strong> 作为普通任务进行调度。两个任务谁先完成，另外一个任务都将会被终止。<p class='post-footer'>
            显示信息
            <a href='http://untitled-life.github.io/blog/2019/04/30/a-brief-analysis-of-sparks-speculative-execution-mechanism/'><a href="http://untitled-life.github.io/blog/2019/04/30/a-brief-analysis-of-sparks-speculative-execution-mechanism/">http://untitled-life.github.io/blog/2019/04/30/a-brief-analysis-of-sparks-speculative-execution-mechanism/</a></a><br/>
            written by <a href='http://untitled-life.github.io'>Mike Cao</a>
            &nbsp;posted at <a href='http://untitled-life.github.io'><a href="http://untitled-life.github.io">http://untitled-life.github.io</a></a>
            </p></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[阿里技术101]]></title>
    <link href="http://untitled-life.github.io/blog/2019/04/27/a-li-ji-zhu-101/"/>
    <updated>2019-04-27T14:42:53+08:00</updated>
    <id>http://untitled-life.github.io/blog/2019/04/27/a-li-ji-zhu-101</id>
    <content type="html"><![CDATA[<p>人总要慢慢成熟，将这个浮华的世界看得更清楚，看穿伪装的真实，看清隐匿的虚假，很多原本相信的事便不再相信。但是，要相信，这个世界里美好总要多过阴暗，欢乐总要多过苦难，还有很多事，值得你一如既往的相信。 &ndash;龙应台 《不相信》</p>

<!-- more -->


<p>最近花时间读了阿里技术参考手册，部分内容之前在阿里搬砖的时候就了解了，但整体看下来还是收益不少，所以决定分享出来</p>

<h2>阿里技术参考图册-研发</h2>

<p><a href="https://pan.baidu.com/s/1qx9SCN2LTdv05rbBDQiASw">阿里技术参考图册-研发(提取码: e4wi)</a></p>

<h2>阿里技术参考图册-算法</h2>

<p><a href="https://pan.baidu.com/s/1SJ8Toi6gLpBWEsNT8xlv-g">阿里技术参考图册-算法(提取码: jsca)</a></p>

<p class='post-footer'>
            显示信息
            <a href='http://untitled-life.github.io/blog/2019/04/27/a-li-ji-zhu-101/'>http://untitled-life.github.io/blog/2019/04/27/a-li-ji-zhu-101/</a><br/>
            written by <a href='http://untitled-life.github.io'>Mike Cao</a>
            &nbsp;posted at <a href='http://untitled-life.github.io'>http://untitled-life.github.io</a>
            </p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[关于Lambda架构的疑问(译文)]]></title>
    <link href="http://untitled-life.github.io/blog/2019/04/27/questioning-the-lambda-architecture/"/>
    <updated>2019-04-27T14:09:55+08:00</updated>
    <id>http://untitled-life.github.io/blog/2019/04/27/questioning-the-lambda-architecture</id>
    <content type="html"><![CDATA[<blockquote><p>这个社会不知为什么充满了对过去的怀念，对现在又充满了幻灭，往前看去，似乎有没什么新鲜的想象。我们的时代仿佛是个没有标记的时代，连叛逆的题目都找不到。 &ndash;龙应台 《亲爱的安德烈》</p></blockquote>

<!-- more -->


<p>原文地址：<a href="https://www.oreilly.com/ideas/questioning-the-lambda-architecture">https://www.oreilly.com/ideas/questioning-the-lambda-architecture</a></p>

<p><a href="http://nathanmarz.com/about/">Nathan Marz</a> 写过一篇很受欢迎的博客文章(“<a href="http://nathanmarz.com/blog/how-to-beat-the-cap-theorem.html">如何击败CAP定理</a>”)，文章中描述了一种被称为Lambda的架构。Lambda架构是在MapReduce和<a href="http://storm.incubator.apache.org/">Storm</a>或类似系统之上构建流处理应用程序的一种方法。事实证明，这是一个非常受欢迎的方法，并且有专门的<a href="http://lambda-architecture.net/">网站</a>和<a href="https://www.manning.com/books/big-data">书</a>来传播这种架构方法。由于我一直在使用<a href="http://kafka.apache.org/">Kafka</a>和<a href="http://samza.apache.org/">Samza</a>构建LinkedIn的实时数据处理基础设施，所以会被经常问到Lambda架构。我想我应该描述一下我的想法和经历。</p>

<h1>什么是Lambda架构</h1>

<p>在正式介绍Lambda架构是什么之前，我们先看下Lambda架构的样子。下图就是是Lambda架构的通用模型。</p>

<p><img src="http://untitled-life.github.io/images/post/lambda-archtecture.png" alt="lambda-archtecture" /></p>

<p>由上图我们可知：Lambda架构中会将数据(不可变记录)并行的灌入到批处理系统和流处理系统。分别在这两个系统进行数据转换处理(一次在批处理系统中，一次在流处理系统中)。然后将处理结果分别送到一个被称为Serving DB的地方，用户在查询时可以将两个系统的结果拼接在一起，用以生成最终的查询结果。</p>

<p>关于Lambda架构其实有很多变种，在这里我有意进行了简化。例如上图中的Kafka、Storm、Hadoop,您都可以换成其他功能类似的组件。比如Spark、Flink、Tez等。另外对于Serving DB，人们也通常使用两个不同的数据库来存储输出表，一个为实时优化，另一个为批量更新优化。</p>

<p>Lambda架构的目标是围绕复杂的异步转换构建的应用程序，这些应用系统需要以低延迟运行(例如，几秒钟到几小时)。一个很好的例子是新闻推荐系统，它需要抓取各种新闻源，处理和规范化所有输入，然后索引、排序，并将其存储起来便于提供服务。</p>

<p>我在LinkedIn参与了许多实时数据系统和管道的建设。其中一些方法就是以这种方式工作的，仔细想想，这并不是我最喜欢的方法。我认为有必要描述下我所看到的这种体系结构的优缺点，并给出我更喜欢的替代方案。</p>

<h1>Lambda架构的优势</h1>

<p>Lambda架构强调保持输入数据的不可变性，这点我非常喜欢并赞同。我认为将数据转换建模为从原始输入到一系列物化阶段的有很多优点。这是使大型MapReduce工作流易于处理的原因之一，因为它使您能够独立调试每个阶段。我认为这一点也很好地被用于流处理领域。关于捕获和转换不可变数据流的想法，我之前写过<a href="http://engineering.linkedin.com/distributed-systems/log-what-every-software-engineer-should-know-about-real-time-datas-unifying">一篇文章</a>对其进行了介绍。</p>

<p>我还喜欢这种体系结构突出的数据再处理问题。再处理是流处理的关键挑战之一，但常常被忽略。所谓“再处理”，我的意思是重新处理输入数据以重新获得输出。这是一个非常明显但经常被忽略的要求。代码总是会改变的。因此，如果您有从输入流派生输出数据的代码，每当代码发生更改时，您都需要重新计算输出以查看代码更改的效果。</p>

<p>为什么代码会发生改变？因为您的应用程序在发展，您希望计算以前不需要的新输出字段。或者因为您发现了一个bug，需要修复它。无论如何，当它发生变化时，您需要重新生成输出。我发现，许多试图构建实时数据处理系统的人并没有在这个问题上花太多心思，最终得到的只是一个无法快速发展的系统，因为它没有方便的方法来处理数据再处理问题。Lambda体系结构强调了这个问题，这一点值得赞扬。</p>

<p>针对Lambda架构的出现，其实还有许多其他的动机，但是我认为它们没有多大意义。其中一个说是实时处理本质上是近似的，比批处理功能更弱，更容易丢失数据。我不认为这是真的。但不可否认，现有的一些流处理框架不如MapReduce成熟，但是，流处理系统没有理由不能提供像批处理系统那样强大的语义保证。</p>

<p>我听到的另一种解释是Lambda架构允许不同的数据系统使用不同的trade-offs，从而在某种程度上“击败了CAP定理”。长话短说，虽然流处理中确实存在延迟和可用性之间的trade-offs，但这是异步处理体系结构，因此计算的结果不会立即与传入的数据保持一致。遗憾的是，CAP定理仍然完好无损。</p>

<h1>Lambda架构的劣势</h1>

<p>Lambda体系结构的问题在于需要维护在两个复杂的分布式系统中生成相同结果的代码，看起来就很痛苦。并且我认为这个问题是无法解决的。</p>

<p>在Storm和Hadoop这样的分布式框架中编程是很复杂的。不可避免地，代码最终会被设计成和它所运行的框架强相关。实现Lambda体系结构系统的操作复杂性似乎是每个人都一致同意的。</p>

<p>为什么不能改进流处理系统来处理目标域中的全部问题集?解决此问题的一种建议方法是使用一种语言或框架，该语言或框架对实时和批处理框架进行抽象。您使用这个高级框架编写代码，然后它“向下编译”以在幕后进行流处理或MapReduce。<a href="http://github.com/twitter/summingbird">Summingbird</a>就是这样一个框架。这确实让事情好了一点，但我不认为它解决了问题。</p>

<p>最后，即使可以避免对应用程序进行两次编码，运行和调试两个系统的操作负担也会非常高。任何新的抽象都只能提供这两个系统交集所支持的特性。更糟的是，致力于这种新的超级框架将使Hadoop如此强大的工具和语言的丰富生态系统隔离开来(Hive、Pig、Crunch、Cascading、Oozie等)。</p>

<p>打个比方，考虑一下让跨数据库ORM真正透明所面临的众所周知的困难。考虑到这只是对非常相似的系统进行抽象的问题，这些系统使用(几乎)标准化的接口语言提供几乎相同的功能。这在几乎不稳定的分布式系统上构建完全不同的编程范例上进行抽象的问题要困难得多。</p>

<h1>我们做过的</h1>

<p>实际上，在LinkedIn已经经历了好几轮这种情况。我们已经构建了各种混合的Hadoop体系结构，甚至还构建了一个特定领域的API，它允许代码“透明地”运行在实时或Hadoop中。这些方法都奏效了，但没有一种非常令人愉快或富有成效。保持两个不同系统中编写的代码完全同步是非常非常困难的。用于隐藏底层框架的API被证明是最容易泄漏的抽象。它最终需要深入了解Hadoop知识以及实时层的知识——并添加了新的需求，即当您调试问题或试图解释性能问题时，您必须充分了解API如何转换到这些底层系统。最终需要深入的Hadoop知识以及实时层的知识，如果需要添加一个新的功能，即无论何时调试问题或试图推断性能时，您都要充分了解如何将API转换为这些底层系统。</p>

<p>现在，我的建议是，如果您对延迟不敏感，那么使用批处理框架(如MapReduce)，如果您对延迟敏感，那么使用流处理框架，但是除非绝对必要，否则不要尝试同时使用这两种处理框架。</p>

<p>那么，为什么Lambda架构如此令人兴奋呢? 我认为原因是人们越来越需要构建复杂的、低延迟的处理系统。他们手头上的两种工具不能完全解决问题：一个可伸缩的高延迟批处理系统可以处理历史数据，另一个低延迟流处理系统不能重新处理结果。通过管道胶带把这两种东西粘在一起，他们实际上可以建立一个可行的解决方案。</p>

<p>从这个意义上说，尽管它可能很痛苦，但我认为Lambda体系结构解决了一个通常被忽略的重要问题。但我不认为这是一个新的范式或大数据的未来。它只是由现有工具的当前限制所驱动的临时状态。我也认为还有更好的选择。</p>

<h1>另一种选择</h1>

<p>作为一个设计基础设施的人，我认为最突出的问题是:为什么不能改进流处理系统来处理其目标领域中的全部问题集?为什么需要粘在另一个系统上? 为什么不能同时进行实时处理和代码更改时的再处理? 流处理系统已经有了并行的概念; 为什么不通过增加并行度和非常快地重放历史来处理重新处理呢? 答案是可以这么做。我认为这实际上是一个合理的替代架构，如果你现在正在构建这种类型的系统。</p>

<p>当我与人们讨论这个问题时，他们有时会告诉我，流处理不适用于历史数据的高吞吐量处理。但我认为这是一种基于他们所使用系统局限性的直觉，主要是，这些系统要么伸缩性很差，要么无法保存历史数据。这给他们一种感觉，流处理系统本质上是计算一些临时流的结果，然后丢弃所有底层数据的东西。但没有理由相信这是应该的。流处理中的基本抽象是数据流DAGs，它与传统数据仓库(<a href="http://paperhub.s3.amazonaws.com/dace52a42c07f7f8348b08dc2b186061.pdf">Volcano</a>)中的底层抽象完全相同，也是MapReduce后续<a href="http://hortonworks.com/hadoop/tez/">Tez</a>中的基本抽象。流处理只是这个数据流模型的泛化，它向最终用户公开中间结果的检查点和连续输出。</p>

<p>那么，我们如何直接在流处理系统中进行再处理操作呢?我喜欢的方法其实非常简单:</p>

<ol>
<li>使用Kafka或其他系统，可以保留希望能够重新处理的数据的完整日志，并允许多个订阅者。例如，如果您希望重新处理最多30天的数据，请将Kafka中的保留时间设置为30天。</li>
<li>当您希望进行再处理时，启动流处理作业的第二个实例，该实例从保留数据的开始处开始处理，但将此输出数据定向到一个新的输出表。</li>
<li>当第二个作业完成时，将应用程序切换为从新表读取。</li>
<li>停止作业的旧版本，并删除旧的输出表。</li>
</ol>


<p>下图简单展示了上面的处理逻辑:</p>

<p><img src="http://untitled-life.github.io/images/post/kappa-architecture.png" alt="kappa-architecture" /></p>

<p>与Lambda体系结构不同，在这种方法中，您只需要在处理代码更改时进行重新处理，并且实际上需要重新计算结果。当然，重新计算的工作只是相同代码的改进版本，运行在相同的框架上，获取相同的输入数据。当然，您会希望提高再处理作业的并行性，以便它能够非常快地完成。</p>

<p>也许我们可以称它为Kappa架构，尽管这个想法可能太简单了，不值得用希腊字母来写。</p>

<p>当然，您可以进一步优化它。在许多情况下，您可以组合这两个输出表。然而，我认为在短时间内同时拥有两者是有好处的。这允许您通过一个将应用程序重定向到旧表的按钮立即恢复到旧逻辑。在特别重要的情况下(比如，您的广告目标标准)，您可以使用自动A/B测试或<a href="http://shop.oreilly.com/product/0636920027393.do">bandit算法</a>来控制切换，以确保与以前的版本相比，您正在推出的任何bug修复或代码改进都没有意外地降低性能。</p>

<p>注意，这并不意味着您的数据不能进入HDFS;这就意味着你不会在那里进行再处理。Kafka与Hadoop有很好的集成，因此将任何Kafka主题镜像到HDFS中都很容易。在Hadoop中，流处理作业的输出流，甚至中间流，用于Hive之类的工具中的分析，或者用作其他离线数据处理流的输入，这通常很有用。</p>

<p>我们在<a href="http://samza.apache.org/learn/documentation/0.7.0/jobs/reprocessing.html">这篇文章</a>中已经记录了使用Samza实现此方法以及对再处理体系结构的其他变体。</p>

<h1>一些背景知识</h1>

<p>对于那些不太熟悉Kafka的人来说，我刚才所描述的可能没有意义。快速复习一下就有希望把事情弄清楚。Kafka是这样维护有序日志的:</p>

<p><img src="http://untitled-life.github.io/images/post/kafka-log.png" alt="kafka-log" /></p>

<p>Kafka中的“主题”其实就是这些日志的集合:</p>

<p><img src="http://untitled-life.github.io/images/post/kafka_partitioned_log.png" alt="kafka_partitioned_log" /></p>

<p>使用此数据的流处理器只维护一个“偏移量”，即它在每个分区上处理的最后一条记录的日志条目号。因此，更改使用者返回并重新处理数据的位置与使用不同的偏移量重新启动作业一样简单。为相同的数据添加第二个使用者只是指向日志中不同位置的另一个读取器。</p>

<p>Kafka支持复制和容错，运行在廉价的普通硬件上，并且乐于为每台机器存储许多TB级别的数据。因此，保留大量数据是一件非常自然和经济的事情，不会影响性能。LinkedIn在网上保存了超过PB的Kafka存储，许多应用程序都很好地利用了这种长时间的保留模式。</p>

<p>廉价的消费者和保留大量数据的能力使得添加第二个“再处理”任务只需要启动代码的第二个实例，但是要从日志中的不同位置开始。</p>

<p>这种设计并非偶然。我们构建Kafka的目的是使用它作为流处理的根基，我们想要的正是这个处理数据再处理的模型。好奇的读者可以在这里找到更多关于<a href="https://kafka.apache.org/documentation.html#introduction">Kafka的信息</a>。</p>

<p>然而，从根本上说，没有任何东西把这个想法与Kafka联系在一起。您可以替换任何支持长时间保留有序数据的系统(例如HDFS或某种数据库)。实际上，很多人都熟悉类似的模式，即事件源或CQRS。当然，分布式数据库的人会告诉你，这只是对物化视图维护的一个轻微的重新命名，他们会很高兴地提醒你，他们很久以前就知道了。</p>

<h1>对照</h1>

<p>我知道使用Samza作为流处理系统可以很好地使用这种方法，因为我们在LinkedIn上就是这么做的。但是我不知道它在Storm或其他流处理系统中不能正常工作的原因。我对Storm还不是很熟悉，所以我很高兴听到其他人已经在这么做了。在任何情况下，我认为总体思想和系统是相对独立的。</p>

<p>这两种方法之间的效率和资源权衡多少有些勉强。Lambda架构需要一直运行再处理和实时处理，而我所建议的只需要在需要重新处理时运行作业的第二个副本。然而，我的建议要求在输出数据库中临时拥有2倍的存储空间，并要求数据库支持用于重新加载的大容量写操作。在这两种情况下，额外的再处理负荷可能会平均出来。如果您有许多这样的作业，它们就不会同时进行再处理，因此在一个包含几十个这样的作业的共享集群中，您可能会为在任何给定时间内正在积极进行再处理的少数作业额外预算几个百分点的容量。</p>

<p>真正的优势根本不在于效率，而在于允许人们在一个处理框架上开发、测试、调试和操作他们的系统。因此，在简单性很重要的场景下，可以将此方法视为Lambda体系结构的替代方案。</p>

<p class='post-footer'>
            显示信息
            <a href='http://untitled-life.github.io/blog/2019/04/27/questioning-the-lambda-architecture/'>http://untitled-life.github.io/blog/2019/04/27/questioning-the-lambda-architecture/</a><br/>
            written by <a href='http://untitled-life.github.io'>Mike Cao</a>
            &nbsp;posted at <a href='http://untitled-life.github.io'>http://untitled-life.github.io</a>
            </p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[The Lambda Architecture for Big Data Systems]]></title>
    <link href="http://untitled-life.github.io/blog/2019/03/13/the-lambda-architecture-for-big-data-systems/"/>
    <updated>2019-03-13T15:14:52+08:00</updated>
    <id>http://untitled-life.github.io/blog/2019/03/13/the-lambda-architecture-for-big-data-systems</id>
    <content type="html"><![CDATA[<blockquote><p>某些人在某一个特定的时机热烈主张某一特定的正义，其中隐藏着深不可测的不正义。 &ndash;龙应台</p></blockquote>

<!-- more -->


<h1>the lambda architecture</h1>

<p><img src="http://untitled-life.github.io/images/post/the_lambda_architecture_for_big_data_systems.png" alt="the_lambda_architecture_for_big_data_systems" /></p>

<h1>reference</h1>

<ul>
<li><a href="https://en.wikipedia.org/wiki/Lambda_architecture">https://en.wikipedia.org/wiki/Lambda_architecture</a></li>
<li><a href="https://mapr.com/resources/stream-processing-mapr/">https://mapr.com/resources/stream-processing-mapr/</a></li>
<li><a href="http://lambda-architecture.net/">http://lambda-architecture.net/</a></li>
<li><a href="https://towardsdatascience.com/lambda-architecture-how-to-build-a-big-data-pipeline-part-1-8b56075e83fe">https://towardsdatascience.com/lambda-architecture-how-to-build-a-big-data-pipeline-part-1-8b56075e83fe</a></li>
<li><a href="https://medium.com/walmartlabs/how-we-built-a-data-pipeline-with-lambda-architecture-using-spark-spark-streaming-9d3b4b4555d3">https://medium.com/walmartlabs/how-we-built-a-data-pipeline-with-lambda-architecture-using-spark-spark-streaming-9d3b4b4555d3</a></li>
</ul>


<p class='post-footer'>
            显示信息
            <a href='http://untitled-life.github.io/blog/2019/03/13/the-lambda-architecture-for-big-data-systems/'>http://untitled-life.github.io/blog/2019/03/13/the-lambda-architecture-for-big-data-systems/</a><br/>
            written by <a href='http://untitled-life.github.io'>Mike Cao</a>
            &nbsp;posted at <a href='http://untitled-life.github.io'>http://untitled-life.github.io</a>
            </p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Apache Spark Resource Management and YARN App Models]]></title>
    <link href="http://untitled-life.github.io/blog/2019/03/11/apache-spark-resource-management-and-yarn-app-models/"/>
    <updated>2019-03-11T11:16:23+08:00</updated>
    <id>http://untitled-life.github.io/blog/2019/03/11/apache-spark-resource-management-and-yarn-app-models</id>
    <content type="html"><![CDATA[<blockquote><p>爱情能持久多半是因为两人有一种“互利”的基础。 &ndash;龙应台 《目送》</p></blockquote>

<!-- more -->


<p>原文地址:<a href="https://blog.cloudera.com/blog/2014/05/apache-spark-resource-management-and-yarn-app-models/">https://blog.cloudera.com/blog/2014/05/apache-spark-resource-management-and-yarn-app-models/</a></p>

<p><strong>A concise look at the differences between how Spark and MapReduce manage cluster resources under YARN</strong></p>

<p>The most popular Apache YARN application after MapReduce itself is Apache Spark. At Cloudera, we have worked hard to stabilize Spark-on-YARN (<a href="https://issues.apache.org/jira/browse/SPARK-1101">SPARK-1101</a>), and CDH 5.0.0 added support for Spark on YARN clusters.</p>

<p>In this post, you’ll learn about the differences between the Spark and MapReduce architectures, why you should care, and how they run on the YARN cluster ResourceManager.</p>

<h1>Applications</h1>

<p>In MapReduce, the highest-level unit of computation is a job. The system loads the data, applies a map function, shuffles it, applies a reduce function, and writes it back out to persistent storage. Spark has a similar job concept (although a job can consist of more stages than just a single map and reduce), but it also has a higher-level construct called an “application,” which can run multiple jobs, in sequence or in parallel.</p>

<p><img src="http://untitled-life.github.io/images/post/spark-yarn-f1.png" alt="spark-yarn" /></p>

<h2>Spark application architecture</h2>

<p>For those familiar with the Spark API, an application corresponds to an instance of the SparkContext class. An application can be used for a single batch job, an interactive session with multiple jobs spaced apart, or a long-lived server continually satisfying requests. Unlike MapReduce, an application will have processes, called Executors, running on the cluster on its behalf even when it’s not running any jobs. This approach enables data storage in memory for quick access, as well as lightning-fast task startup time.</p>

<h1>Executors</h1>

<p>MapReduce runs each task in its own process. When a task completes, the process goes away. In Spark, many tasks can run concurrently in a single process, and this process sticks around for the lifetime of the Spark application, even when no jobs are running.</p>

<p>The advantage of this model, as mentioned above, is speed: Tasks can start up very quickly and process in-memory data. The disadvantage is coarser-grained resource management. As the number of executors for an app is fixed and each executor has a fixed allotment of resources, an app takes up the same amount of resources for the full duration that it’s running. (When YARN supports <a href="https://issues.apache.org/jira/browse/YARN-1197">container resizing</a>, we plan to take advantage of it in Spark to acquire and give back resources dynamically.)</p>

<h1>Active Driver</h1>

<p>To manage the job flow and schedule tasks Spark relies on an active driver process. Typically, this driver process is the same as the client process used to initiate the job, although in YARN mode (covered later), the driver can run on the cluster. In contrast, in MapReduce, the client process can go away and the job can continue running. In Hadoop 1.x, the JobTracker was responsible for task scheduling, and in Hadoop 2.x, the MapReduce application master took over this responsibility.</p>

<h1>Pluggable Resource Management</h1>

<p>Spark supports pluggable cluster management. The cluster manager is responsible for starting executor processes. Spark application writers do not need to worry about what cluster manager against which Spark is running.</p>

<p>Spark supports YARN, Mesos, and its own “standalone” cluster manager. All three of these frameworks have two components. A central master service (the YARN ResourceManager, Mesos master, or Spark standalone master) decides which applications get to run executor processes, as well as where and when they get to run. A slave service running on every node (the YARN NodeManager, Mesos slave, or Spark standalone slave) actually starts the executor processes. It may also monitor their liveliness and resource consumption.</p>

<h1>Why Run on YARN?</h1>

<p>Using YARN as Spark’s cluster manager confers a few benefits over Spark standalone and Mesos:</p>

<ul>
<li>YARN allows you to dynamically share and centrally configure the same pool of cluster resources between all frameworks that run on YARN. You can throw your entire cluster at a MapReduce job, then use some of it on an Impala query and the rest on Spark application, without any changes in configuration.</li>
<li>You can take advantage of <a href="http://hadoop.apache.org/docs/r2.4.0/hadoop-yarn/hadoop-yarn-site/FairScheduler.html">all the features of YARN schedulers</a> for categorizing, isolating, and prioritizing workloads.</li>
<li>Spark standalone mode requires each application to run an executor on every node in the cluster, whereas with YARN, you choose the number of executors to use.</li>
<li>Finally, YARN is the only cluster manager for Spark that supports security. With YARN, Spark can run against Kerberized Hadoop clusters and uses secure authentication between its processes.</li>
</ul>


<h1>Running on YARN</h1>

<p>When running Spark on YARN, each Spark executor runs as a YARN container. Where MapReduce schedules a container and fires up a JVM for each task, Spark hosts multiple tasks within the same container. This approach enables several orders of magnitude faster task startup time.</p>

<p>Spark supports two modes for running on YARN, “yarn-cluster” mode and “yarn-client” mode.  Broadly, yarn-cluster mode makes sense for production jobs, while yarn-client mode makes sense for interactive and debugging uses where you want to see your application’s output immediately.</p>

<p>Understanding the difference requires an understanding of YARN’s Application Master concept. In YARN, each application instance has an Application Master process, which is the first container started for that application. The application is responsible for requesting resources from the ResourceManager, and, when allocated them, telling NodeManagers to start containers on its behalf. Application Masters obviate the need for an active client — the process starting the application can go away and coordination continues from a process managed by YARN running on the cluster.</p>

<p>In yarn-cluster mode, the driver runs in the Application Master. This means that the same process is responsible for both driving the application and requesting resources from YARN, and this process runs inside a YARN container. The client that starts the app doesn’t need to stick around for its entire lifetime.</p>

<p><img src="http://untitled-life.github.io/images/post/spark-yarn-f31.png" alt="spark-yarn-f31" /></p>

<p>The yarn-cluster mode, however, is not well suited to using Spark interactively. Spark applications that require user input, like spark-shell and PySpark, need the Spark driver to run inside the client process that initiates the Spark application. In yarn-client mode, the Application Master is merely present to request executor containers from YARN. The client communicates with those containers to schedule work after they start:</p>

<p><img src="http://untitled-life.github.io/images/post/spark-yarn-f22.png" alt="spark-yarn-f22" /></p>

<p>This table offers a concise list of differences between these modes:</p>

<p><img src="http://untitled-life.github.io/images/post/spark-yarn-table.png" alt="spark-yarn-table" /></p>

<h1>Key Concepts in Summary</h1>

<ul>
<li><strong>Application</strong>: This may be a single job, a sequence of jobs, a long-running service issuing new commands as needed or an interactive exploration session.</li>
<li><strong>Spark Driver</strong>: The Spark driver is the process running the spark context (which represents the application session). This driver is responsible for converting the application to a directed graph of individual steps to execute on the cluster. There is one driver per application.</li>
<li><strong>Spark Application Master</strong>: The Spark Application Master is responsible for negotiating resource requests made by the driver with YARN and finding a suitable set of hosts/containers in which to run the Spark applications. There is one Application Master per application.</li>
<li><strong>Spark Executor</strong>: A single JVM instance on a node that serves a single Spark application. An executor runs multiple tasks over its lifetime, and multiple tasks concurrently. A node may have several Spark executors and there are many nodes running Spark Executors for each client application.</li>
<li><strong>Spark Task</strong>: A Spark Task represents a unit of work on a partition of a distributed dataset. <p class='post-footer'>
          显示信息
          <a href='http://untitled-life.github.io/blog/2019/03/11/apache-spark-resource-management-and-yarn-app-models/'><a href="http://untitled-life.github.io/blog/2019/03/11/apache-spark-resource-management-and-yarn-app-models/">http://untitled-life.github.io/blog/2019/03/11/apache-spark-resource-management-and-yarn-app-models/</a></a><br/>
          written by <a href='http://untitled-life.github.io'>Mike Cao</a>
          &nbsp;posted at <a href='http://untitled-life.github.io'><a href="http://untitled-life.github.io">http://untitled-life.github.io</a></a>
          </p></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Hadoop YARN 深入浅出]]></title>
    <link href="http://untitled-life.github.io/blog/2019/03/11/untangling-apache-hadoop-yarn/"/>
    <updated>2019-03-11T11:12:59+08:00</updated>
    <id>http://untitled-life.github.io/blog/2019/03/11/untangling-apache-hadoop-yarn</id>
    <content type="html"><![CDATA[<blockquote><p>寂寞的感觉，像沙尘暴的漫天黑尘，以鬼魅的速度，细微地渗透地包围过来。 &ndash;龙应台 《目送》</p></blockquote>

<!-- more -->




<p class='post-footer'>
            显示信息
            <a href='http://untitled-life.github.io/blog/2019/03/11/untangling-apache-hadoop-yarn/'>http://untitled-life.github.io/blog/2019/03/11/untangling-apache-hadoop-yarn/</a><br/>
            written by <a href='http://untitled-life.github.io'>Mike Cao</a>
            &nbsp;posted at <a href='http://untitled-life.github.io'>http://untitled-life.github.io</a>
            </p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[The Guide of Hive Performance Optimization]]></title>
    <link href="http://untitled-life.github.io/blog/2019/03/10/the-guide-of-hive-performance-optimization/"/>
    <updated>2019-03-10T15:55:54+08:00</updated>
    <id>http://untitled-life.github.io/blog/2019/03/10/the-guide-of-hive-performance-optimization</id>
    <content type="html"><![CDATA[<blockquote><p>人生无常，每一次相聚都可能是最后一次啊！ &ndash;龙应台</p></blockquote>

<!-- more -->


<h1>Hive是什么</h1>

<p>关于Hive的定义，想必大家都都听过或看过一些。为了让之前没有接触过大数据的同学方便了解，这里也将给一个简单的定义：Hive 是一个基于 Hadoop 构建的开源数据仓库系统，我们使用它来查询和分析存储在 Hadoop 文件中的大型数据集。此外，通过使用 Hive，我们可以在 Hadoop 中处理结构化和半结构化数据。换句话说，Hive 是一个数据仓库基础设施，便于查询和管理驻留在分布式存储系统中的大型数据集。它提供了一种类 SQL 的查询语言 HiveQL（Hive Query Language）查询数据的方法。 此外，编译器在内部将 HiveQL 语句转换为 MapReduce、Tez、Spark 等作业。进一步提交给 Hadoop 框架执行。</p>

<h1>Hive的存在价值</h1>

<p>随着 Hadoop MapReduce 的出现，极大的简化大数据编程的难度，使得普通程序员也能从事开发大数据编程。但在生产活动中经常要对大数据计算分析是从事商务智能行业（BI）的工程师，他们通常使用 SQL 语言进行大数据统计及分析工作，而 Mapreduce 编程是有一定的门槛，如果每次都采用 MapReduce 开发计算分析任务，一是成本较高，二是效率太低，那么有没有更简单的办法，可以直接通过 SQL 在大数据平台下运行进行统计分析？有的，答案之一就是 Hive。</p>

<p>Hive 主要用于数据查询，统计和分析，提高开发人员的工作效率。Hive 通过内置函数将 SQL 语句生成 DAG（有向无环图），再让 Mapreduce 计算处理。从而得到我们想要的统计结果。而且在处理具有挑战性的复杂分析处理和数据格式时，极大的简化了开发难度。</p>

<h1>Hive架构</h1>

<p>本文的中心是关于Hive性能优化，但是在开始之前，了解下Hive在Hadoop生态中的位置及Hive的基础架构有助于对后面性能优化部分的理解。</p>

<h2>Hive在Hadoop生态的位置</h2>

<p><img src="http://untitled-life.github.io/images/post/hadoop_ecosystem.png" alt="hadoop_ecosystem" /></p>

<h2>Hive架构</h2>

<p><img src="http://untitled-life.github.io/images/post/hive_architecture.png" alt="hive_architecture" /></p>

<h1>性能优化</h1>

<p>大数据生态系统中的各种组件的构建都会重点考量一个问题：性能，Hive也不例外。好的性能不仅能节省资源，而且还能得到用户的满意。关于Hive的性能考量，主要包含以下四个方面：</p>

<ul>
<li>性能工具</li>
<li>设计优化</li>
<li>数据优化</li>
<li>任务优化</li>
</ul>


<h2>性能工具</h2>

<p>如果要进行Hive性能调优，首先需要判断是否有性能问题，Hive提供了多种工具帮助用户检查和判断查询的性能情况，其中比较常用的工具包括EXPLAIN和ANALYZE语句。除此之外，查询执行日志包含了非常多的信息，对性能问题的定位与解决也非常有帮助。</p>

<h3>EXPLAIN</h3>

<p>Hive通过EXPLAIN命令来显示查询的执行计划。语法如下：</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='sql'><span class='line'><span class="k">EXPLAIN</span> <span class="p">[</span><span class="n">FORMATTED</span><span class="o">|</span><span class="n">EXTENDED</span><span class="o">|</span><span class="n">DEPENDENCY</span><span class="o">|</span><span class="k">AUTHORIZATION</span><span class="p">]</span> <span class="n">hql_query</span>
</span></code></pre></td></tr></table></div></figure>


<ul>
<li>FORMATTED：以JSON格式返回查询计划</li>
<li>EXTENDED：提供执行计划关于操作的额外的信息，如文件名</li>
<li>DEPENDENCY：执行计划信息中包含依赖信息，如表、分区等</li>
<li>AUTHORIZATION：执行计划信息中包含此查询需要授权的对象</li>
</ul>


<p>Hive查询会被转换成（这是一个有向无环图）阶段序列。这些阶段可能是mapper/reducer阶段，或者做metastore或文件系统的操作，如移动和重命名的阶段。 EXPLAIN的输出包括三个部分：</p>

<ul>
<li>查询的抽象语法树(<a href="https://en.wikipedia.org/wiki/Abstract_syntax_tree">AST</a>), Hive使用的是一个名为<a href="http://www.antlr.org/">ANTLR</a>的解析器生成器为HQL生成语法树。ANTLR简单概括就是牛X强大。</li>
<li>执行计划的不同阶段之间的依赖关系</li>
<li>每个阶段的详细描述</li>
</ul>


<p>每个阶段的描述信息包括一个关于操作符(operators)的序列，这些操作符还会关联一些元数据信息。元数据可能包括FilterOperator的筛选表达式、SelectOperator的选择表达式或FileSinkOperator的输出文件名。</p>

<h3>ANALYSE</h3>

<p>类似于Oracle的分析表，Hive中也提供了分析表和分区的功能，通过自动和手动分析Hive表，将Hive表的一些统计信息存储到元数据中。表和分区的统计信息主要包括：行数、文件数、原始数据大小、所占存储大小、最后一次操作时间等；</p>

<h4>列统计</h4>

<p>针对表中的列数，特定列数据的直方图，有多种方式可以实现。作为查询优化的一种方法，统计输入给优化器的代价函数，然后优化器比较不同的计划，并从中获取较优的计划。统计有时能够满足用户的查询，从而让用户，快速获取结果（需执行存储的统计信息，而不需要触发长时间的执行计划）</p>

<h4>范围</h4>

<p>Hive 现在支持的表和分区级别的统计， 这些统计会存在MetaStore中,统计项包括Number of Rows、Number of files、Size in Bytes、Number of Partitions等
比如分区：</p>

<h2>设计优化</h2>

<p>设计优化主要包括分区表设计、分桶表设计、索引设计、倾斜/临时表设计等。下面分别对这四种设计进行介绍。</p>

<h3>分区表设计</h3>

<p>分区表设计是提高查询性能的最有效方法之一，尤其是在体量较大的表上。带有分区筛选的查询只从指定的分区(子目录)加载数据，大大减少了加载的数据量，节省IO、CPU等资源。除了极少量的静态维度表可以不适用分区，数据仓库原则上都应该是分区表。</p>

<h3>分桶表设计</h3>

<p>与分区表类似，分桶表在HDFS中将数据组织到单独的文件中。如果Join操作的判断条件是分桶字段，那Join的性能将会被大大提升。选择更好的桶列可以使桶表Join执行得更好。</p>

<h3>索引设计</h3>

<p>使用索引是关系数据库中性能调优的一种常见的实践手段。目前Hive也支持在表/分区上创建索引。Hive中的索引提供一个基于键的数据视图，并为某些操作(如WHERE、GROUP BY和JOIN)提供更好的数据访问。使用索引总是比使用全表扫描性能更好。在HQL中创建索引的命令非常简单，如下所示,详细请参考<a href="https://cwiki.apache.org/confluence/display/Hive/IndexDev">Hive官方文档</a>。</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='sql'><span class='line'><span class="k">CREATE</span> <span class="k">INDEX</span> <span class="n">index_name</span> <span class="k">ON</span> <span class="k">TABLE</span> <span class="k">table_name</span> <span class="p">(</span><span class="k">column_name</span><span class="p">)</span> <span class="k">AS</span> <span class="s1">&#39;COMPACT&#39;</span> <span class="k">WITH</span> <span class="k">DEFERRED</span> <span class="n">REBUILD</span><span class="p">;</span>   
</span></code></pre></td></tr></table></div></figure>


<h3>倾斜/临时表设计</h3>

<p>除了使用常规的内部/外部或分区表之外，某些场景下我们还可以考虑使用倾斜或临时表，以获得更好的设计和性能。HQL支持创建一个特殊的表来组织倾斜的数据。倾斜表可以通过将这些倾斜的值自动分割到单独的文件或目录中来提高性能。结果，文件或分区文件夹的总数减少了。此外，查询可以快速有效地包含或忽略这些数据。下面是一个用来创建倾斜表的例子,详细请参考<a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+DDL#LanguageManualDDL-SkewedTables">Hive官方文档</a>。</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class='sql'><span class='line'><span class="k">CREATE</span> <span class="k">TABLE</span> <span class="k">table_name</span> <span class="p">(</span>
</span><span class='line'>  <span class="n">dept_no</span> <span class="nb">int</span><span class="p">,</span>
</span><span class='line'>  <span class="n">dept_name</span> <span class="n">string</span>
</span><span class='line'><span class="p">)</span>
</span><span class='line'><span class="n">SKEWED</span> <span class="k">BY</span> <span class="p">(</span><span class="n">dept_no</span><span class="p">)</span> <span class="k">ON</span> <span class="p">(</span><span class="mi">1000</span><span class="p">,</span> <span class="mi">2000</span><span class="p">);</span>
</span></code></pre></td></tr></table></div></figure>


<h2>数据优化</h2>

<p>数据优化主要包括对数据文件格式、压缩和存储方面的性能改进。</p>

<h3>数据文件格式</h3>

<p>Hive支持的文件格式比价多，主要有TEXTFILE, SEQUENCEFILE, AVRO, RCFILE, ORC, and PARQUET。用户不仅可以在创建表时指定数据文件格式，还可以后期修改数据文件格式。</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='sql'><span class='line'><span class="k">CREATE</span> <span class="k">TABLE</span> <span class="p">...</span> <span class="n">STORE</span> <span class="k">AS</span> <span class="o">&lt;</span><span class="n">file_format</span><span class="o">&gt;</span>
</span></code></pre></td></tr></table></div></figure>




<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='sql'><span class='line'><span class="k">ALTER</span> <span class="k">TABLE</span> <span class="p">...</span> <span class="p">[</span><span class="n">PARTITION</span> <span class="n">partition_spec</span><span class="p">]</span> <span class="k">SET</span> <span class="n">FILEFORMAT</span>
</span></code></pre></td></tr></table></div></figure>


<p>一旦创建了以文本格式存储的表，我们就可以直接将文本数据加载到其中。要将文本数据加载到具有其他文件格式的表中，我们可以先将数据加载到存储格式为文本的表中，然后使用INSERT OVERWRITE/ into table…从中选择数据，然后将数据插入具有其他文件格式的表中。</p>

<p>TEXT, SEQUENCE, and AVRO等面向行的文件存储格式虽然比较易于理解，但是缺陷也显而易见，如即使查询结果只包含一列，查询也必须读取完整的一行。为了解决这个问题，RCFILE、ORC、PARQUET 等行列混合文件存储方案诞生。</p>

<p>根据所使用的技术栈，如果Hive是用于定义或处理数据的主要工具，建议使用ORC格式。如果您在数据生态系统中使用多种工具，那么PARQUET在适应性方面是更好的选择。</p>

<h3>数据压缩</h3>

<p>Hive中的压缩技术可以通过适当地压缩中间和最终输出数据，大大减少mappers和reducers之间的数据传输量，因此，查询将具有更好的性能。要压缩多个MapReduce作业之间生成的中间文件，我们需要在命令行会话或hive-site.xml文件中设置以下属性(默认为false):</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='sql'><span class='line'><span class="k">SET</span> <span class="n">hive</span><span class="p">.</span><span class="k">exec</span><span class="p">.</span><span class="n">compress</span><span class="p">.</span><span class="n">intermediate</span><span class="o">=</span><span class="k">true</span><span class="p">;</span>
</span></code></pre></td></tr></table></div></figure>


<p>下面列举了一些常用的数据压缩选项</p>

<p>source/images/post/.png</p>

<p><img src="http://untitled-life.github.io/images/post/data_compress_option.png" alt="data_compress_option" /></p>

<p>常用的压缩命令如下(可以在hive- site.xml或者命令行设置)</p>

<p><img src="http://untitled-life.github.io/images/post/data_compress_option_2.png" alt="data_compress_option_2" /></p>

<h3>数据存储</h3>

<p>数据根据使用频率可以简单分为冷热数据。如果我们能提高在热数据上的查询性能，其实整体的数据查询性能将有较大的改善。那么如何提高热数据的查询性能呢？其中一个方法就是增加数据的replication值，增大replication值在一定程度上会减少数据在集群节点之间的传输过程，进而提高性能。</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'>hdfs dfs -setrep -R -w <span class="m">4</span> /user/hive/warehouse/dwd.db/dwd_putong_profile_yay_private_question_a_d  
</span></code></pre></td></tr></table></div></figure>


<p>任何事情都是有限度的，过多的冗余文件会使namenode的内存耗尽，特别是大量的小文件。Hadoop本身已经有一些解决方案，可以通过以下方式处理很多小文件问题:</p>

<ul>
<li>Hadoop Archive/HAR:对小文件进行打包归档</li>
<li>数据采用SEQUENCEFILE格式进行存储，一定程度上可以将小文件压缩成大文件</li>
<li>HDFS Federation支持多个namenode来管理更多的文件</li>
<li>自己开发一个文件合并程序来合并HDFS中的小文件</li>
</ul>


<p>针对Hive的数据存储优化，可以设置下列参数来避免创建更多的小文件</p>

<ul>
<li>hive.merge.mapfiles: 将在仅使用map的作业结束时合并小文件</li>
<li>hive.merge.mapredfiles: 在MapReduce作业结束时合并小文件</li>
<li>hive.merge.size.per。定义作业结束时合并文件的大小</li>
<li>hive.merge.smallfiles.avgsize: 触发文件合并的阈值</li>
</ul>


<h2>作业优化</h2>

<p>作业优化主要关注运行模式、JVM 重用、作业并行和Join查询优化等方面。其中还会介绍下执行引擎和优化器相关的概念。</p>

<h3>运行模式</h3>

<p>大多数的Hive查询是需要hadoop集群来处理大数据的，不过，有时Hive的输入数据量是非常小的。在这种情况下，为查询执行任务的时间消耗可能会比实际job的执行时间要多的多，因此hive0.7版本后Hive开始支持任务执行选择本地模式(local mode)，即任务提交到本地机器处理。如此一来，对数据量比较小的操作，就可以在本地执行，这样要比提交任务到集群执行效率要快很多。本地模式的配置命令如下：</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='sql'><span class='line'><span class="k">set</span> <span class="n">hive</span><span class="p">.</span><span class="k">exec</span><span class="p">.</span><span class="k">mode</span><span class="p">.</span><span class="k">local</span><span class="p">.</span><span class="n">auto</span><span class="o">=</span><span class="k">true</span><span class="p">;</span>
</span></code></pre></td></tr></table></div></figure>


<p>需要注意是的执行本地模式需要满足几个条件：</p>

<ul>
<li>作业的总输入大小低于：hive.exec.mode.local.auto.inputbytes.max，默认为128MB</li>
<li>map任务的总数小于：hive.exec.mode.local.auto.tasks.max，默认为4</li>
<li>所需的reduce任务总数为1或0。</li>
</ul>


<h3>JVM 重用</h3>

<p>默认情况下，Hadoop为每个map或reduce作业启动一个新的JVM，并行运行map或reduce任务。当在map或reduce作业是只运行几秒钟的轻量级作业时，JVM启动过程可能会带来很大的开销。Hadoop有一个重用JVM的选项，通过共享JVM来串行运行mapper/reducer，而不是并行运行。JVM重用适用于映射或减少同一作业中的任务。来自不同作业的任务总是在单独的JVM中运行。为了支持重用，我们可以使用以下属性为JVM重用设置单个作业的最大任务数。它的默认值是1。如果设置为-1，则没有限制:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='sql'><span class='line'><span class="k">SET</span> <span class="n">mapreduce</span><span class="p">.</span><span class="n">job</span><span class="p">.</span><span class="n">jvm</span><span class="p">.</span><span class="n">numtasks</span><span class="o">=</span><span class="mi">5</span><span class="p">;</span>
</span></code></pre></td></tr></table></div></figure>


<h3>作业并行</h3>

<p>Hive查询通常被转换成许多按默认顺序执行的阶段。这些阶段并不总是相互依赖的。相反，它们可以并行运行，以减少整个作业的运行时间。我们可以通过以下设置启用此功能，并设置并行运行作业的预期数量。并行执行将提高集群利用率。如果集群的利用率已经非常高，那么并行执行对总体性能没有多大帮助。</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class='sql'><span class='line'><span class="k">SET</span> <span class="n">hive</span><span class="p">.</span><span class="k">exec</span><span class="p">.</span><span class="n">parallel</span><span class="o">=</span><span class="k">true</span><span class="p">;</span>
</span><span class='line'><span class="k">SET</span> <span class="n">hive</span><span class="p">.</span><span class="k">exec</span><span class="p">.</span><span class="n">parallel</span><span class="p">.</span><span class="n">thread</span><span class="p">.</span><span class="nb">number</span><span class="o">=</span><span class="mi">16</span><span class="p">;</span>
</span></code></pre></td></tr></table></div></figure>


<h3>Join查询优化</h3>

<p>Hive提供的多种类型的Join，每种Join类型的优化不太一样。</p>

<h4>Reduce端Join</h4>

<p>其实我们经常用的就是这种Join, 对于这种Join类型，我们需要确保数据量大的表位于Join的最右边。</p>

<h4>Map join</h4>

<p>当其中一个用于Join的表小到足以完全装入内存时，使用Map Join的速度会更快，缺点就是受表大小的限制。设置Map Join的相关命令如下：</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class='sql'><span class='line'><span class="k">SET</span> <span class="n">hive</span><span class="p">.</span><span class="n">auto</span><span class="p">.</span><span class="k">convert</span><span class="p">.</span><span class="k">join</span><span class="o">=</span><span class="k">true</span><span class="p">;</span>
</span><span class='line'><span class="k">SET</span> <span class="n">hive</span><span class="p">.</span><span class="n">mapjoin</span><span class="p">.</span><span class="n">smalltable</span><span class="p">.</span><span class="n">filesize</span><span class="o">=</span><span class="mi">600000000</span><span class="p">;</span>
</span><span class='line'><span class="k">SET</span> <span class="n">hive</span><span class="p">.</span><span class="n">auto</span><span class="p">.</span><span class="k">convert</span><span class="p">.</span><span class="k">join</span><span class="p">.</span><span class="n">noconditionaltask</span><span class="o">=</span><span class="k">true</span><span class="p">;</span>
</span><span class='line'><span class="k">SET</span> <span class="n">hive</span><span class="p">.</span><span class="n">auto</span><span class="p">.</span><span class="k">convert</span><span class="p">.</span><span class="k">join</span><span class="p">.</span><span class="n">noconditionaltask</span><span class="p">.</span><span class="k">size</span><span class="o">=</span><span class="mi">10000000</span><span class="p">;</span> 
</span></code></pre></td></tr></table></div></figure>


<p>启用Join类型自动转换后，Hive将自动检查较小的表文件大小是否大于 Hive .mapjoin.smalltable 指定的值。如果文件大小小于此阈值，则尝试将Join的类型转换为Map Join。</p>

<h4>Bucket Map Join</h4>

<p>和Map Join类似，只不过用于分桶表而已。需要注意的是：在 Bucket Map Join中，所有连接表都必须是桶表和桶列上的连接。此外，较大表中的桶号必须是较小表中的桶号的倍数。</p>

<h3>Skew Join</h3>

<p>在处理分布高度不均匀的数据时，数据倾斜可能以这样一种方式发生，即少数计算节点必须处理大量计算。如果发生数据倾斜，则通知Hive进行适当的优化:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class='sql'><span class='line'><span class="k">SET</span> <span class="n">hive</span><span class="p">.</span><span class="n">optimize</span><span class="p">.</span><span class="n">skewjoin</span><span class="o">=</span><span class="k">true</span><span class="p">;</span>   <span class="c1">--If there is data skew in join, set it to true. Default is false.</span>
</span><span class='line'><span class="k">SET</span> <span class="n">hive</span><span class="p">.</span><span class="n">skewjoin</span><span class="p">.</span><span class="k">key</span><span class="o">=</span><span class="mi">100000</span><span class="p">;</span>      <span class="c1">--This is the default value. If the number of key is bigger than --this, the new keys will send to the other unused reducers. </span>
</span></code></pre></td></tr></table></div></figure>


<h3>任务执行引擎</h3>

<p>Hive支持在不同的引擎上运行作业。引擎的选择也会影响整体性能。然而，与其他设置相比，这是一个更大的变化。可选的执行引擎包括mr、spark、tez</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='sql'><span class='line'><span class="k">SET</span> <span class="n">hive</span><span class="p">.</span><span class="n">execution</span><span class="p">.</span><span class="n">engine</span><span class="o">=&lt;</span><span class="n">engine</span><span class="o">&gt;</span><span class="p">;</span> <span class="c1">-- &lt;engine&gt; = mr|tez|spark</span>
</span></code></pre></td></tr></table></div></figure>


<h3>优化器</h3>

<p>与关系数据库类似，Hive在提交最终执行之前会生成并优化每个查询的逻辑和物理执行计划。目前Hive中存在两种主要的优化器：即向量化优化(Vectorization optimization)和基于成本的优化(Cost-based optimization)。</p>

<h4>Vectorization optimization</h4>

<p>应用场景主要是在同时处理大量数据时，而不是逐行处理数据，详细内容参考<a href="https://cwiki.apache.org/confluence/display/Hive/Vectorized+Query+Execution%EF%BC%8C%E5%BC%80%E5%90%AF%E7%9A%84%E9%85%8D%E7%BD%AE%E5%A6%82%E4%B8%8B">https://cwiki.apache.org/confluence/display/Hive/Vectorized+Query+Execution%EF%BC%8C%E5%BC%80%E5%90%AF%E7%9A%84%E9%85%8D%E7%BD%AE%E5%A6%82%E4%B8%8B</a></p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='sql'><span class='line'><span class="k">SET</span> <span class="n">hive</span><span class="p">.</span><span class="n">vectorized</span><span class="p">.</span><span class="n">execution</span><span class="p">.</span><span class="n">enabled</span><span class="o">=</span><span class="k">true</span><span class="p">;</span>  
</span></code></pre></td></tr></table></div></figure>


<h4>Cost-based optimization(CBO)</h4>

<p>CBO是通过<a href="https://calcite.apache.org/">Apache Calcite</a>实现的。Hive CBO通过检查查询成本(由ANALYZE语句或metastore本身收集)来生成高效的执行计划，最终减少查询执行时间和资源利用率。要使用CBO，设置以下属性:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class='sql'><span class='line'><span class="k">SET</span> <span class="n">hive</span><span class="p">.</span><span class="n">cbo</span><span class="p">.</span><span class="n">enable</span><span class="o">=</span><span class="k">true</span><span class="p">;</span> <span class="c1">-- default true after v0.14.0</span>
</span><span class='line'><span class="k">SET</span> <span class="n">hive</span><span class="p">.</span><span class="n">compute</span><span class="p">.</span><span class="n">query</span><span class="p">.</span><span class="k">using</span><span class="p">.</span><span class="n">stats</span><span class="o">=</span><span class="k">true</span><span class="p">;</span> <span class="c1">-- default false </span>
</span><span class='line'><span class="k">SET</span> <span class="n">hive</span><span class="p">.</span><span class="n">stats</span><span class="p">.</span><span class="k">fetch</span><span class="p">.</span><span class="k">column</span><span class="p">.</span><span class="n">stats</span><span class="o">=</span><span class="k">true</span><span class="p">;</span> <span class="c1">-- default false </span>
</span><span class='line'><span class="k">SET</span> <span class="n">hive</span><span class="p">.</span><span class="n">stats</span><span class="p">.</span><span class="k">fetch</span><span class="p">.</span><span class="n">partition</span><span class="p">.</span><span class="n">stats</span><span class="o">=</span><span class="k">true</span><span class="p">;</span> <span class="c1">-- default true</span>
</span></code></pre></td></tr></table></div></figure>


<p class='post-footer'>
            显示信息
            <a href='http://untitled-life.github.io/blog/2019/03/10/the-guide-of-hive-performance-optimization/'>http://untitled-life.github.io/blog/2019/03/10/the-guide-of-hive-performance-optimization/</a><br/>
            written by <a href='http://untitled-life.github.io'>Mike Cao</a>
            &nbsp;posted at <a href='http://untitled-life.github.io'>http://untitled-life.github.io</a>
            </p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Hive MetaStore 数据库表结构]]></title>
    <link href="http://untitled-life.github.io/blog/2019/03/10/the-table-structure-and-relation-in-hive-metastore/"/>
    <updated>2019-03-10T15:50:57+08:00</updated>
    <id>http://untitled-life.github.io/blog/2019/03/10/the-table-structure-and-relation-in-hive-metastore</id>
    <content type="html"><![CDATA[<blockquote><p>他的坐着，其实是奔波，他的热闹，其实是孤独，他，和他的政治对少们，所开的车，没有”R”挡，更缺空挡。 &ndash;龙应台 《目送》</p></blockquote>

<!-- more -->


<p><img src="http://untitled-life.github.io/images/post/HiveMetaStore.jpg" alt="HiveMetaStore" /></p>

<p class='post-footer'>
            显示信息
            <a href='http://untitled-life.github.io/blog/2019/03/10/the-table-structure-and-relation-in-hive-metastore/'>http://untitled-life.github.io/blog/2019/03/10/the-table-structure-and-relation-in-hive-metastore/</a><br/>
            written by <a href='http://untitled-life.github.io'>Mike Cao</a>
            &nbsp;posted at <a href='http://untitled-life.github.io'>http://untitled-life.github.io</a>
            </p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Apache Flink中保存点和检查点之间的差异(译文)]]></title>
    <link href="http://untitled-life.github.io/blog/2019/03/09/3-differences-between-savepoints-and-checkpoints-in-apache-flink/"/>
    <updated>2019-03-09T19:48:36+08:00</updated>
    <id>http://untitled-life.github.io/blog/2019/03/09/3-differences-between-savepoints-and-checkpoints-in-apache-flink</id>
    <content type="html"><![CDATA[<blockquote><p>我相信，如果你会看见敌人的伤口，你就不会拿起枪来对着他。 &ndash;龙应台 《谁欠了她们的人生》</p></blockquote>

<!-- more -->


<p>原文地址：<a href="https://www.da-platform.com/blog/differences-between-savepoints-and-checkpoints-in-flink">https://www.da-platform.com/blog/differences-between-savepoints-and-checkpoints-in-flink</a></p>

<p>在Apache Flink中，经常有人会把Savepoints和Checkpoints搞混淆，今天我们就给大家分别解释下这两个东西以及他们之间的差异。</p>

<h1>保存点和检查点分别是什么?</h1>

<p>Apache Flink保存点是一种允许对整个流应用程序进行“时间点”快照的特性。此快照包含关于您在输入中的位置的信息，以及关于源的所有位置和整个应用程序状态的信息。我们可以在不停止应用程序的情况下通过使用Chandy-Lamport算法的变体获得整个状态的一致快照。保存点包含两个主要元素:</p>

<ol>
<li>首先，保存点包括一个目录，目录中包含(通常较大的)二进制文件，这些文件表示流应用程序在检查点/保存点处的整个状态</li>
<li>一个(相对较小的)元数据文件，它包含指向保存点的所有文件的指针(路径)，这些文件存储在所选的分布式文件系统或数据存储中。</li>
</ol>


<p>请阅读我们之前的博客文章，了解<a href="https://www.da-platform.com/blog/turning-back-time-savepoints">如何在流应用程序中启用保存点的详细步骤</a>。</p>

<p>上面关于保存点的所有内容听起来与我们在前面的一篇文章中解释的Apache Flink中的检查点非常相似。检查点是Apache Flink用于从故障中恢复的内部机制，包括应用程序状态的副本和输入的读取位置。在失败的情况下，Flink通过从检查点加载应用程序状态并继续从恢复的读取位置恢复应用程序，就像什么都没有发生一样。</p>

<h1>Apache Flink中保存点和检查点之间的3个差异</h1>

<p>检查点和保存点是Apache Flink作为流处理框架非常独特的两个特性。保存点和检查点在它们的实现中可能看起来很相似，但是，这两个特性在以下3个方面有所不同:</p>

<ol>
<li>目的:从概念上讲，Flink的保存点不同于检查点，就像备份不同于传统数据库系统中的恢复日志一样。检查点的主要目标是充当Apache Flink中的恢复机制，确保一个容错处理框架能够从潜在的作业失败中恢复。相反，Savepoints的主要目标是作为在用户手动备份和恢复活动之后重新启动、继续或重新打开暂停的应用程序的方式。</li>
<li>实现:检查点和保存点的实现是不同的。检查点被设计成轻量级和快速的。例如，使用RocksDB状态后端的增量检查点使用RocksDB的内部格式，而不是Flink的本机格式。这用于加快RocksDB的检查点进程，使其成为更轻量级检查点机制的第一个实例。相反，保存点的设计目的是更加关注数据的可移植性，并支持对作业所做的任何更改，这些更改会使生成和恢复保存点的成本稍微高一些。</li>
<li>生命周期:检查点本质上是自动的、周期性的。它们由Flink自动地、定期地创建和删除，不需要任何用户交互，以确保在意外的作业失败时能够完全恢复。相反，保存点是由用户手动拥有和管理的(即它们是计划、创建和删除的)。</li>
</ol>


<p><img src="http://untitled-life.github.io/images/post/3-diff-1.jpeg" alt="3-diff-1" /></p>

<h1>什么时候在流应用程序中使用保存点?</h1>

<p>尽管流处理应用程序处理连续生成的数据(数据“在动”)，但是在某些情况下，应用程序可能需要重新处理以前处理过的数据。Apache Flink中的保存点允许您在以下情况下这样做:</p>

<ol>
<li>部署流应用程序的更新版本，包括新特性、bug修复或更好的机器学习模型。</li>
<li>为应用程序引入A/B测试，使用相同的源数据流测试程序的不同版本，在不牺牲先前状态的情况下从相同的时间点开始测试。</li>
<li>在需要更多资源的情况下重新调整应用程序。</li>
<li>将流应用程序迁移到新的Apache Flink版本，或者将应用程序升级到不同的集群。</li>
</ol>


<h1>结论</h1>

<p>Apache Flink检查站和保存点是两个不同的特性,适用于不同的需求,以确保一致性、容错和确保应用程序状态保存在意想不到的工作失败(检查点)以及在升级的情况下,bug修复、迁移或A / B测试(使用保存点)。这两个特性结合在一起，可以在不同的实例中确保应用程序的状态在不同的场景和环境中保持不变。</p>

<p class='post-footer'>
            显示信息
            <a href='http://untitled-life.github.io/blog/2019/03/09/3-differences-between-savepoints-and-checkpoints-in-apache-flink/'>http://untitled-life.github.io/blog/2019/03/09/3-differences-between-savepoints-and-checkpoints-in-apache-flink/</a><br/>
            written by <a href='http://untitled-life.github.io'>Mike Cao</a>
            &nbsp;posted at <a href='http://untitled-life.github.io'>http://untitled-life.github.io</a>
            </p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[开发Flink应用程序的5个起步步骤]]></title>
    <link href="http://untitled-life.github.io/blog/2019/03/09/5-baby-steps-to-develop-a-flink-application/"/>
    <updated>2019-03-09T19:39:10+08:00</updated>
    <id>http://untitled-life.github.io/blog/2019/03/09/5-baby-steps-to-develop-a-flink-application</id>
    <content type="html"><![CDATA[<blockquote><p>一路上，两个人都很忙碌。是这样的，妈妈必须做导游，给安安介绍这个世界，安安是新来的。而妈妈漏掉的东西，安安得指出来，提醒他。 &ndash;龙应台 《孩子你慢慢来》</p></blockquote>

<!-- more -->


<p>原文地址：<a href="https://www.da-platform.com/blog/5-steps-flink-application-development">https://www.da-platform.com/blog/5-steps-flink-application-development</a></p>

<p>使Flink运转起来非常简单。在这篇文章中，我们将介绍5个起步步骤，指导您在本地环境搭建第一个可以运行的Flink应用程序。我们从讨论软件需求开始，并指出一些帮助您理解框架功能的培训资源。如果您喜欢从头开始，我们还将向您展示如何引导应用程序!这个快速概述将使您能够几乎不耗费任何时间的情况下启动Flink应用程序。</p>

<h1>1. 软件需求</h1>

<p>您可以在Linux、Mac OS X和Windows上开发和执行Flink应用程序。因为社区中的大多数开发人员都是在基于unix的设置中操作的，所以这个环境包含了最丰富的工具支持。首先，真正的需求是安装Java开发工具包(JDK) 8(或更高版本)——无论您是要使用Java还是Scala进行开发。虽然开发Flink应用程序并不是严格要求的，但我们建议您也在您的计算机上安装以下软件:</p>

<ul>
<li> Apache Maven 3. x。在我们的介绍培训中，大多数示例都使用Maven实现构建自动化。此外，Flink还提供了Maven原型来引导新的Flink Maven项目。</li>
<li> 用于Java和/或Scala开发的IDE。特别是对于Scala，我们推荐使用IntelliJ，因为它对Maven和易于安装的Scala插件提供了开箱即用的支持。对于Java, Eclipse或Netbeans也能正常工作。</li>
</ul>


<h1>2. 为你准备的训练材料</h1>

<p>如果您喜欢从头开始您自己的项目，那么您可以直接跳到下一个步骤。否则，data Artisans将提供一系列的培训练习，帮助您熟悉Flink的操作方式，并随着时间的推移使您轻松掌握时间和状态管理等更复杂的主题。这些练习(和可能的解决方案)可以在GitHub上找到，所以您可以轻松地克隆项目并使用Maven来构建它:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'>git clone https://github.com/dataArtisans/flink-training-exercises.git
</span><span class='line'><span class="nb">cd </span>flink-training-exercises
</span><span class='line'>mvn clean package
</span></code></pre></td></tr></table></div></figure>


<p>
这需要几分钟，具体时间取决于连接的速度——Maven将下载所有必需的依赖项。如果一切按计划进行，构建成功，那么您就走上了正确的轨道!</p>

<h1>3.引导您自己的Flink项目</h1>

<p>当然，从现有项目开始是使用Flink进行应用程序开发的最简单方法。但是如果您想在某个时候从头开始创建自己的项目呢?Flink提供Maven原型来为Java和Scala应用程序生成Maven项目。例如，要创建一个quickstart Java项目作为Flink应用程序的基础，请运行以下命令:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'>mvn archetype:generate <span class="se">\</span>
</span><span class='line'>-DarchetypeGroupId<span class="o">=</span>org.apache.flink <span class="se">\</span>
</span><span class='line'>-DarchetypeArtifactId<span class="o">=</span>flink-quickstart-java <span class="se">\</span>
</span><span class='line'>-DarchetypeVersion<span class="o">=</span>1.7.0
</span></code></pre></td></tr></table></div></figure>


<p>上面的命令为Flink 1.7.0生成一个Maven项目，其中包含两个类:StreamingJob和BatchJob;它们分别为流和批处理Flink程序提供了基本框架。您可以调整参数以匹配您的版本和命名首选项!我们建议您将该项目导入您选择的IDE中，以便开发一个可运行的示例。如果您正在与灵感作斗争，您可以在<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.7/tutorials/datastream_api.html#writing-a-flink-program">Flink文档</a>中获得一些提示。</p>

<h1>4. 运行和调试您的第一个Flink应用程序</h1>

<p>尽管Flink是一个分布式数据处理系统，但在本地环境中使用您的机器更容易启动它。在典型的设置中，您会让master (JobManager)和workers (taskmanager)在不同的机器上作为独立的JVM进程运行;但是Flink还包括一种模式，允许您像多线程进程一样在相同的JVM中执行应用程序。这种模式允许您在IDE中轻松地开发、调试和执行Flink应用程序，就像任何其他Java或Scala项目一样。要启动应用程序，只需像往常一样运行main()方法!</p>

<p><img src="http://untitled-life.github.io/images/post/flink-application.png" alt="flink-application" /></p>

<h1>5. 监视您的Flink应用程序</h1>

<p>如果您想知道正在运行的应用程序的底层发生了什么，那么您可以轻松地使用捆绑在Flink中的web接口来可视化和监视它。要为本地开发启用该接口，您需要向POM文件添加一个新的依赖项:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class='xml'><span class='line'><span class="nt">&lt;dependency&gt;</span>
</span><span class='line'>    <span class="nt">&lt;groupId&gt;</span>org.apache.flink<span class="nt">&lt;/groupId&gt;</span>
</span><span class='line'>    <span class="nt">&lt;artifactId&gt;</span>flink-runtime-web_2.11<span class="nt">&lt;/artifactId&gt;</span>
</span><span class='line'>    <span class="nt">&lt;version&gt;</span>${flink.version}<span class="nt">&lt;/version&gt;</span>
</span><span class='line'><span class="nt">&lt;/dependency&gt;</span>
</span></code></pre></td></tr></table></div></figure>


<p>并显式创建具有所需配置的本地执行环境:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
</pre></td><td class='code'><pre><code class='java'><span class='line'><span class="kn">import</span> <span class="nn">org.apache.flink.configuration.ConfigConstants</span><span class="o">;</span>
</span><span class='line'><span class="o">...</span>
</span><span class='line'><span class="n">Configuration</span> <span class="n">config</span> <span class="o">=</span> <span class="k">new</span> <span class="nf">Configuration</span><span class="o">();</span>
</span><span class='line'><span class="n">config</span><span class="o">.</span><span class="na">setBoolean</span><span class="o">(</span><span class="n">ConfigConstants</span><span class="o">.</span><span class="na">LOCAL_START_WEBSERVER</span><span class="o">,</span> <span class="kc">true</span><span class="o">);</span>
</span><span class='line'><span class="n">StreamExecutionEnvironment</span> <span class="n">env</span> <span class="o">=</span> <span class="n">StreamExecutionEnvironment</span><span class="o">.</span><span class="na">createLocalEnvironment</span><span class="o">(</span><span class="n">config</span><span class="o">);</span>
</span><span class='line'><span class="o">...</span>
</span></code></pre></td></tr></table></div></figure>


<p>在程序运行之后，Flink web接口现在应该可以在<a href="http://localhost:8081">本地环境</a> 下使用，看起来有点像下面的图像。记住，更改POM文件后要更新Maven项目，如果您的IDE中没有启用自动导入——这可能会在您第一次尝试访问web界面时导致错误。</p>

<p><img src="http://untitled-life.github.io/images/post/flink-application-1.png" alt="flink-application-1" /></p>

<p><img src="http://untitled-life.github.io/images/post/flink-application-2.png" alt="flink-application-2" /></p>

<p>恭喜你!您刚刚在IDE中运行了第一个Flink应用程序。Flink文档中的教程更进一步，展示了如何设置本地Flink集群，并像向远程集群提交作业一样提交作业。</p>

<p>我们鼓励您浏览Flink文档以获得进一步的支持或更详细的信息。如果您在任何时候感到困惑或有任何问题:我们的社区在Stack Overflow上非常活跃，您也可以使用邮件列表联系开发人员。</p>

<p>  <p class='post-footer'>
            显示信息
            <a href='http://untitled-life.github.io/blog/2019/03/09/5-baby-steps-to-develop-a-flink-application/'><a href="http://untitled-life.github.io/blog/2019/03/09/5-baby-steps-to-develop-a-flink-application/">http://untitled-life.github.io/blog/2019/03/09/5-baby-steps-to-develop-a-flink-application/</a></a><br/>
            written by <a href='http://untitled-life.github.io'>Mike Cao</a>
            &nbsp;posted at <a href='http://untitled-life.github.io'><a href="http://untitled-life.github.io">http://untitled-life.github.io</a></a>
            </p></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Apache Flink中事件时间的介绍]]></title>
    <link href="http://untitled-life.github.io/blog/2019/03/09/stream-processing-an-introduction-to-event-time-in-apache-flink/"/>
    <updated>2019-03-09T19:30:43+08:00</updated>
    <id>http://untitled-life.github.io/blog/2019/03/09/stream-processing-an-introduction-to-event-time-in-apache-flink</id>
    <content type="html"><![CDATA[<blockquote><p>一个刽子手的责任，在看准了头颈的分寸，一刀霍下，让鲜血喷起，人头落地。被杀的人究竟有罪或者冤枉，不是刽子手的事情。甚至于即使他明明知道眼前跪着的人其实无辜，也没有人会指责刽子手为凶手。我们可以说，刽子手只是奉命行事，做一天和尚当然就得撞一天钟。应该负责的，是判官；或者，是那个不健全的审判制度；再抽象一点，我们不妨这么说，错在那个封建的社会。 &ndash;龙应台 《人在欧洲》</p></blockquote>

<!-- more -->


<p>原文地址：<a href="https://www.da-platform.com/blog/stream-processing-introduction-event-time-apache-flink">https://www.da-platform.com/blog/stream-processing-introduction-event-time-apache-flink</a></p>

<p>Apache Flink在有状态流处理中支持多个时间概念。本文主要关注Apache Flink对事件时间的支持。在下面的小节中，我们将定义Apache Flink的事件时间是什么，我们将研究流处理框架中不同的时间概念，并描述Flink如何使用水印来度量事件时间的进度。</p>

<h1>流处理中的事件时间</h1>

<p>顾名思义，Apache Flink中的事件时间是在生产源上生成每个单独事件的时间。在标准场景中，从不同生产者收集的事件(即与移动应用程序、金融交易、应用程序和机器日志、传感器事件等的交互)在其元数据中存储一个时间元素。这个时间表示特定事件在生产源中生成的时间。当应用程序逻辑需要基于事件生成时间的数据进行处理或计算时，有状态流处理和实时计算将使用事件时间。</p>

<h1>事件时间、处理时间和摄入时间的不同之处</h1>

<p>为了有效地描述Flink如何在流处理中管理不同的时间概念，让我们设想一个场景(如下所示)，在这个场景中，一个手机游戏玩家坐地铁去上班。玩家在平台上启动手机游戏，生成的事件被发送到Flink操作符。但是，当玩家在地铁车厢内时，wifi连接丢失，数据存储在移动设备上，没有传输到流处理系统。一旦用户重新上线，设备就会将剩余的数据发送到流媒体基础设施，然后步行到办公室。</p>

<p>source/images/post/</p>

<p><img src="http://untitled-life.github.io/images/post/Event-time-FFT-Icon-london-underground.png" alt="Event-time-FFT-Icon-london-underground" /></p>

<p>在上面的场景中，移动游戏提供商可能使用基于Apache Flink的实时消息应用程序来分析游戏上的用户活动，并通过推送通知或消息提供实时优惠。这个应用程序可以基于不同的时间概念来处理传入的数据:我们前面描述的事件时间、处理时间以及我们在下一节中描述的摄入时间。</p>

<p><strong>处理时间</strong>是指在流处理应用程序中执行特定操作的机器时间。当流处理应用程序使用处理时间时，它使用机器的时钟来运行任何操作。一个5小时处理时间窗口将对完整的5个小时时间间隔内到达操作符的所有事件进行合并。处理时间很简单，不需要流和机器之间的协调，而且它提供了最好的性能和最低的延迟。然而，在分布式系统和上面描述的示例这样的场景中，使用处理时间进行计算可能并不总是最合适的，因为事件将异步或无序地到达操作符。</p>

<p><img src="http://untitled-life.github.io/images/post/Event-time-FFT-Icon-Event-time.png" alt="Event-time-FFT-Icon-Event-time" /></p>

<p><strong>摄取时间</strong>是事件到达流处理应用程序的时间。摄入时间是任何处理延迟及其潜在波动的原因，并且在处理系统“消费”消息时标记时间戳。然后将此时间戳与任何基于时间的操作的特定事件相关联。与处理时间相比，摄入时间提供了更可预测的结果，尽管仍然不是100%准确，因为它不能处理原始时间和无序数据。</p>

<p>与处理时间相比，摄入时间提供了更可预测的结果，因为它在源操作符上一次性分配了一个稳定的时间戳，这使得所有与时间相关的操作都返回到所分配的时间戳。相反，随着处理时间的增加，每个操作符可以根据本地系统时钟将记录分配到不同的窗口。</p>

<p>Apache Flink通过在源上分配自动时间戳和生成自动水印(参见下面)，以类似于事件时间的方式处理摄入时间。</p>

<p>Apache Flink中的默认时间是处理时间。但是，如果应用程序异步工作或与分布式系统一起工作，可能需要将Flink设置为事件时间。相对其他两种时间，事件时间将提供最准确的结果，因为，正如我们前面解释的，它在事件或“消息”源生成时分配时间戳。然后，这个时间戳在所有与时间相关的操作中跟踪事件。事件时间由Apache Flink中的水印处理和支持，我们将在下面介绍。</p>

<p>在Apache Flink中，处理时间可以通过以下代码更新为事件时间:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='java'><span class='line'><span class="n">env</span><span class="o">.</span><span class="na">setStreamTimeCharacteristic</span><span class="o">(</span><span class="n">TimeCharacteristic</span><span class="o">.</span><span class="na">EventTime</span><span class="o">)</span>
</span></code></pre></td></tr></table></div></figure>


<h1>Flink中的水印和事件时间</h1>

<p>水印是Apache Flink在事件时间中度量进展的机制。水印是数据流的一部分，其携带了一个时间戳t。在数据流中，一个水印(t)宣称事件时间已达到时间t,意思就是不应该有更多的元素，其流时间戳t &lsquo; &lt; = t(即事件时间戳大于或等于水印)。水印对于无序流和不按时间戳排序的异步操作非常重要。通常，水印是一种声明，表示流中的某个特定点，即某个时间戳之前的所有事件都应该已经到达。一旦水印到达操作符，操作符可以将其内部事件时间时钟推进到水印的值。举个例子，在我们使用小时窗口操作的情况下，水印将允许Flink了解特定的小时窗口何时超过小时，以便操作符可以关闭正在运行的现有窗口。</p>

<p>我们之前的<a href="https://www.da-platform.com/blog/watermarks-in-apache-flink-made-easy">一篇文章</a>更详细地描述了这个特性，并在使用Apache Flink处理有状态流的水印时提供了一些有用的观察。</p>

<p>流处理应用程序所选择的时间支持将取决于应用程序的需求和特定的系统特性。为流应用程序选择正确的时间概念至关重要，因为这将最终影响您以后的结果和操作。Apache Flink的设计目的是提供灵活性，并根据应用程序的需求支持不同的时间概念。有关调试窗口和事件时间的更多信息，请阅读<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.6/monitoring/debugging_event_time.html">Flink文档</a>。</p>

<p class='post-footer'>
            显示信息
            <a href='http://untitled-life.github.io/blog/2019/03/09/stream-processing-an-introduction-to-event-time-in-apache-flink/'>http://untitled-life.github.io/blog/2019/03/09/stream-processing-an-introduction-to-event-time-in-apache-flink/</a><br/>
            written by <a href='http://untitled-life.github.io'>Mike Cao</a>
            &nbsp;posted at <a href='http://untitled-life.github.io'>http://untitled-life.github.io</a>
            </p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[4 Characteristics of Timers in Apache Flink to Keep in Mind]]></title>
    <link href="http://untitled-life.github.io/blog/2019/03/09/4-characteristics-of-timers-in-apache-flink-to-keep-in-mind/"/>
    <updated>2019-03-09T16:23:32+08:00</updated>
    <id>http://untitled-life.github.io/blog/2019/03/09/4-characteristics-of-timers-in-apache-flink-to-keep-in-mind</id>
    <content type="html"><![CDATA[<blockquote><p>虽然心中有爱，但是爱，冻结在经年累月的沉默里，好像藏着一个疼痛的伤口，没有纱布可绑。 &ndash;龙应台 《亲爱的安德烈》</p></blockquote>

<!-- more -->


<p>原文地址：<a href="https://www.da-platform.com/blog/4-characteristics-of-timers-in-apache-flink">https://www.da-platform.com/blog/4-characteristics-of-timers-in-apache-flink</a></p>

<p>本文描述了在Apache Flink中使用计时器的一些基本概念和注意事项。开发人员可以使用Flink的ProcessFunction操作符注册自己的计时器，以便可以访问流应用程序的一些基本构建块，例如：</p>

<ul>
<li>事件(流元素)</li>
<li>状态(容错、一致、仅在键控流上)</li>
<li>计时器(事件时间和处理时间，仅在键控流上)</li>
</ul>


<p>有关Apache Flink ProcessFunction的更多信息，我们建议阅读<a href="https://ci.apache.org/projects/flink/flink-docs-stable/dev/stream/operators/process_function.html">Apache Flink 1.7文档</a>以获得更多的说明和指导。</p>

<h1>Apache Flink中的计时器是什么?</h1>

<p>计时器使Flink流应用程序具有响应性，并可以对处理和事件时间的更改做出适配。我们之前的<a href="https://www.da-platform.com/blog/stream-processing-introduction-event-time-apache-flink">一篇文章</a>更详细地介绍了Apache Flink中时间的其他概念以及处理、摄入和事件时间之间的差异。当使用计时器处理事件流时，每当调用<strong>processElement(…)</strong>时，都会传递一个上下文对象，允许您访问元素的事件时间戳和TimerService。然后可以使用TimerService为将来的事件/处理时间瞬间注册回调。通过这样做，一旦到达计时器的特定时间瞬间，<strong>onTimer(…)</strong>方法将被调用。</p>

<p><strong>onTimer(…)</strong>回调被调用的时间点首先取决于是使用处理时间还是事件时间来注册计时器。特别是:</p>

<ul>
<li>当使用处理时间在Flink应用程序中注册计时器时，当机器的时钟时间达到计时器的时间戳时，将调用onTimer(…)方法。</li>
<li>当使用事件时间在Flink应用程序中注册计时器时，当操作符的水印达到或超过计时器的时间戳时，将调用onTimer(…)方法。</li>
</ul>


<p>与<strong>processElement(…)</strong>方法类似，<strong>onTimer(…)</strong>回调中的状态访问也限定在当前键(其注册计时器的键)。</p>

<p>这里值得注意的是，onTimer(…)和processElement(…)的调用都是同步的，因此在<strong>onTimer(…)</strong>和<strong>processElement(…)</strong>方法中对状态的访问和修改是安全的。</p>

<h1>牢记定时器的4个特点</h1>

<p>在这一段中，我们将讨论Apache Flink中计时器的4个基本特性，在使用它们之前应该记住这些特性。这些是:</p>

<h2>1. 定时器在KeyedStream上注册</h2>

<p>由于计时器是按每个键注册和触发的，所以KeyedStream是Apache Flink中使用计时器的任何操作和函数的先决条件。</p>

<h2>2.计时器会自动删除重复数据</h2>

<p>TimerService会自动删除计时器的重复项，每个键和时间戳最多只能有一个计时器。这意味着，当多个计时器为同一个键或时间戳注册时，onTimer()方法将只调用一次。</p>

<h2>3.定时器具有检查点特性</h2>

<p>计时器由Flink进行检查点，就像任何其他托管状态一样。当Flink从检查点或保存点恢复作业时，每一个注册的计时器应该在恢复操作触发之前被启动。</p>

<h2>4. 计时器可以被删除</h2>

<p>从Flink 1.6开始，计时器可以暂停和删除。如果您使用的Apache Flink版本比Flink 1.5老，那么您可能会遇到一个糟糕的检查点性能，因为有许多计时器无法删除或停止。</p>

<p>您可以使用以下命令停止处理时间定时器:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class='scala'><span class='line'><span class="n">long</span> <span class="n">timestampOfTimerToStop</span> <span class="k">=</span> <span class="o">...</span>
</span><span class='line'><span class="n">ctx</span><span class="o">.</span><span class="n">timerService</span><span class="o">(</span> <span class="o">).</span><span class="n">deleteProcessingTimeTimer</span><span class="o">(</span><span class="n">timestampOfTimerToStop</span><span class="o">);</span>
</span></code></pre></td></tr></table></div></figure>


<p>您还可以通过以下命令停止事件时间计时器:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class='scala'><span class='line'><span class="n">long</span> <span class="n">timestampOfTimerToStop</span> <span class="k">=</span> <span class="o">...</span>
</span><span class='line'><span class="n">ctx</span><span class="o">.</span><span class="n">timerService</span><span class="o">(</span> <span class="o">).</span><span class="n">deleteEventTimeTimer</span><span class="o">(</span><span class="n">timestampOfTimerToStop</span><span class="o">);</span>
</span></code></pre></td></tr></table></div></figure>


<p><img src="http://untitled-life.github.io/images/post/FFT-Timers-in-flink.png" alt="Timers-in-flink" /></p>

<p>这里值得一提的是，如果没有注册具有给定时间戳的计时器，则停止计时器没有效果。</p>

<p class='post-footer'>
            显示信息
            <a href='http://untitled-life.github.io/blog/2019/03/09/4-characteristics-of-timers-in-apache-flink-to-keep-in-mind/'>http://untitled-life.github.io/blog/2019/03/09/4-characteristics-of-timers-in-apache-flink-to-keep-in-mind/</a><br/>
            written by <a href='http://untitled-life.github.io'>Mike Cao</a>
            &nbsp;posted at <a href='http://untitled-life.github.io'>http://untitled-life.github.io</a>
            </p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[4 Steps to Get Your Flink Application Ready for Production]]></title>
    <link href="http://untitled-life.github.io/blog/2019/03/09/4-steps-to-get-your-flink-application-ready-for-production/"/>
    <updated>2019-03-09T16:12:19+08:00</updated>
    <id>http://untitled-life.github.io/blog/2019/03/09/4-steps-to-get-your-flink-application-ready-for-production</id>
    <content type="html"><![CDATA[<blockquote><p>独裁，专制，腐败，不是哪一个主义制度所独有，但是东欧革命狂潮就应该给所有的专制政权，不管它是否什么主义，一个冰冷的警告，暴力，不能持久。 &ndash;龙应台 《百年思索》</p></blockquote>

<!-- more -->


<p>原文地址：<a href="https://www.da-platform.com/blog/4-steps-flink-application-production-ready">https://www.da-platform.com/blog/4-steps-flink-application-production-ready</a></p>

<p>本文将描述使Flink应用程序投入生产的必要配置步骤。在下面的部分中，我们概述了重要的配置参数，而这些参数是工程领导、DevOps和数据工程师将Flink作业引入生产阶段之前需要仔细考虑的。Apache Flink为大多数配置选项提供了开箱即用的默认设置，在许多情况下，这是POC阶段(概念验证)或探索不同api和Flink抽象的良好起点。</p>

<p>然而，将Flink应用程序引入生产环境需要额外的配置，这些配置能够有效地扩展和再次扩展您的应用程序，并使其可用于生产，并与不同的系统需求、Flink版本和连接器兼容，以用于未来的迭代和潜在的升级。</p>

<p>下面，我们将收集一些配置要点，以便在将Flink应用程序投入生产之前进行审查:
<img src="http://untitled-life.github.io/images/post/FFT-4-steps-to-get-your-app-production-ready.png" alt="FFT-4-steps-to-get-your-app-production-ready.png" /></p>

<h2>1. 明确定义Flink运算符的最大并行度</h2>

<p>Flink的键控状态被组织在所谓的键组中，然后这些键组被分发到您的Flink操作符的并行实例中。这是要分发的最小原子单元，因此也会影响Flink应用程序的可伸缩性。每个操作符的键组数对于每个作业只选择一次:手动或默认情况下。默认值将给出大致的操作并行度* 1.5，下界为128，上界为32768。它可以通过setMaxParallelism(int maxParallelism)手动定义每个作业和/或每个操作符。</p>

<p>任何进入生产环境的Flink作业都应该指定最大的并行度。但是，这个值的决定应该经过仔细考虑，因为此时，一旦设置了最大并行度，就不能在稍后的阶段更新它。更改最大并行度的Flink作业只能使用全新的状态从头开始。在更改最大并行度时，从以前的检查点或保存点进行恢复不可用。</p>

<p>建议将最大并行度设置为足以满足应用程序未来对可伸缩性和可用性的需求，同时又相对较低，以避免影响应用程序的总体性能。这是由于这样一个事实:在具有最高并行度的情况下，Flink为其重新伸缩的能力维护某些元数据，这可能会增加Flink应用程序的总体状态大小。</p>

<p>Flink文档提供了关于如何<a href="https://ci.apache.org/projects/flink/flink-docs-stable/ops/state/large_state_tuning.html">使用检查点配置使用大状态的应用程序</a>的额外信息和指导。</p>

<h2>2. 为Flink运算符分配唯一的用户id (uuid)</h2>

<p>对于有状态Flink应用程序，建议为所有运算符分配惟一的用户id (uuid)。这是必要的，因为一些内置Flink运算符(比如windows)是有状态的，而其他运算符可能是无状态的，这使得很难知道哪些内置运算符实际上是有状态的，哪些不是。</p>

<p>Flink运算符的uuid可以使用uid(String uid)方法分配。运算符uuid允许Apache Flink有效地将运算符状态从保存点映射到适当的运算符，这是保存点在Flink应用程序中正常工作所必需的元素。</p>

<h2>3.全面考虑Flink应用程序的状态后端</h2>

<p>在投入生产之前，开发人员和工程领导应该仔细考虑他们的Flink应用程序的状态后端类型，因为Apache Flink目前不支持状态后端互操作性。这使得有必要从保存点恢复状态，用于最初获取保存点的相同状态后端。</p>

<p>在之前的一篇<a href="https://data-artisans.com/blog/stateful-stream-processing-apache-flink-state-backends">博客</a>文章中介绍了Apache Flink中目前支持的3种状态后端之间的差异。</p>

<p>对于生产用例，强烈建议使用RocksDB状态后端，因为这是目前唯一一种支持大状态和异步操作(如快照)的状态后端，这些操作允许在不停止Flink操作的情况下编写快照。另一方面，使用RocksDB状态后端可能会带来性能折衷，因为所有状态访问和检索都需要序列化(和反序列化)才能跨越JNI边界，这可能会影响应用程序的吞吐量(与内存状态后端相比)。</p>

<h2>4. 使作业管理器具有高可用性(HA)</h2>

<p>高可用性(HA)配置确保Flink应用程序中的JobManager组件的潜在故障能够自动恢复，从而最大限度地减少停机时间。JobManager的主要职责是协调Flink部署，比如调度和适当的资源分配。</p>

<p>默认情况下，Flink为每个Flink集群设置一个JobManager实例。这将创建一个单点故障点(SPOF):如果JobManager崩溃，则无法提交任何新程序，并且正在运行的程序会失败。因此，强烈建议为生产环境<a href="https://ci.apache.org/projects/flink/flink-docs-stable/ops/config.html#high-availability-ha">配置高可用性HA</a>。</p>

<p>以上4个步骤遵循社区设置的最佳实践，这些实践允许Flink应用程序在维护状态的同时任意扩展，处理更大容量的数据流和状态，并增加它们的可用性保证——生产用例的特定需求。Apache Flink文档中的<a href="https://ci.apache.org/projects/flink/flink-docs-stable/ops/">部署和操作</a>部分为稳定的Flink操作提供了额外的指导和支持。我们强烈建议在将应用程序转移到生产环境之前，遵循上述步骤并仔细阅读文档。</p>

<p class='post-footer'>
            显示信息
            <a href='http://untitled-life.github.io/blog/2019/03/09/4-steps-to-get-your-flink-application-ready-for-production/'>http://untitled-life.github.io/blog/2019/03/09/4-steps-to-get-your-flink-application-ready-for-production/</a><br/>
            written by <a href='http://untitled-life.github.io'>Mike Cao</a>
            &nbsp;posted at <a href='http://untitled-life.github.io'>http://untitled-life.github.io</a>
            </p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[An Introduction to the Presto]]></title>
    <link href="http://untitled-life.github.io/blog/2019/02/25/an-introduction-to-the-presto/"/>
    <updated>2019-02-25T12:19:25+08:00</updated>
    <id>http://untitled-life.github.io/blog/2019/02/25/an-introduction-to-the-presto</id>
    <content type="html"><![CDATA[<p>斜坡上的杂化野草，谁说不是一草一千秋，一花一世界呢？ &ndash;龙应台 《目送》</p>

<!-- more -->


<h1>先来个背景</h1>

<p>大数据持续火热，需要处理的数据越来越多，在大数据技术的浪潮中，出现了很多优秀的工具和产品，其中最具影响力的非Hadoop莫属，Hadoop提供了存储和计算方案，基本解决了大数据处理遇到的问题。但是也暴露了一些问题，如基于Hadoop的Map-Reduce计算框架，仅适合批量的离线计算，虽然吞吐率可以满足，但计算效率不能满足即席查询Ad-Hoc的性能要求，有了市场痛点，相应的解决痛点的产品就会应声而出，所以Presto就在Facebook诞生了。</p>

<h1>如何描述这货呢</h1>

<p>Presto是一个开源的分布式SQL查询引擎，用于对 GB 到 PB 量级的数据源，进行交互式查询。</p>

<h1>这货能干啥</h1>

<p>Presto可以查询多种平台上的数据，包括Hive、Cassandra、关系数据库甚至专有数据存储，并且可以将多个数据源的数据进行组合查询。Presto的目标客户是那些预期查询响应时间从亚秒到分钟不等的分析师。之前的数据分析工具分为两种：一种是使用昂贵的商业解决方案进行快速分析，另外一种是需要过多硬件的缓慢的“免费”解决方案。Presto提供的方案不用以上两种，但同时具备了他们的优点：分析速度快、免费。</p>

<h1>特点列举</h1>

<table>
<thead>
<tr>
<th> 特点  </th>
<th> 说明  </th>
</tr>
</thead>
<tbody>
<tr>
<td>多数据源   </td>
<td> 可以支持MySQL、PG、Cassandra、Hive、Kafka、JMX等多种数据源 </td>
</tr>
<tr>
<td>扩展性  </td>
<td> 设计非常牛X, 扩展性非常强悍，目测是Apache产品扩展性前三的产品  </td>
</tr>
<tr>
<td>支持SQL查询  </td>
<td>完全支持ANSI SQL, 并带有Presto特有的SQL扩展项   </td>
</tr>
<tr>
<td>流水线   </td>
<td>基于Pipeline模式设计，在海量数据处理过程中，终端用户不用等待所有的数据都处理完毕后才能看到结果，一旦计算开始，结果数据就会一部分一部分的产出，并被终端用户看到   </td>
</tr>
<tr>
<td>混合计算    </td>
<td>针对一种类型的Connector可以配置一个或者多个Catalog，终端用户可以混合多个Catalog进行相关的计算，例如stats和hive的表进行join   </td>
</tr>
<tr>
<td>高性能     </td>
<td>查询性能是Hive MR的10倍以上   </td>
</tr>
</tbody>
</table>


<h1>基本概念</h1>

<h2>服务进程</h2>

<p>Presto中有两种类型的服务进程: Coordinators服务进程和Workers服务进程；</p>

<h3>Coordinators服务进程</h3>

<p>Coordinators服务进程部署于集群中一个独立的节点上，是Presto集群的管理节点。主要的作用包括接受查询请求、解析查询语句、生成查询执行计划、任务调度和Worker进程管理。不仅与Worker进程进行通信从而获得Worker的状态信息，还需要与Client进行通信，接受查询请求等。</p>

<h3>Worker服务进程</h3>

<p>Worker进程主要是执行查询任务，直白说就是真正干活的。一个Presto集群中，存在一个Coordinators节点个多个Worker节点，Coordinators节点是管理节点，Worker节点是工作节点。在每个Worker节点上都会存在至少一个Worker服务进程，该服务进程主要进行数据处理及任务Task的执行。</p>

<h2>连接器(Connector)</h2>

<p>Presto是通过各种连接器(Connector)来访问不同的数据源的，可以将连接器(Connector)当作Presto访问各种数据源的驱动程序。连接器(Connector)是Presto的SPI的实现，允许Presto使用标准API与资源进行交互。
Presto包含多个内置连接器(Connector)，并且允许第三方开发自定义连接器(Connector)，以便Presto能够访问各种数据源中的数据。</p>

<h2>目录(Catalog)</h2>

<p>Presto中的目录(Catalog)类似于MySQL中的一个数据库实例，每个目录(Catalog)都与特定的连接器(Connector)相关联。可以让多个目录(Catalog)使用同一个连接器(Connector)访问相同数据库的两个不同实例。例如，如果您有两个Hive集群，您可以在一个Presto集群中配置两个目录(Catalog)，它们都使用Hive连接器(Connector)，允许您从两个Hive集群(甚至在同一个SQL查询中)查询数据。</p>

<h2>模式(Schema)</h2>

<p>模式(Schema)是组织表的一种方式。类似MySQL中的Database。目录(Catalog)和模式(Schema)一起定义了一组可以查询的表。当使用Presto访问Hive或PG等关系数据库时，模式(Schema)在目标数据库中转换为Database的概念。其他类型的连接器(Connector)可以选择对底层数据源有意义的方式将表组织到模式(Schema)中。</p>

<h2>表(Table)</h2>

<p>表(Table)是一组无序的行，它们被组织成具有类型的命名列。与传统数据库中的Table含义是一样的，从数据源到表(Table)的映射是由连接器(Connector)定义的。</p>

<h1>查询执行模型</h1>

<p>Presto在执行SQL语句时，会将这些语句转换为可在分布式Coordinators和Worker节点上执行的查询。</p>

<p><img src="http://untitled-life.github.io/images/post/query_planner.png" alt="query_planner" /></p>

<h2>语句(Statement)</h2>

<p>其实就是我们输入的SQL语句，这种语句由子句(Clause)、表达式(Expression)、断言(Predicate)组成.</p>

<h2>查询执行(Query)</h2>

<p>当Presto接受一个SQL语句(Statement)后，会解析该SQL语句(Statement)，将其转换成一个查询执行(Query)和相关的查询执行计划。一个查询执行表示可以在Presto集群上运行的查询，是由运行在各个Worker上且各自之间相互关联的阶段(Stage)组成的。这表名在Presto中，语句(Statement)和查询(Query)是两个不同的概念。两者的区别在于：语句(Statement)是用文字表示的SQL执行语句，而查询执行(Query)是由阶段(Stage)、任务(Task)、驱动器(Driver)、分片(Split)、操作符(Operator)和数据源(Data Source)组成，这些组件通过内部联系共同组成了一个查询执行，从而得到SQL语句表述的查询。</p>

<h2>阶段(Stage)</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>Stage: unit of work that does not require shuffling</span></code></pre></td></tr></table></div></figure>


<p>Stage表示查询执行阶段。当Presto运行查询执行(Query)时，Presto会将一个查询执行(Query)拆分成具有层次关系的多个阶段(Stage),一个阶段(Stage)代表查询执行计划的一部分。例如，我们执行一个查询，从Hive的某张表中查询数据并进行一些聚合操作，Presto会创建一个Root Stage, 该Stage聚合其上游Stage的输出数据，然后将结果输出给Coordinator, 并由Coordinator将结果输出给终端用户。
阶段(Stage)是有层级关系的，每个查询执行(Query)都会有一个Root Stage, 该阶段(Stage)用于聚集上游阶段(Stage)的输出数据，并将最终结果反馈给用户。再次强调，阶段(Stage)只是Coordinator用于对查询执行计划进行管理和建模的逻辑概念。阶段(Stage)根据作用可分为以下四类：</p>

<table>
<thead>
<tr>
<th> Stage类型  </th>
<th> 作用  </th>
</tr>
</thead>
<tbody>
<tr>
<td> Coordinator_Only Stage  </td>
<td> 表结构的变更(创建或者更改)  </td>
</tr>
<tr>
<td> Single or Root Stage  </td>
<td> 聚合上游Stage的输出数据，并将最终数据输出给终端用户  </td>
</tr>
<tr>
<td> Fixed Stage  </td>
<td> 接收子Stage产出的数据并在集群中对这些数据进行分布式的聚合或者分组计算  </td>
</tr>
<tr>
<td> Source Stage  </td>
<td> 链接数据源的Stage, 负责从数据源读取数据，同时会对查询执行计划的优化结果完成相应的操作，如断言下发、条件过滤等。  </td>
</tr>
</tbody>
</table>


<h2>任务(Task)</h2>

<p>上面讲到的Stage并不会在Presto集群中实际运行，他仅仅代表针对于一个SQL语句查询执行(Query)中的一部分查询的执行过程，只是用来对查询执行(Query)计划进行管理和建模。Stage在逻辑上被分为一系列的任务(Task), 而这些任务(Task)则是需要实际运行在Presto的各个Worker节点上的。Presto的层次设计非常清晰，一个查询执行(Query)被分解成具有层析关系的多个阶段(Stage),一个阶段(Stage)又被拆分成一系列的任务(Task),每个任务(Task)处理一个或者读个分片(Split)；每个阶段(Stage)被分解成多个任务(Task), 从而可以并行的执行一个阶段(Stage);任务(Task)也采用了相同的机制，一个Task也被分成了多个驱动器(Driver)，从而可以并行的执行一个任务(Task);</p>

<h2>驱动器(Driver)</h2>

<p>任务(Task)包含一个或多个并行驱动器(Driver)。驱动器(Driver)对数据进行操作，并结合操作符(Operator)生成输出，然后由一个任务(Task)聚合输出，然后在另一个阶段(Stage)交付给另一个任务(Task)。驱动器(Driver)是操作符(Operator)实例的序列，或者您可以将驱动器(Driver)看作内存中的操作符(Operator)的物理集合。它是Presto架构中最低的并行级别。</p>

<h2>分片(Split)</h2>

<p>一个分片(Split)其实就是一个大的数据集中的一个小的子集，分布式查询计划的最低级别阶段通过连接器的片段检索数据，而分布式查询计划(Query)的较高级别阶段从其他阶段(Stage)检索数据。当Presto调度查询时，协调器(Coordinator)将查询连接器(Connector)，以获得表可用的所有分片(Split)的列表。协调器(Coordinator)跟踪哪些机器正在运行哪些任务(Task)，以及哪些分片(Split)由哪些任务(Task)处理。</p>

<h2>操作符(Operator)</h2>

<p>一个操作符(Operator)代表对一个分片(Split)的一种操作，例如过滤、转换等。一个操作符(Operator)依次读取一个分片(Split)中的数据，将操作符(Operator)所代表的计算和操作用于分片(Split)的数据上，并产生输出。每个操作符(Operator)均会以页(Page)为最小处理单位分别读取输入数据和产出输出数据。操作符(Operator)每次一会读取一个页(Page)对象，同理也只会产生一个页(Page)对象。
page## 页(Page)
页(Page)是Presto中处理的最小数据单元，一个页(Page)对象包含多个数据Block； 可以将数据Block理解成一个字节数组，存储一个字段的若干行；多个Block横切的一行其实就是一行真实的数据。下图展示了Page和Block的关系</p>

<p><img src="http://untitled-life.github.io/images/post/page_block.png" alt="page_block" /></p>

<h2>交换(Exchange)</h2>

<p>交换(Exchange)用于查询的不同阶段(Stage)的Presto节点之间的数据传输。任务(Task)将数据生成到输出缓冲区中，下游阶段(Stage)通过名为Exchange Client的Exchange从上游阶段(Stage)读取数据。 交换(Exchange)其实就是用于完成具有上下游关系的阶段(Stage)之间的数据交换。</p>

<h2>模型关系</h2>

<p><img src="http://untitled-life.github.io/images/post/modle_relation.png" alt="modle_relation" /></p>

<h1>整体架构</h1>

<p>下面两张图(有色和无色)比较直观的表示出Presto的整体架构，主从结构在大数据项目上真是无所不在。</p>

<h2>无色结构图</h2>

<p><img src="http://untitled-life.github.io/images/post/presto_architecture.jpg" alt="presto_architecture" /></p>

<h2>无色结构图</h2>

<p><img src="http://untitled-life.github.io/images/post/presto_architecture2.png" alt="presto_architecture2" /></p>

<h2>查询执行步骤</h2>

<ul>
<li>客户端将SQL发送给Presto集群的Coordinator</li>
<li>Coordinator收到查询语句后，对语句进行解析，生成查询执行计划，并且会根据数据本地性生成对应的HttpRemoteTask</li>
<li>Coordinator将每个Task发送到对应的Worker上，策略就是数据本地性</li>
<li>执行处于上游Source Stage中的Task,主要是从Connector读取数据</li>
<li>处于下游Stage的Task读取上游Stage产出的数据结果，并在该Stage每个Task所在的Worker的内存中进行后续的计算和处理</li>
<li>Coordinator将Task分发后，就会连续不断的从Root Stage中的Task获取计算结果，并缓存起来，直到所有计算结束</li>
<li>Client提交查询后，会不停的从Coordinator获取查询结果，获取一部分展示一部分，结果全部获取完表示查询结束

<h1>应用场景</h1></li>
<li>Ad-Hoc(常用)</li>
<li>ETL(INSERT INTO TABLE AS SELECT )</li>
<li>Pseudo Real Time Computation(Kakfa-Connector)

<h1>总结</h1>

<p>从背景、功能、特点、概念、架构、使用场景等角度描述了Presto，算是入门篇吧~~~</p></li>
</ul>


<p class='post-footer'>
            显示信息
            <a href='http://untitled-life.github.io/blog/2019/02/25/an-introduction-to-the-presto/'>http://untitled-life.github.io/blog/2019/02/25/an-introduction-to-the-presto/</a><br/>
            written by <a href='http://untitled-life.github.io'>Mike Cao</a>
            &nbsp;posted at <a href='http://untitled-life.github.io'>http://untitled-life.github.io</a>
            </p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[5 Baby Steps to Develop a Flink Application]]></title>
    <link href="http://untitled-life.github.io/blog/2019/01/06/5-baby-steps-to-develop-a-flink-application/"/>
    <updated>2019-01-06T10:25:13+08:00</updated>
    <id>http://untitled-life.github.io/blog/2019/01/06/5-baby-steps-to-develop-a-flink-application</id>
    <content type="html"><![CDATA[<p>一个人固然寂寞，两个人孤灯下无言相对却可以更寂寞。 &ndash;龙应台 《目送》</p>

<!-- more -->


<p>Getting up and running with Flink is easy as ABC. In this post, we go over 5 “baby” steps to guide you through setting up your first running Flink application locally. We discuss the software requirements to get started and point out some training resources that will help you understand the functionality of the framework. We also show you how to bootstrap an application, if you prefer to start from scratch! This quick overview should enable you to spin up a Flink application in close to no time.</p>

<h2>Software requirements</h2>

<p>You can develop and execute Flink applications on Linux, Mac OS X and Windows. Because most of the developers in the community operate in Unix-based setups, this environment includes the richest tooling support. To get started, the only real requirement is to have a Java Development Kit (JDK) 8 (or higher) installed — regardless of whether you are going to use Java or Scala for development. Although it is not strictly required to develop Flink applications, we recommend that you also set up the following software on your machine:</p>

<ul>
<li>Apache Maven 3.x. Most examples in our introduction trainings assume Maven for build automation. Moreover, Flink provides Maven archetypes to bootstrap new Flink Maven projects.</li>
<li>an IDE for Java and/or Scala development. Especially if you are going for Scala, we recommend using IntelliJ due to its out-of-the-box support for Maven and easy-to-install Scala plugin. For Java, Eclipse or Netbeans will work just as well.</li>
</ul>


<h2>Training material to get you going</h2>

<p>If you prefer to start your own project from scratch, then feel free to skip to this and go directly to the next step. Otherwise, data Artisans provides a whole range of training exercises that will help you become familiar with how Flink operates and ease you over time into more complex topics such as time and state management. These exercises (and possible solutions) are available on GitHub, so you can easily clone the project and use Maven to build it:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'>git clone https://github.com/dataArtisans/flink-training-exercises.git
</span><span class='line'><span class="nb">cd </span>flink-training-exercises
</span><span class='line'>mvn clean package
</span></code></pre></td></tr></table></div></figure>


<p>This should take a couple of minutes, depending on the speed of your connection — Maven will download all the required dependencies. If everything goes as expected and the build is successful, you are on the right track!</p>

<h2>Bootstrapping your own Flink project</h2>

<p>Getting started from an existing project is the easiest way to make your first steps in application development with Flink, sure. But what if you want to create your own project from scratch at some point? Flink provides Maven archetypes to generate Maven projects for both Java and Scala applications. To create a quickstart Java project as a basis for your Flink application, for instance, run the following command:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'>mvn archetype:generate <span class="se">\</span>
</span><span class='line'>-DarchetypeGroupId<span class="o">=</span>org.apache.flink <span class="se">\</span>
</span><span class='line'>-DarchetypeArtifactId<span class="o">=</span>flink-quickstart-java <span class="se">\</span>
</span><span class='line'>-DarchetypeVersion<span class="o">=</span>1.7.0
</span></code></pre></td></tr></table></div></figure>


<p>The command above generates a Maven project for Flink 1.7.0 containing two classes: StreamingJob and BatchJob; that respectively provide the basic skeletons for a streaming and batch Flink program. You can adapt the parameters to match your version and naming preferences! We recommend that you import the project into your IDE of choice to get your hands on developing a runnable example. In case you are struggling with inspiration, you can get some hints in the Flink documentation.</p>

<h2>Running and debugging your first Flink application</h2>

<p>Although Flink is a distributed data processing system, it is easier to get started in a local environment, using just your machine. In a typical setting, you would have the master (JobManager) and workers (TaskManagers) running as separate JVM processes on separate machines; but Flink also includes a mode that allows you to execute applications within the same JVM as a multi-threaded process. This mode allows you to easily develop, debug and execute your Flink application within an IDE pretty much like any other Java or Scala project. To start your application, just run the main() method as you would normally!</p>

<p><img src="http://untitled-life.github.io/images/post/first_flink_application.png" alt="first_flink_application" /></p>

<h2>Monitoring your Flink application</h2>

<p>If you are wondering what is happening under the hood of your running application, you can easily make use of the web interface that is bundled up in Flink to visualize and monitor it. To enable the interface for your local development, you will need to add a new dependency to your POM file:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class='properties'><span class='line'><span class="err">&lt;dependency&gt;</span>
</span><span class='line'>    <span class="err">&lt;groupId&gt;org.apache.flink&lt;/groupId&gt;</span>
</span><span class='line'>    <span class="err">&lt;artifactId&gt;flink-runtime-web_2.11&lt;/artifactId&gt;</span>
</span><span class='line'>    <span class="err">&lt;version&gt;${flink.version}&lt;/version&gt;</span>
</span><span class='line'><span class="err">&lt;/dependency&gt;</span>
</span></code></pre></td></tr></table></div></figure>


<p>And explicitly create a local execution environment with the required configuration:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class='java'><span class='line'><span class="kn">import</span> <span class="nn">org.apache.flink.configuration.ConfigConstants</span><span class="o">;</span>
</span><span class='line'>
</span><span class='line'><span class="n">Configuration</span> <span class="n">config</span> <span class="o">=</span> <span class="k">new</span> <span class="nf">Configuration</span><span class="o">();</span>
</span><span class='line'><span class="n">config</span><span class="o">.</span><span class="na">setBoolean</span><span class="o">(</span><span class="n">ConfigConstants</span><span class="o">.</span><span class="na">LOCAL_START_WEBSERVER</span><span class="o">,</span> <span class="kc">true</span><span class="o">);</span>
</span><span class='line'><span class="n">StreamExecutionEnvironment</span> <span class="n">env</span> <span class="o">=</span> <span class="n">StreamExecutionEnvironment</span><span class="o">.</span><span class="na">createLocalEnvironment</span><span class="o">(</span><span class="n">config</span><span class="o">);</span>
</span></code></pre></td></tr></table></div></figure>


<p>The Flink web interface should now be available under <a href="http://localhost:8081">http://localhost:8081</a> once you get your program running, and look somewhat like the images below. Remember to update the Maven project once you change the POM file, if you don’t have auto-import enabled in your IDE — this might lead to an error when you first try to access the web interface.</p>

<p><img src="http://untitled-life.github.io/images/post/flink_web_interface.png" alt="flink_web_interface" /></p>

<p>Congratulations! You have just run your first Flink application in an IDE. A tutorial in the Flink documentation goes one step further and shows how to setup a local Flink cluster and submit a job just like submitting it to a remote cluster.</p>

<p>We encourage you to have a look through the Flink documentation for further support or more detailed information. If at any point you feel lost or have any questions: our community is very active on Stack Overflow, and you can also reach out to the developers using the Mailing List. The data Artisans Standard training goes through the training exercises in more detail, so if you would prefer to get some personal guidance, you can find more about future trainings and locations here. We are looking forward to your next adventures with Flink!flri</p>

<p class='post-footer'>
            显示信息
            <a href='http://untitled-life.github.io/blog/2019/01/06/5-baby-steps-to-develop-a-flink-application/'>http://untitled-life.github.io/blog/2019/01/06/5-baby-steps-to-develop-a-flink-application/</a><br/>
            written by <a href='http://untitled-life.github.io'>Mike Cao</a>
            &nbsp;posted at <a href='http://untitled-life.github.io'>http://untitled-life.github.io</a>
            </p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[The Difference of ETL vs ELT]]></title>
    <link href="http://untitled-life.github.io/blog/2018/12/29/the-difference-of-etl-vs-elt/"/>
    <updated>2018-12-29T11:04:31+08:00</updated>
    <id>http://untitled-life.github.io/blog/2018/12/29/the-difference-of-etl-vs-elt</id>
    <content type="html"><![CDATA[<p>一个人走路，才是你和风景之间的单独私会。 &ndash;龙应台 《目送》</p>

<!-- more -->


<p>For the last couple of decades ETL (extract, transform, load) has been the traditional approach for data warehousing and analytics. The ELT (extract, load, transform) approach changes the old paradigm. But, what’s actually happening when the “T” and “L” are switched?</p>

<p>ETL and ELT solve the same need:</p>

<blockquote><p>Billions of data and events need to be collected, processed and analyzed by businesses. The data needs to be be clean, manageable and ready to analyze. It needs to be enriched, molded and transformed. To make it meaningful.</p></blockquote>

<p>But, the <strong>“how”</strong> is what’s different and leads to new possibilities in many modern data projects. There are differences in how raw data is managed, when processing is done and how analysis is performed.</p>

<p>In this article, we’ll demonstrate the ETL and ELT technological differences showing data engineering and analysis examples of the two approaches and summarizing <strong>10 pros and cons of ETL and ELT.</strong></p>

<p><strong>The Technological Differences: Lets first align on the 3 stages - E, T, L:</strong></p>

<ul>
<li><strong>Extraction</strong>: Retrieving raw data from an unstructured data pool and migrating it into a temporary, staging data repository</li>
<li><strong>Transformation</strong>: Structuring, enriching and converting the raw data to match the target source</li>
<li><strong>Loading</strong>: Loading the structured data into a data warehouse to be analyzed and used by business intelligence (BI) tools</li>
</ul>


<h2>ETL vs. ELT: What is ETL?</h2>

<p>ETL requires management of the raw data, including the extraction of the required information and running the right transformations to ultimately serve the business needs. Each stage - extraction, transformation and loading - requires interaction by data engineers and developers, and dealing with capacity limitations of traditional data warehouses. Using ETL, analysts and other BI users have become accustomed to waiting, since simple access to the information is not available until the whole ETL process has been completed.</p>

<p><img src="http://untitled-life.github.io/images/post/ETL.jpeg" alt="ETL" /></p>

<h2>What is ELT?</h2>

<p>In the ELT approach, after you’ve extracted your data, you immediately start the loading phase - moving all the data sources into a single, centralized data repository. With today’s infrastructure technologies using the cloud, systems can now support large storage and scalable compute. Therefore, a large, expanding data pool and fast processing is virtually endless for maintaining all the extracted raw data.</p>

<p><img src="http://untitled-life.github.io/images/post/ELT.jpeg" alt="ELT" /></p>

<p>In this way, the ELT approach provides a modern alternative to ETL. However, it’s still evolving. Therefore, the frameworks and tools to support the ELT process are not always fully developed to facilitate load and processing of large amount of data. The upside is very promising - enabling unlimited access to all of your data at any time and saving developers efforts and time for BI users and analysts.</p>

<h2>Summarizing 10 Pros &amp; Cons of ETL and ELT</h2>

<p><strong>1. Time - Load</strong></p>

<p>ETL: Uses staging area and system, extra time to load data</p>

<p>ELT: All in one system, load only once</p>

<p><strong>2. Time - Transformation</strong></p>

<p>ETL: Need to wait, especially for big data sizes - as data grows, transformation time increases</p>

<p>ELT: All in one system, speed is not dependent on data size</p>

<p><strong>3. Time - Maintenance</strong></p>

<p>ETL: High maintenance - choice of data to load and transform and must do it again if deleted or want to enhance the main data repository</p>

<p>ELT: Low maintenance - all data is always available</p>

<p><strong>4. Implementation complexity</strong></p>

<p>ETL: At early stage, requires less space and result is clean</p>

<p>ELT: Requires in-depth knowledge of tools and expert design of the main large repository</p>

<p><strong>5. Analysis &amp; processing style</strong></p>

<p>ETL: Based on multiple scripts to create the views - deleting view means deleting data</p>

<p>ELT: Creating adhoc views - low cost for building and maintaining</p>

<p><strong>6. Data limitation or restriction in supply</strong></p>

<p>ETL: By presuming and choosing data a priori</p>

<p>ELT: By HW (none) and data retention policy</p>

<p><strong>7. Data warehouse support</strong></p>

<p>ETL: Prevalent legacy model used for on-premises and relational, structured data</p>

<p>ELT: Tailored to using in scalable cloud infrastructure to support structured, unstructured such big data sources</p>

<p><strong>8. Data lake support</strong></p>

<p>ETL: Not part of approach</p>

<p>ELT: Enables use of lake with unstructured data supported</p>

<p><strong>9. Usability</strong></p>

<p>ETL: Fixed tables, Fixed timeline, Used mainly by IT</p>

<p>ELT: Ad Hoc, Agility, Flexibility, Usable by everyone from developer to citizen integrator</p>

<p><strong>10. Cost-effective</strong></p>

<p>ETL: Not cost-effective for small and medium businesses</p>

<p>ELT: Scalable and available to all business sizes using online SaaS solutions</p>

<p class='post-footer'>
            显示信息
            <a href='http://untitled-life.github.io/blog/2018/12/29/the-difference-of-etl-vs-elt/'>http://untitled-life.github.io/blog/2018/12/29/the-difference-of-etl-vs-elt/</a><br/>
            written by <a href='http://untitled-life.github.io'>Mike Cao</a>
            &nbsp;posted at <a href='http://untitled-life.github.io'>http://untitled-life.github.io</a>
            </p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Wide vs Narrow Dependencies]]></title>
    <link href="http://untitled-life.github.io/blog/2018/12/27/wide-vs-narrow-dependencies/"/>
    <updated>2018-12-27T17:41:52+08:00</updated>
    <id>http://untitled-life.github.io/blog/2018/12/27/wide-vs-narrow-dependencies</id>
    <content type="html"><![CDATA[<p>太疼的伤口，你不敢去触碰；太深的忧伤，你不敢去安慰；太残酷的残酷，有时候，你不敢去注视。 &ndash;龙应台 《目送》</p>

<!-- more -->


<p>In this session, we are going to focus on wide versus narrow dependencies, which dictate relationships between RDDs in graphs of computation, which we&rsquo;ll see has a lot to do with shuffling.</p>

<p>So far, we have seen that some transformations significantly more expensive (latency) than others.</p>

<p>In this session we will:</p>

<ul>
<li>look at how RDD&rsquo;s are represented</li>
<li>dive into how and when Spark decides it must shuffle data</li>
<li>see how these dependencies make fault tolerance possible</li>
</ul>


<h2>Lineages</h2>

<p>Computations on RDDs are represented as a lineage graph, a DAG representing the computations done on the RDD. This representation/DAG is what Spark analyzes to do optimizations. Because of this, for a particular operation, it is possible to step back and figure out how a result of a computation is derived from a particular point.</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class='scala'><span class='line'><span class="k">val</span> <span class="n">rdd</span> <span class="k">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">textFile</span><span class="o">(...)</span>
</span><span class='line'><span class="k">val</span> <span class="n">filtered</span> <span class="k">=</span> <span class="n">rdd</span><span class="o">.</span><span class="n">map</span><span class="o">(...).</span><span class="n">filter</span><span class="o">(...).</span><span class="n">persist</span><span class="o">()</span>
</span><span class='line'><span class="k">val</span> <span class="n">count</span> <span class="k">=</span> <span class="n">filtered</span><span class="o">.</span><span class="n">count</span><span class="o">()</span>
</span><span class='line'><span class="k">val</span> <span class="n">reduced</span> <span class="k">=</span> <span class="n">filtered</span><span class="o">.</span><span class="n">reduce</span><span class="o">()</span>
</span></code></pre></td></tr></table></div></figure>


<p><img src="http://untitled-life.github.io/images/post/lineage_graph.png" alt="lineage_graph" /></p>

<h2>How RDDs are represented</h2>

<p>RDDs are made up of 4 parts:</p>

<ul>
<li>Partitions: Atomic pieces of the dataset. One or many per compute node.</li>
<li>Dependencies: Models relationship between this RDD and its partitions with the RDD(s) it was derived from. (Note that the dependencies maybe modeled per partition as shown below).</li>
<li>A function for computing the dataset based on its parent RDDs.</li>
<li>Metadata about it partitioning scheme and data placement.</li>
</ul>


<p>/Users/caolei/IdeaProjects/untitled-life/source/images/post/.png</p>

<p><img src="http://untitled-life.github.io/images/post/rdd_anatomy_1.png" alt="rdd_anatomy_1" /></p>

<p><img src="http://untitled-life.github.io/images/post/rdd_anatomy_2.png" alt="rdd_anatomy_2" /></p>

<h2>RDD Dependencies and Shuffles</h2>

<p>Previously we saw the Rule of thumb: a shuffle can occur when the resulting RDD depends on other elements from the same RDD or another RDD.</p>

<p><strong>In fact, RDD dependencies encode when data must move across network</strong>. Thus they tell us when data is going to be shuffled.</p>

<p>Transformations cause shuffles, and can have 2 kinds of dependencies:
1. Narrow dependencies: Each partition of the parent RDD is used by at most one partition of the child RDD.</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='text'><span class='line'>[parent RDD partition] ---&gt; [child RDD partition]
</span></code></pre></td></tr></table></div></figure>


<p><strong>Fast</strong>! No shuffle necessary. Optimizations like pipelining possible. Thus transformations which have narrow dependencies are fast.</p>

<ol>
<li>Wide dependencies: Each partition of the parent RDD may be used by multiple child partitions</li>
</ol>


<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class='text'><span class='line'>                   ---&gt; [child RDD partition 1]
</span><span class='line'>[parent RDD partition] ---&gt; [child RDD partition 2]
</span><span class='line'>                   ---&gt; [child RDD partition 3]
</span></code></pre></td></tr></table></div></figure>


<p><strong>Slow</strong>! Shuffle necessary for all or some data over the network. Thus transformations which have narrow dependencies are slow.</p>

<h3>Visual: Narrow dependencies Vs. Wide dependencies</h3>

<p><img src="http://untitled-life.github.io/images/post/narrow_vs_wide_dependencies.png" alt="narrow_vs_wide_dependencies" /></p>

<h3>Visual: Example</h3>

<p>Assume that we have a following DAG:</p>

<p><img src="http://untitled-life.github.io/images/post/visual_dag.png" alt="visual_dag" /></p>

<p>What do the dependencies look like? Which are wide and which are narrow?</p>

<p><img src="http://untitled-life.github.io/images/post/visual_dag_resolved.png" alt="visual_dag_resolved" /></p>

<p>The B to G join is narrow because groupByKey already partitions the keys and places them appropriately in B after shuffling.</p>

<p>Thus operations like join can <strong>sometimes be narrow and sometimes be wide.</strong></p>

<p><strong>Transformations with (usually) Narrow dependencies:</strong>
- map
- mapValues
- flatMap
- filter
- mapPartitions
- mapPartitionsWithIndex</p>

<p><strong>Transformations with (usually) Wide dependencies: (might cause a shuffle)</strong>
- cogroup
- groupWith
- join
- leftOuterJoin
- rightOuterJoin
- groupByKey
- reduceByKey
- combineByKey
- distinct
- intersection
- repartition
- coalesce</p>

<p><strong>This list usually holds, but as seen above, in case of join, depending on the use case, the dependency of an operation may be different from the above lists</strong></p>

<h3>How can I find out?</h3>

<p>dependencies method on RDDs: returns a sequence of Dependency objects, which are actually the dependencies used by Spark&rsquo;s scheduler to know how this RDD depends on RDDs.</p>

<p>The sorts of dependency objects that this method may return include:</p>

<ul>
<li><p>Narrow dependency objects</p>

<ul>
<li>OneToOneDependency</li>
<li>PruneDependency</li>
<li>RangeDependency</li>
</ul>
</li>
<li><p>Wide dependency objects</p>

<ul>
<li>ShuffleDependency</li>
</ul>
</li>
</ul>


<p>Another method toDebugString prints out a visualization of the RDD lineage along with other information relevant to scheduling. For example, indentations in the output separate groups of narrow transformations that may be pipelined together with wide transformations that require shuffles. These groupings are called <strong>stages.</strong></p>

<h5>Example</h5>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
</pre></td><td class='code'><pre><code class='scala'><span class='line'><span class="k">val</span> <span class="n">wordsRDD</span> <span class="k">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">parallelize</span><span class="o">(</span><span class="n">largeList</span><span class="o">)</span>
</span><span class='line'>
</span><span class='line'><span class="cm">/* dependencies */</span>
</span><span class='line'>
</span><span class='line'><span class="k">val</span> <span class="n">pairs</span> <span class="k">=</span> <span class="n">wordsRdd</span><span class="o">.</span><span class="n">map</span><span class="o">(</span><span class="n">c</span><span class="o">=&gt;(</span><span class="n">c</span><span class="o">,</span><span class="mi">1</span><span class="o">))</span>
</span><span class='line'>                    <span class="o">.</span><span class="n">groupByKey</span>
</span><span class='line'>                    <span class="o">.</span><span class="n">dependencies</span>          <span class="c1">// &lt;-------------</span>
</span><span class='line'><span class="c1">// pairs: Seq[org.apache.spark.Dependency[_]] = List(org.apache.spark.ShuffleDependency@4294a23d)</span>
</span><span class='line'>
</span><span class='line'><span class="cm">/* toDebugString */</span>
</span><span class='line'>
</span><span class='line'><span class="k">val</span> <span class="n">pairs</span> <span class="k">=</span> <span class="n">wordsRdd</span><span class="o">.</span><span class="n">map</span><span class="o">(</span><span class="n">c</span><span class="o">=&gt;(</span><span class="n">c</span><span class="o">,</span><span class="mi">1</span><span class="o">))</span>
</span><span class='line'>                    <span class="o">.</span><span class="n">groupByKey</span>
</span><span class='line'>                    <span class="o">.</span><span class="n">toDebugString</span>         <span class="c1">// &lt;-------------</span>
</span><span class='line'><span class="c1">// pairs: String =</span>
</span><span class='line'><span class="c1">// (8) ShuffledRDD[219] at groupByKey at &lt;console&gt;:38 []</span>
</span><span class='line'><span class="c1">//  +-(8) MapPartitionsRDD[218] at map at &lt;console&gt;:37 []</span>
</span><span class='line'><span class="c1">//     | ParallelCollectionRDD[217] at parallelize at &lt;console&gt;:36 []</span>
</span></code></pre></td></tr></table></div></figure>


<h2>Lineages and Fault Tolerance</h2>

<p><strong>Lineages are the key to fault tolerance in Spark</strong></p>

<p>Ideas from <strong>functional programming</strong> enable fault tolerance in Spark:</p>

<ul>
<li>RDDs are immutable</li>
<li>We use higher-order-functions like map,flatMap,filter to do functional transformations on this immutable data</li>
<li>A function for computing the dataset based on its parent RDDs also is part of an RDD&rsquo;s representation.</li>
</ul>


<p>We just need to keep the information required by these 3 properties.</p>

<p>Along with keeping track of the dependency information between partitions as well, these 3 properties allow us to: <strong>Recover from failures by recomputing lost partitions from lineage graphs</strong>, as it is easy to back track in a lineage graph.</p>

<p>Thus we get Fault Tolerance without having to write the RDDs/Data to the disk! The whole data can be re-derived using the above information.</p>

<h3>Visual Example</h3>

<p>Lets assume one of our partitions from our previous example fails:</p>

<p><img src="http://untitled-life.github.io/images/post/fault_tol_1.png" alt="fault_tol_1" /></p>

<p>We only have to recompute the data shown below to get back on track:</p>

<p><img src="http://untitled-life.github.io/images/post/fault_tol_2.png" alt="fault_tol_2" /></p>

<p>Recomputing missing partitions is <strong>fast for narrow</strong> but <strong>slow for wide</strong> dependencies. So if above, a partition in G would have failed, it would have taken us more time to recompute that partition. So losing partitions that were derived from a transformation with wide dependencies, can be much slower.<p class='post-footer'>
            显示信息
            <a href='http://untitled-life.github.io/blog/2018/12/27/wide-vs-narrow-dependencies/'><a href="http://untitled-life.github.io/blog/2018/12/27/wide-vs-narrow-dependencies/">http://untitled-life.github.io/blog/2018/12/27/wide-vs-narrow-dependencies/</a></a><br/>
            written by <a href='http://untitled-life.github.io'>Mike Cao</a>
            &nbsp;posted at <a href='http://untitled-life.github.io'><a href="http://untitled-life.github.io">http://untitled-life.github.io</a></a>
            </p></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Flink Forward China 2018 Slides]]></title>
    <link href="http://untitled-life.github.io/blog/2018/12/23/flink-forward-china-2018-slides/"/>
    <updated>2018-12-23T11:06:47+08:00</updated>
    <id>http://untitled-life.github.io/blog/2018/12/23/flink-forward-china-2018-slides</id>
    <content type="html"><![CDATA[<blockquote><p>人生由淡淡的悲伤和淡淡的幸福组成，在小小的期待、偶尔的兴奋和沉默的失望中度过每一天，然后带着一种想说却又说不出来的‘懂’，作最后的转身离开。 &ndash;龙应台 《目送》</p></blockquote>

<!-- more -->


<h1>主会场</h1>

<ol>
<li>【主会场01-Craig Russell】Apache Flink: 践行 Apache 之道：<a href="https://files.alicdn.com/tpsservice/b40e46ca0abcef3e3f12f7fe12c46f0a.pdf">https://files.alicdn.com/tpsservice/b40e46ca0abcef3e3f12f7fe12c46f0a.pdf</a></li>
<li>【主会场02-周靖人】云上计算普惠科技：<a href="https://files.alicdn.com/tpsservice/9a90a15cc2666348aa7e2fef4846eea7.pdf">https://files.alicdn.com/tpsservice/9a90a15cc2666348aa7e2fef4846eea7.pdf</a></li>
<li>【主会场03-蒋晓伟】Apache Flink® - Redefining Computation：<a href="https://files.alicdn.com/tpsservice/62fa5ebcd23ea0b8a956f2a06197b57a.pdf">https://files.alicdn.com/tpsservice/62fa5ebcd23ea0b8a956f2a06197b57a.pdf</a></li>
<li>【主会场04-Stephan Ewen】Stream Processing takes on Everything：【Updating&hellip;】</li>
<li>【主会场05-闵万里】城市级实时计算的力量：<a href="https://files.alicdn.com/tpsservice/5f3272c3212b77cd835ff020d9954480.pdf">https://files.alicdn.com/tpsservice/5f3272c3212b77cd835ff020d9954480.pdf</a></li>
<li>【主会场06-罗李】滴滴实时计算平台架构与实践：<a href="https://files.alicdn.com/tpsservice/aca017af879a657ed0983b8f1cf4bbfd.pdf">https://files.alicdn.com/tpsservice/aca017af879a657ed0983b8f1cf4bbfd.pdf</a></li>
</ol>


<h1>分会场一</h1>

<ol>
<li>【分会场一01-钱正平】为并行图数据处理提供高层抽象／语言：<a href="https://files.alicdn.com/tpsservice/1f9799e34cd9ce22678b1ed13b71e615.pdf">https://files.alicdn.com/tpsservice/1f9799e34cd9ce22678b1ed13b71e615.pdf</a></li>
<li>【分会场一02-秦江杰 / 孙金城】Simplify Machine Learning With Flink TableAPI：<a href="https://files.alicdn.com/tpsservice/69181d1fd85d15635a7fe64ebafbf140.pdf">https://files.alicdn.com/tpsservice/69181d1fd85d15635a7fe64ebafbf140.pdf</a></li>
<li>【分会场一03-时金魁】Flink七武器及应用实战：<a href="https://files.alicdn.com/tpsservice/4346f13e6710d3faed35fa21976ce1cb.pdf">https://files.alicdn.com/tpsservice/4346f13e6710d3faed35fa21976ce1cb.pdf</a></li>
<li>【分会场一04-崔星灿】快速融入Apache Flink开源社区：<a href="https://files.alicdn.com/tpsservice/981cfe593809cc08c11c5f0cee4d19cd.pdf">https://files.alicdn.com/tpsservice/981cfe593809cc08c11c5f0cee4d19cd.pdf</a></li>
<li>【分会场一05-施晓罡 / 郑灿彬】基于Apache Flink的平台化构建及运维优化经验：<a href="https://files.alicdn.com/tpsservice/9bcc469feb3dcca4ea15226e70e23ed5.pdf">https://files.alicdn.com/tpsservice/9bcc469feb3dcca4ea15226e70e23ed5.pdf</a></li>
</ol>


<h1>分会场二</h1>

<ol>
<li>【分会场二01-Radu Tudoran】Real-Time AI in Huawei Cloud: Practicing with Streaming ML：<a href="https://files.alicdn.com/tpsservice/3d836d6366a800dc70f52a0ec0bb5bc5.pdf">https://files.alicdn.com/tpsservice/3d836d6366a800dc70f52a0ec0bb5bc5.pdf</a></li>
<li>【分会场二02-Konstantin Knauf】Apache Flink® 1.7 and Beyond：【Updating&hellip;】</li>
<li>【分会场二03-李峰】FLINK在大规模实时无效广告流量检测中的应用：<a href="https://files.alicdn.com/tpsservice/c429c9351675f89a56000489519135a8.pdf">https://files.alicdn.com/tpsservice/c429c9351675f89a56000489519135a8.pdf</a></li>
<li>【分会场二04-Piotr Nowojsk】Flink Streaming SQL 2018：【Updating&hellip;】</li>
<li>【分会场二05-孟文瑞 / 于淼】Uber 商业性能指标生成与管理：<a href="https://files.alicdn.com/tpsservice/9bf841f251392aedcbb7cc98c5d140fa.pdf">https://files.alicdn.com/tpsservice/9bf841f251392aedcbb7cc98c5d140fa.pdf</a></li>
<li>【分会场二06-戴资力】Apache Flink 流式应用中状态的数据结构定义升级：【Updating&hellip;】</li>
</ol>


<h1>分会场三</h1>

<ol>
<li>【分会场三01-邹丹】Flink在字节跳动的实践：<a href="https://files.alicdn.com/tpsservice/6b7686e18135389a76e2a0e476b270ec.pdf">https://files.alicdn.com/tpsservice/6b7686e18135389a76e2a0e476b270ec.pdf</a></li>
<li>【分会场三02-鞠⼤升】基于Flink的美团点评实时计算平台实践和应⽤：<a href="https://files.alicdn.com/tpsservice/d855dadbdeacb1d7bae82c2780a545b5.pdf">https://files.alicdn.com/tpsservice/d855dadbdeacb1d7bae82c2780a545b5.pdf</a></li>
<li>【分会场三03-杨克特 / 伍翀】基于streaming构建统一的数据处理引擎的挑战与实践：<a href="https://files.alicdn.com/tpsservice/74235d95df4b7b3163a96615e0b61131.pdf">https://files.alicdn.com/tpsservice/74235d95df4b7b3163a96615e0b61131.pdf</a></li>
<li>【分会场三05-李钰 / 唐云】Flink中的两类新型状态存储：<a href="https://files.alicdn.com/tpsservice/1df9ccb8a7b6b2782a558d3c32d40c19.pdf">https://files.alicdn.com/tpsservice/1df9ccb8a7b6b2782a558d3c32d40c19.pdf</a></li>
</ol>


<h1>分会场四</h1>

<ol>
<li>【分会场四01-王绍翾 / 章剑锋】Challenges and Opportunities of Apache Flink® Ecosystem：<a href="https://files.alicdn.com/tpsservice/6122ed352e520aae78a22ed19657d150.pdf">https://files.alicdn.com/tpsservice/6122ed352e520aae78a22ed19657d150.pdf</a></li>
<li>【分会场四02-翟佳 / 郭斯杰】使⽤Flink和Pulsar进⾏批流⼀体弹性计算：<a href="https://files.alicdn.com/tpsservice/36a600d88492560d8f33c86ce9e3f746.pdf">https://files.alicdn.com/tpsservice/36a600d88492560d8f33c86ce9e3f746.pdf</a></li>
<li>【分会场四03-滕昱】为流处理世界重新设计的存储：<a href="https://files.alicdn.com/tpsservice/8c72901db4a4bda83e33d35b8e6d0ecd.pdf">https://files.alicdn.com/tpsservice/8c72901db4a4bda83e33d35b8e6d0ecd.pdf</a></li>
<li>【分会场三04-杨旭】Alink：基于Apache Flink的算法平台：<a href="https://files.alicdn.com/tpsservice/23c67b6682c7d74339af7c53fccac429.pdf">https://files.alicdn.com/tpsservice/23c67b6682c7d74339af7c53fccac429.pdf</a></li>
<li>【分会场四06-徐骁】Apache Flink 和 Elasticsearch 助⼒实时 OLAP 平台：<a href="https://files.alicdn.com/tpsservice/44558decf0f39980283107647d1e5755.pdf">https://files.alicdn.com/tpsservice/44558decf0f39980283107647d1e5755.pdf</a></li>
</ol>


<h1>分会场五</h1>

<ol>
<li>【分会场五01-姜春宇】以标准推动开源技术在行业的落地：<a href="https://files.alicdn.com/tpsservice/13039399808974a7c40be0c5d671061e.pdf">https://files.alicdn.com/tpsservice/13039399808974a7c40be0c5d671061e.pdf</a></li>
<li>【分会场五03-陈华曦】基于Apache Flink的搜索处理平台：<a href="https://files.alicdn.com/tpsservice/8dab3c208f8044a26937a7bd7aed3c3d.pdf">https://files.alicdn.com/tpsservice/8dab3c208f8044a26937a7bd7aed3c3d.pdf</a></li>
<li>【分会场五05-倪春恩】Apache Kylin:大数据OLAP利器：<a href="https://files.alicdn.com/tpsservice/4a21d431d914e85b0edced063dbc40ff.pdf">https://files.alicdn.com/tpsservice/4a21d431d914e85b0edced063dbc40ff.pdf</a></li>
<li>【分会场五06-李剑】Flink在阿里巴巴电商业务中的应用：<a href="https://files.alicdn.com/tpsservice/badd0c8d32c9008d95addc0a28f1eb11.pdf">https://files.alicdn.com/tpsservice/badd0c8d32c9008d95addc0a28f1eb11.pdf</a><p class='post-footer'>
         显示信息
         <a href='http://untitled-life.github.io/blog/2018/12/23/flink-forward-china-2018-slides/'><a href="http://untitled-life.github.io/blog/2018/12/23/flink-forward-china-2018-slides/">http://untitled-life.github.io/blog/2018/12/23/flink-forward-china-2018-slides/</a></a><br/>
         written by <a href='http://untitled-life.github.io'>Mike Cao</a>
         &nbsp;posted at <a href='http://untitled-life.github.io'><a href="http://untitled-life.github.io">http://untitled-life.github.io</a></a>
         </p></li>
</ol>

]]></content>
  </entry>
  
</feed>
